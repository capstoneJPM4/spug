{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use pytorch geometric temporal, make sure you have torch 1.9.0 installed (uninstall 1.10.0 before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cpu.html\n",
    "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cpu.html\n",
    "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.9.0+cpu.html\n",
    "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.9.0+cpu.html\n",
    "!pip install torch-geometric\n",
    "!pip install torch-geometric-temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def transform_and_split(data):\n",
    "    # Normalize node features and transform data type\n",
    "    data.x = normalize(data.x, axis=1, norm='max')\n",
    "    data.x = torch.from_numpy(data.x).to(torch.float64)\n",
    "    data.y = data.y.apply_(lambda x:  1 if (x > 0) else 0) # Change y into {0, 1} for binary classification\n",
    "    data.y = data.y.to(torch.float64)    \n",
    "    data.edge_attr = data.edge_attr.to(torch.double)\n",
    "\n",
    "\n",
    "    # Split into train/test set\n",
    "#    split = nodeSplit(split=\"train_rest\", num_splits = 1, num_val = 0.0, num_test= 0.2)\n",
    "#    masked_data = split(data)\n",
    "\n",
    "#    print(\"Training samples:\", torch.sum(masked_data.train_mask).item())\n",
    "#    print(\"Validation samples:\", torch.sum(masked_data.val_mask ).item())\n",
    "#    print(\"Test samples:\", torch.sum(masked_data.test_mask ).item())\n",
    "    print_basic_info(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_basic_info(data):\n",
    "    print()\n",
    "    print(data)\n",
    "    print('===========================================================================================================')\n",
    "\n",
    "    print(f'Number of nodes: {data.num_nodes}')\n",
    "    print(f'Number of edges: {data.num_edges}')\n",
    "    print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "    print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "    print(f'Has self-loops: {data.has_self_loops()}')\n",
    "    print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data(x=[29, 61], edge_index=[2, 400], edge_attr=[400], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 400\n",
      "Average node degree: 13.79\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/processed/twitter/2018_q1.pt\" # Customize...\n",
    "dataset = torch.load(path)\n",
    "data = dataset[0]\n",
    "transformed_data = transform_and_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/news/news_data_weekly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>url</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/04/opinion/the...</td>\n",
       "      <td>Donald Trump is a thug. He’s a thug who talks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/08/us/politics...</td>\n",
       "      <td>In lucrative paid speeches that Hillary Clint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/07/technology/...</td>\n",
       "      <td>SAN FRANCISCO — Marc Benioff, the founder and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/05/business/pr...</td>\n",
       "      <td>Prepaid debit cards are a financial lifeline ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/04/world/ameri...</td>\n",
       "      <td>RIO DE JANEIRO — It was not a banner day for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>2021-10-02</td>\n",
       "      <td>https://www.nytimes.com/2021/09/30/sports/socc...</td>\n",
       "      <td>Looking to expand its global footprint beyond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>2021-10-02</td>\n",
       "      <td>https://www.nytimes.com/2021/10/01/business/cr...</td>\n",
       "      <td>Despite the popularity of mobile apps promisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>2021-10-02</td>\n",
       "      <td>https://www.nytimes.com/2021/10/02/your-money/...</td>\n",
       "      <td>Introducing your child to the real-world use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>2021-10-02</td>\n",
       "      <td>https://www.nytimes.com/2021/09/26/fashion/wat...</td>\n",
       "      <td>Like their counterparts in industries such as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>2021-10-02</td>\n",
       "      <td>https://www.nytimes.com/2021/09/27/business/ca...</td>\n",
       "      <td>LOS ANGELES — Creative Artists Agency announc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3636 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date                                                url  \\\n",
       "2746  2016-10-08  https://www.nytimes.com/2016/10/04/opinion/the...   \n",
       "33    2016-10-08  https://www.nytimes.com/2016/10/08/us/politics...   \n",
       "32    2016-10-08  https://www.nytimes.com/2016/10/07/technology/...   \n",
       "31    2016-10-08  https://www.nytimes.com/2016/10/05/business/pr...   \n",
       "30    2016-10-08  https://www.nytimes.com/2016/10/04/world/ameri...   \n",
       "...          ...                                                ...   \n",
       "1518  2021-10-02  https://www.nytimes.com/2021/09/30/sports/socc...   \n",
       "1517  2021-10-02  https://www.nytimes.com/2021/10/01/business/cr...   \n",
       "1516  2021-10-02  https://www.nytimes.com/2021/10/02/your-money/...   \n",
       "2736  2021-10-02  https://www.nytimes.com/2021/09/26/fashion/wat...   \n",
       "2744  2021-10-02  https://www.nytimes.com/2021/09/27/business/ca...   \n",
       "\n",
       "                                                  texts  \n",
       "2746   Donald Trump is a thug. He’s a thug who talks...  \n",
       "33     In lucrative paid speeches that Hillary Clint...  \n",
       "32     SAN FRANCISCO — Marc Benioff, the founder and...  \n",
       "31     Prepaid debit cards are a financial lifeline ...  \n",
       "30     RIO DE JANEIRO — It was not a banner day for ...  \n",
       "...                                                 ...  \n",
       "1518   Looking to expand its global footprint beyond...  \n",
       "1517   Despite the popularity of mobile apps promisi...  \n",
       "1516   Introducing your child to the real-world use ...  \n",
       "2736   Like their counterparts in industries such as...  \n",
       "2744   LOS ANGELES — Creative Artists Agency announc...  \n",
       "\n",
       "[3636 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [part for _, part in df.groupby(pd.Grouper(key='Date', freq='W-MON'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>url</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/04/opinion/the...</td>\n",
       "      <td>Donald Trump is a thug. He’s a thug who talks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/08/us/politics...</td>\n",
       "      <td>In lucrative paid speeches that Hillary Clint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/07/technology/...</td>\n",
       "      <td>SAN FRANCISCO — Marc Benioff, the founder and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/05/business/pr...</td>\n",
       "      <td>Prepaid debit cards are a financial lifeline ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/04/world/ameri...</td>\n",
       "      <td>RIO DE JANEIRO — It was not a banner day for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/07/business/de...</td>\n",
       "      <td>WASHINGTON — Nearly five years after Jon S. C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/08/business/in...</td>\n",
       "      <td>LONDON — As Europe has grappled with the trau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/08/world/europ...</td>\n",
       "      <td>LONDON — For those blithely inclined toward t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/04/business/de...</td>\n",
       "      <td>The Janus Capital Group and the Henderson Gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/09/world/middl...</td>\n",
       "      <td>TEHRAN — Rushing for a plane to Tehran becaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/06/us/politics...</td>\n",
       "      <td>Following is a transcript of the vice-preside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/06/t-magazine/...</td>\n",
       "      <td>IN THE RANK OF UNFLATTERING monikers for an a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date                                                url  \\\n",
       "2746 2016-10-08  https://www.nytimes.com/2016/10/04/opinion/the...   \n",
       "33   2016-10-08  https://www.nytimes.com/2016/10/08/us/politics...   \n",
       "32   2016-10-08  https://www.nytimes.com/2016/10/07/technology/...   \n",
       "31   2016-10-08  https://www.nytimes.com/2016/10/05/business/pr...   \n",
       "30   2016-10-08  https://www.nytimes.com/2016/10/04/world/ameri...   \n",
       "36   2016-10-08  https://www.nytimes.com/2016/10/07/business/de...   \n",
       "37   2016-10-08  https://www.nytimes.com/2016/10/08/business/in...   \n",
       "38   2016-10-08  https://www.nytimes.com/2016/10/08/world/europ...   \n",
       "34   2016-10-08  https://www.nytimes.com/2016/10/04/business/de...   \n",
       "2747 2016-10-08  https://www.nytimes.com/2016/10/09/world/middl...   \n",
       "35   2016-10-08  https://www.nytimes.com/2016/10/06/us/politics...   \n",
       "3252 2016-10-08  https://www.nytimes.com/2016/10/06/t-magazine/...   \n",
       "\n",
       "                                                  texts  \n",
       "2746   Donald Trump is a thug. He’s a thug who talks...  \n",
       "33     In lucrative paid speeches that Hillary Clint...  \n",
       "32     SAN FRANCISCO — Marc Benioff, the founder and...  \n",
       "31     Prepaid debit cards are a financial lifeline ...  \n",
       "30     RIO DE JANEIRO — It was not a banner day for ...  \n",
       "36     WASHINGTON — Nearly five years after Jon S. C...  \n",
       "37     LONDON — As Europe has grappled with the trau...  \n",
       "38     LONDON — For those blithely inclined toward t...  \n",
       "34     The Janus Capital Group and the Henderson Gro...  \n",
       "2747   TEHRAN — Rushing for a plane to Tehran becaus...  \n",
       "35     Following is a transcript of the vice-preside...  \n",
       "3252   IN THE RANK OF UNFLATTERING monikers for an a...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'companies': [{'wba': {'alias': ['$wba', 'wba', 'walgreen boots alliance inc', 'walgreen boots alliance', 'walgreenbootsalliance']}}, {'v': {'alias': ['$v', 'v', 'visa inc class a', 'visa']}}, {'crm': {'alias': ['$crm', 'crm', 'salesforce.com inc', 'salesforce']}}, {'cvx': {'alias': ['$cvx', 'cvx', 'chevron corp', 'chevron']}}, {'pg': {'alias': ['$pg', 'pg', 'procter & gamble', 'procter&gamble']}}, {'vz': {'alias': ['$vz', 'vz', 'verizon communications inc', 'verizon']}}, {'wmt': {'alias': ['$wmt', 'wmt', 'walmart stores inc', 'walmart stores', 'walmart']}}, {'unh': {'alias': ['$unh', 'unh', 'unitedhealth group inc', 'unitedhealth group', 'unitedhealthgroup']}}, {'trv': {'alias': ['$trv', 'trv', 'travelers companies inc', 'travelers companies', 'travelers', 'travelerscompanies']}}, {'mcd': {'alias': ['$mcd', 'mcd', 'mcdonalds corp', 'mcdonalds']}}, {'mmm': {'alias': ['$mmm', 'mmm', '3m', '3m']}}, {'nke': {'alias': ['$nke', 'nke', 'nike inc class b', 'nike']}}, {'mrk': {'alias': ['$mrk', 'mrk', 'merck & co inc', 'merck & co', 'merck&co']}}, {'msft': {'alias': ['$msft', 'msft', 'microsoft corp', 'microsoft']}}, {'jpm': {'alias': ['$jpm', 'jpm', 'jp morgan chase & co', 'jp morgan chase', 'jpmorgan', ' j.p. morgan', ' jp morgan', 'jpm']}}, {'ko': {'alias': ['$ko', 'ko', 'coca-cola', 'cocacola']}}, {'jnj': {'alias': ['$jnj', 'jnj', 'johnson & johnson', 'johnson&johnson']}}, {'gs': {'alias': ['$gs', 'gs', 'goldman sachs group inc', 'goldman sachs group', 'goldman sachs', 'goldman']}}, {'hd': {'alias': ['$hd', 'hd', 'home depot inc', 'home depot']}}, {'hon': {'alias': ['$hon', 'hon', 'honeywell international inc', 'honeywell international', 'honeywell']}}, {'ibm': {'alias': ['$ibm', 'ibm', 'international business machines co', 'international business machines', 'internationalbusinessmachines']}}, {'intc': {'alias': ['$intc', 'intc', 'intel corporation corp', 'intel corporation', 'intel']}}, {'dis': {'alias': ['$dis', 'dis', 'walt disney', 'disney', 'waltdisney']}}, {'cat': {'alias': ['$cat', 'cat', 'caterpillar inc', 'caterpillar inc', 'caterpillarinc']}}, {'csco': {'alias': ['$csco', 'csco', 'cisco systems inc', 'cisco systems', 'cisco']}}, {'axp': {'alias': ['$axp', 'axp', 'american express', 'american express', 'amex', 'americanexpress']}}, {'ba': {'alias': ['$ba', 'ba', 'boeing', 'boeing']}}, {'amgn': {'alias': ['$amgn', 'amgn', 'amgen inc', 'amgen']}}, {'aapl': {'alias': ['$aapl', 'aapl', 'apple inc', 'apple inc', 'appleinc']}}], 'data_root': './data', 'start_date': datetime.date(2016, 10, 1), 'end_date': datetime.date(2021, 10, 1), 'data_pipeline_config': {'components': ['sec'], 'redownload': False}, 'twitter_conifg': {'sleep_time': 5}, 'etf_config': {'external_path': './data/external/etf'}, 'sec_config': {'parser_config': {'directory': './data/raw/sec', 'amount': 5, 'items': ['1A', '1B', '7A', '8'], 'external_fp': './data/external/sec'}, 'emb_config': {'top_k': 20, 'model_name': 'all-mpnet-base-v2'}}, 'news_config': {'sleep_time': 2, 'nyt_key': 'MN3m4QX4XDbGsaBFDBpJG4waHymmqZ3O', 'base_url': 'https://api.nytimes.com/svc/search/v2/articlesearch.json?', 'search_keyword': ['3M', 'American+Express', 'Amgen', 'Apple', 'Boeing', 'Caterpillar', 'Chevron', 'Cisco', 'Coca+Cola', 'Dow', 'Goldman+Sachs', 'Home+Depot', 'Honeywell', 'IBM', 'Intel', 'Johnson+Johnson', 'JPMorgan', 'McDonald', 'Merck', 'Microsoft', 'Nike', 'Procter+Gamble', 'Salesforce', 'Travelers', 'UnitedHealth', 'Verizon', 'Visa', 'Walgreens', 'Walmart', 'Disney']}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open('../configs/dow_jones.yaml') as f:\n",
    "    \n",
    "    data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix(df):\n",
    "    companies = [list(com.keys())[0] for com in data['companies']]\n",
    "    alias = list(map(lambda x: list(x.items())[0][1][\"alias\"], data['companies']))\n",
    "    res = pd.DataFrame(0, index=companies, columns=companies)\n",
    "    for company1, search_items1 in zip(companies, alias):\n",
    "        for company2, search_items2 in zip(companies, alias):\n",
    "            if company1 != company2:\n",
    "                search_items = search_items1 + search_items2\n",
    "            else:\n",
    "                search_items = search_items1\n",
    "            pat = \"|\".join(search_items)\n",
    "            res[company1][company2] += df.texts.str.contains(\n",
    "                pat\n",
    "            ).sum()\n",
    "    return res.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_list = [get_matrix(df) for df in df_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "week = 0\n",
    "for i in range(261):\n",
    "    np.save('../data/raw/news/week_'+str(week)+'.npy', mat_list[i])\n",
    "    week += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "stock_df = pd.read_csv(\n",
    "            os.path.join('../data/raw',\"stock\",\"raw.csv\"),\n",
    "            usecols=[\"ticker_symbol\", \"Date\", \"Close\"],\n",
    "            parse_dates=[\"Date\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>ticker_symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-10-03</td>\n",
       "      <td>28.129999</td>\n",
       "      <td>aapl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-10-04</td>\n",
       "      <td>28.250000</td>\n",
       "      <td>aapl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-10-05</td>\n",
       "      <td>28.262501</td>\n",
       "      <td>aapl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-10-06</td>\n",
       "      <td>28.472500</td>\n",
       "      <td>aapl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-10-07</td>\n",
       "      <td>28.514999</td>\n",
       "      <td>aapl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36506</th>\n",
       "      <td>2021-09-27</td>\n",
       "      <td>142.250000</td>\n",
       "      <td>wmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36507</th>\n",
       "      <td>2021-09-28</td>\n",
       "      <td>140.500000</td>\n",
       "      <td>wmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36508</th>\n",
       "      <td>2021-09-29</td>\n",
       "      <td>140.440002</td>\n",
       "      <td>wmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36509</th>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>139.380005</td>\n",
       "      <td>wmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36510</th>\n",
       "      <td>2021-10-01</td>\n",
       "      <td>137.050003</td>\n",
       "      <td>wmt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36511 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Close ticker_symbol\n",
       "0     2016-10-03   28.129999          aapl\n",
       "1     2016-10-04   28.250000          aapl\n",
       "2     2016-10-05   28.262501          aapl\n",
       "3     2016-10-06   28.472500          aapl\n",
       "4     2016-10-07   28.514999          aapl\n",
       "...          ...         ...           ...\n",
       "36506 2021-09-27  142.250000           wmt\n",
       "36507 2021-09-28  140.500000           wmt\n",
       "36508 2021-09-29  140.440002           wmt\n",
       "36509 2021-09-30  139.380005           wmt\n",
       "36510 2021-10-01  137.050003           wmt\n",
       "\n",
       "[36511 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.strptime('2016-10-02', \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = [start]\n",
    "cur = start\n",
    "for i in range(261):\n",
    "    cur = cur + timedelta(days=7)\n",
    "    date_list.append(cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2016, 10, 2, 0, 0),\n",
       " datetime.datetime(2016, 10, 9, 0, 0),\n",
       " datetime.datetime(2016, 10, 16, 0, 0),\n",
       " datetime.datetime(2016, 10, 23, 0, 0),\n",
       " datetime.datetime(2016, 10, 30, 0, 0),\n",
       " datetime.datetime(2016, 11, 6, 0, 0),\n",
       " datetime.datetime(2016, 11, 13, 0, 0),\n",
       " datetime.datetime(2016, 11, 20, 0, 0),\n",
       " datetime.datetime(2016, 11, 27, 0, 0),\n",
       " datetime.datetime(2016, 12, 4, 0, 0),\n",
       " datetime.datetime(2016, 12, 11, 0, 0),\n",
       " datetime.datetime(2016, 12, 18, 0, 0),\n",
       " datetime.datetime(2016, 12, 25, 0, 0),\n",
       " datetime.datetime(2017, 1, 1, 0, 0),\n",
       " datetime.datetime(2017, 1, 8, 0, 0),\n",
       " datetime.datetime(2017, 1, 15, 0, 0),\n",
       " datetime.datetime(2017, 1, 22, 0, 0),\n",
       " datetime.datetime(2017, 1, 29, 0, 0),\n",
       " datetime.datetime(2017, 2, 5, 0, 0),\n",
       " datetime.datetime(2017, 2, 12, 0, 0),\n",
       " datetime.datetime(2017, 2, 19, 0, 0),\n",
       " datetime.datetime(2017, 2, 26, 0, 0),\n",
       " datetime.datetime(2017, 3, 5, 0, 0),\n",
       " datetime.datetime(2017, 3, 12, 0, 0),\n",
       " datetime.datetime(2017, 3, 19, 0, 0),\n",
       " datetime.datetime(2017, 3, 26, 0, 0),\n",
       " datetime.datetime(2017, 4, 2, 0, 0),\n",
       " datetime.datetime(2017, 4, 9, 0, 0),\n",
       " datetime.datetime(2017, 4, 16, 0, 0),\n",
       " datetime.datetime(2017, 4, 23, 0, 0),\n",
       " datetime.datetime(2017, 4, 30, 0, 0),\n",
       " datetime.datetime(2017, 5, 7, 0, 0),\n",
       " datetime.datetime(2017, 5, 14, 0, 0),\n",
       " datetime.datetime(2017, 5, 21, 0, 0),\n",
       " datetime.datetime(2017, 5, 28, 0, 0),\n",
       " datetime.datetime(2017, 6, 4, 0, 0),\n",
       " datetime.datetime(2017, 6, 11, 0, 0),\n",
       " datetime.datetime(2017, 6, 18, 0, 0),\n",
       " datetime.datetime(2017, 6, 25, 0, 0),\n",
       " datetime.datetime(2017, 7, 2, 0, 0),\n",
       " datetime.datetime(2017, 7, 9, 0, 0),\n",
       " datetime.datetime(2017, 7, 16, 0, 0),\n",
       " datetime.datetime(2017, 7, 23, 0, 0),\n",
       " datetime.datetime(2017, 7, 30, 0, 0),\n",
       " datetime.datetime(2017, 8, 6, 0, 0),\n",
       " datetime.datetime(2017, 8, 13, 0, 0),\n",
       " datetime.datetime(2017, 8, 20, 0, 0),\n",
       " datetime.datetime(2017, 8, 27, 0, 0),\n",
       " datetime.datetime(2017, 9, 3, 0, 0),\n",
       " datetime.datetime(2017, 9, 10, 0, 0),\n",
       " datetime.datetime(2017, 9, 17, 0, 0),\n",
       " datetime.datetime(2017, 9, 24, 0, 0),\n",
       " datetime.datetime(2017, 10, 1, 0, 0),\n",
       " datetime.datetime(2017, 10, 8, 0, 0),\n",
       " datetime.datetime(2017, 10, 15, 0, 0),\n",
       " datetime.datetime(2017, 10, 22, 0, 0),\n",
       " datetime.datetime(2017, 10, 29, 0, 0),\n",
       " datetime.datetime(2017, 11, 5, 0, 0),\n",
       " datetime.datetime(2017, 11, 12, 0, 0),\n",
       " datetime.datetime(2017, 11, 19, 0, 0),\n",
       " datetime.datetime(2017, 11, 26, 0, 0),\n",
       " datetime.datetime(2017, 12, 3, 0, 0),\n",
       " datetime.datetime(2017, 12, 10, 0, 0),\n",
       " datetime.datetime(2017, 12, 17, 0, 0),\n",
       " datetime.datetime(2017, 12, 24, 0, 0),\n",
       " datetime.datetime(2017, 12, 31, 0, 0),\n",
       " datetime.datetime(2018, 1, 7, 0, 0),\n",
       " datetime.datetime(2018, 1, 14, 0, 0),\n",
       " datetime.datetime(2018, 1, 21, 0, 0),\n",
       " datetime.datetime(2018, 1, 28, 0, 0),\n",
       " datetime.datetime(2018, 2, 4, 0, 0),\n",
       " datetime.datetime(2018, 2, 11, 0, 0),\n",
       " datetime.datetime(2018, 2, 18, 0, 0),\n",
       " datetime.datetime(2018, 2, 25, 0, 0),\n",
       " datetime.datetime(2018, 3, 4, 0, 0),\n",
       " datetime.datetime(2018, 3, 11, 0, 0),\n",
       " datetime.datetime(2018, 3, 18, 0, 0),\n",
       " datetime.datetime(2018, 3, 25, 0, 0),\n",
       " datetime.datetime(2018, 4, 1, 0, 0),\n",
       " datetime.datetime(2018, 4, 8, 0, 0),\n",
       " datetime.datetime(2018, 4, 15, 0, 0),\n",
       " datetime.datetime(2018, 4, 22, 0, 0),\n",
       " datetime.datetime(2018, 4, 29, 0, 0),\n",
       " datetime.datetime(2018, 5, 6, 0, 0),\n",
       " datetime.datetime(2018, 5, 13, 0, 0),\n",
       " datetime.datetime(2018, 5, 20, 0, 0),\n",
       " datetime.datetime(2018, 5, 27, 0, 0),\n",
       " datetime.datetime(2018, 6, 3, 0, 0),\n",
       " datetime.datetime(2018, 6, 10, 0, 0),\n",
       " datetime.datetime(2018, 6, 17, 0, 0),\n",
       " datetime.datetime(2018, 6, 24, 0, 0),\n",
       " datetime.datetime(2018, 7, 1, 0, 0),\n",
       " datetime.datetime(2018, 7, 8, 0, 0),\n",
       " datetime.datetime(2018, 7, 15, 0, 0),\n",
       " datetime.datetime(2018, 7, 22, 0, 0),\n",
       " datetime.datetime(2018, 7, 29, 0, 0),\n",
       " datetime.datetime(2018, 8, 5, 0, 0),\n",
       " datetime.datetime(2018, 8, 12, 0, 0),\n",
       " datetime.datetime(2018, 8, 19, 0, 0),\n",
       " datetime.datetime(2018, 8, 26, 0, 0),\n",
       " datetime.datetime(2018, 9, 2, 0, 0),\n",
       " datetime.datetime(2018, 9, 9, 0, 0),\n",
       " datetime.datetime(2018, 9, 16, 0, 0),\n",
       " datetime.datetime(2018, 9, 23, 0, 0),\n",
       " datetime.datetime(2018, 9, 30, 0, 0),\n",
       " datetime.datetime(2018, 10, 7, 0, 0),\n",
       " datetime.datetime(2018, 10, 14, 0, 0),\n",
       " datetime.datetime(2018, 10, 21, 0, 0),\n",
       " datetime.datetime(2018, 10, 28, 0, 0),\n",
       " datetime.datetime(2018, 11, 4, 0, 0),\n",
       " datetime.datetime(2018, 11, 11, 0, 0),\n",
       " datetime.datetime(2018, 11, 18, 0, 0),\n",
       " datetime.datetime(2018, 11, 25, 0, 0),\n",
       " datetime.datetime(2018, 12, 2, 0, 0),\n",
       " datetime.datetime(2018, 12, 9, 0, 0),\n",
       " datetime.datetime(2018, 12, 16, 0, 0),\n",
       " datetime.datetime(2018, 12, 23, 0, 0),\n",
       " datetime.datetime(2018, 12, 30, 0, 0),\n",
       " datetime.datetime(2019, 1, 6, 0, 0),\n",
       " datetime.datetime(2019, 1, 13, 0, 0),\n",
       " datetime.datetime(2019, 1, 20, 0, 0),\n",
       " datetime.datetime(2019, 1, 27, 0, 0),\n",
       " datetime.datetime(2019, 2, 3, 0, 0),\n",
       " datetime.datetime(2019, 2, 10, 0, 0),\n",
       " datetime.datetime(2019, 2, 17, 0, 0),\n",
       " datetime.datetime(2019, 2, 24, 0, 0),\n",
       " datetime.datetime(2019, 3, 3, 0, 0),\n",
       " datetime.datetime(2019, 3, 10, 0, 0),\n",
       " datetime.datetime(2019, 3, 17, 0, 0),\n",
       " datetime.datetime(2019, 3, 24, 0, 0),\n",
       " datetime.datetime(2019, 3, 31, 0, 0),\n",
       " datetime.datetime(2019, 4, 7, 0, 0),\n",
       " datetime.datetime(2019, 4, 14, 0, 0),\n",
       " datetime.datetime(2019, 4, 21, 0, 0),\n",
       " datetime.datetime(2019, 4, 28, 0, 0),\n",
       " datetime.datetime(2019, 5, 5, 0, 0),\n",
       " datetime.datetime(2019, 5, 12, 0, 0),\n",
       " datetime.datetime(2019, 5, 19, 0, 0),\n",
       " datetime.datetime(2019, 5, 26, 0, 0),\n",
       " datetime.datetime(2019, 6, 2, 0, 0),\n",
       " datetime.datetime(2019, 6, 9, 0, 0),\n",
       " datetime.datetime(2019, 6, 16, 0, 0),\n",
       " datetime.datetime(2019, 6, 23, 0, 0),\n",
       " datetime.datetime(2019, 6, 30, 0, 0),\n",
       " datetime.datetime(2019, 7, 7, 0, 0),\n",
       " datetime.datetime(2019, 7, 14, 0, 0),\n",
       " datetime.datetime(2019, 7, 21, 0, 0),\n",
       " datetime.datetime(2019, 7, 28, 0, 0),\n",
       " datetime.datetime(2019, 8, 4, 0, 0),\n",
       " datetime.datetime(2019, 8, 11, 0, 0),\n",
       " datetime.datetime(2019, 8, 18, 0, 0),\n",
       " datetime.datetime(2019, 8, 25, 0, 0),\n",
       " datetime.datetime(2019, 9, 1, 0, 0),\n",
       " datetime.datetime(2019, 9, 8, 0, 0),\n",
       " datetime.datetime(2019, 9, 15, 0, 0),\n",
       " datetime.datetime(2019, 9, 22, 0, 0),\n",
       " datetime.datetime(2019, 9, 29, 0, 0),\n",
       " datetime.datetime(2019, 10, 6, 0, 0),\n",
       " datetime.datetime(2019, 10, 13, 0, 0),\n",
       " datetime.datetime(2019, 10, 20, 0, 0),\n",
       " datetime.datetime(2019, 10, 27, 0, 0),\n",
       " datetime.datetime(2019, 11, 3, 0, 0),\n",
       " datetime.datetime(2019, 11, 10, 0, 0),\n",
       " datetime.datetime(2019, 11, 17, 0, 0),\n",
       " datetime.datetime(2019, 11, 24, 0, 0),\n",
       " datetime.datetime(2019, 12, 1, 0, 0),\n",
       " datetime.datetime(2019, 12, 8, 0, 0),\n",
       " datetime.datetime(2019, 12, 15, 0, 0),\n",
       " datetime.datetime(2019, 12, 22, 0, 0),\n",
       " datetime.datetime(2019, 12, 29, 0, 0),\n",
       " datetime.datetime(2020, 1, 5, 0, 0),\n",
       " datetime.datetime(2020, 1, 12, 0, 0),\n",
       " datetime.datetime(2020, 1, 19, 0, 0),\n",
       " datetime.datetime(2020, 1, 26, 0, 0),\n",
       " datetime.datetime(2020, 2, 2, 0, 0),\n",
       " datetime.datetime(2020, 2, 9, 0, 0),\n",
       " datetime.datetime(2020, 2, 16, 0, 0),\n",
       " datetime.datetime(2020, 2, 23, 0, 0),\n",
       " datetime.datetime(2020, 3, 1, 0, 0),\n",
       " datetime.datetime(2020, 3, 8, 0, 0),\n",
       " datetime.datetime(2020, 3, 15, 0, 0),\n",
       " datetime.datetime(2020, 3, 22, 0, 0),\n",
       " datetime.datetime(2020, 3, 29, 0, 0),\n",
       " datetime.datetime(2020, 4, 5, 0, 0),\n",
       " datetime.datetime(2020, 4, 12, 0, 0),\n",
       " datetime.datetime(2020, 4, 19, 0, 0),\n",
       " datetime.datetime(2020, 4, 26, 0, 0),\n",
       " datetime.datetime(2020, 5, 3, 0, 0),\n",
       " datetime.datetime(2020, 5, 10, 0, 0),\n",
       " datetime.datetime(2020, 5, 17, 0, 0),\n",
       " datetime.datetime(2020, 5, 24, 0, 0),\n",
       " datetime.datetime(2020, 5, 31, 0, 0),\n",
       " datetime.datetime(2020, 6, 7, 0, 0),\n",
       " datetime.datetime(2020, 6, 14, 0, 0),\n",
       " datetime.datetime(2020, 6, 21, 0, 0),\n",
       " datetime.datetime(2020, 6, 28, 0, 0),\n",
       " datetime.datetime(2020, 7, 5, 0, 0),\n",
       " datetime.datetime(2020, 7, 12, 0, 0),\n",
       " datetime.datetime(2020, 7, 19, 0, 0),\n",
       " datetime.datetime(2020, 7, 26, 0, 0),\n",
       " datetime.datetime(2020, 8, 2, 0, 0),\n",
       " datetime.datetime(2020, 8, 9, 0, 0),\n",
       " datetime.datetime(2020, 8, 16, 0, 0),\n",
       " datetime.datetime(2020, 8, 23, 0, 0),\n",
       " datetime.datetime(2020, 8, 30, 0, 0),\n",
       " datetime.datetime(2020, 9, 6, 0, 0),\n",
       " datetime.datetime(2020, 9, 13, 0, 0),\n",
       " datetime.datetime(2020, 9, 20, 0, 0),\n",
       " datetime.datetime(2020, 9, 27, 0, 0),\n",
       " datetime.datetime(2020, 10, 4, 0, 0),\n",
       " datetime.datetime(2020, 10, 11, 0, 0),\n",
       " datetime.datetime(2020, 10, 18, 0, 0),\n",
       " datetime.datetime(2020, 10, 25, 0, 0),\n",
       " datetime.datetime(2020, 11, 1, 0, 0),\n",
       " datetime.datetime(2020, 11, 8, 0, 0),\n",
       " datetime.datetime(2020, 11, 15, 0, 0),\n",
       " datetime.datetime(2020, 11, 22, 0, 0),\n",
       " datetime.datetime(2020, 11, 29, 0, 0),\n",
       " datetime.datetime(2020, 12, 6, 0, 0),\n",
       " datetime.datetime(2020, 12, 13, 0, 0),\n",
       " datetime.datetime(2020, 12, 20, 0, 0),\n",
       " datetime.datetime(2020, 12, 27, 0, 0),\n",
       " datetime.datetime(2021, 1, 3, 0, 0),\n",
       " datetime.datetime(2021, 1, 10, 0, 0),\n",
       " datetime.datetime(2021, 1, 17, 0, 0),\n",
       " datetime.datetime(2021, 1, 24, 0, 0),\n",
       " datetime.datetime(2021, 1, 31, 0, 0),\n",
       " datetime.datetime(2021, 2, 7, 0, 0),\n",
       " datetime.datetime(2021, 2, 14, 0, 0),\n",
       " datetime.datetime(2021, 2, 21, 0, 0),\n",
       " datetime.datetime(2021, 2, 28, 0, 0),\n",
       " datetime.datetime(2021, 3, 7, 0, 0),\n",
       " datetime.datetime(2021, 3, 14, 0, 0),\n",
       " datetime.datetime(2021, 3, 21, 0, 0),\n",
       " datetime.datetime(2021, 3, 28, 0, 0),\n",
       " datetime.datetime(2021, 4, 4, 0, 0),\n",
       " datetime.datetime(2021, 4, 11, 0, 0),\n",
       " datetime.datetime(2021, 4, 18, 0, 0),\n",
       " datetime.datetime(2021, 4, 25, 0, 0),\n",
       " datetime.datetime(2021, 5, 2, 0, 0),\n",
       " datetime.datetime(2021, 5, 9, 0, 0),\n",
       " datetime.datetime(2021, 5, 16, 0, 0),\n",
       " datetime.datetime(2021, 5, 23, 0, 0),\n",
       " datetime.datetime(2021, 5, 30, 0, 0),\n",
       " datetime.datetime(2021, 6, 6, 0, 0),\n",
       " datetime.datetime(2021, 6, 13, 0, 0),\n",
       " datetime.datetime(2021, 6, 20, 0, 0),\n",
       " datetime.datetime(2021, 6, 27, 0, 0),\n",
       " datetime.datetime(2021, 7, 4, 0, 0),\n",
       " datetime.datetime(2021, 7, 11, 0, 0),\n",
       " datetime.datetime(2021, 7, 18, 0, 0),\n",
       " datetime.datetime(2021, 7, 25, 0, 0),\n",
       " datetime.datetime(2021, 8, 1, 0, 0),\n",
       " datetime.datetime(2021, 8, 8, 0, 0),\n",
       " datetime.datetime(2021, 8, 15, 0, 0),\n",
       " datetime.datetime(2021, 8, 22, 0, 0),\n",
       " datetime.datetime(2021, 8, 29, 0, 0),\n",
       " datetime.datetime(2021, 9, 5, 0, 0),\n",
       " datetime.datetime(2021, 9, 12, 0, 0),\n",
       " datetime.datetime(2021, 9, 19, 0, 0),\n",
       " datetime.datetime(2021, 9, 26, 0, 0),\n",
       " datetime.datetime(2021, 10, 3, 0, 0)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 768])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_emb = []\n",
    "for fp in sorted(os.listdir(\"../data/raw/sec/\")):\n",
    "    full_path = os.path.join(\"../data/raw\", \"sec\", fp)\n",
    "    if fp.split(\".\")[-1]=='npy':\n",
    "        comp_emb.append(torch.from_numpy(np.load(full_path)))\n",
    "comp_emb = torch.stack(comp_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n"
     ]
    }
   ],
   "source": [
    "X_y = []\n",
    "for i in range(260):\n",
    "    start_date = date_list[i]\n",
    "    end_date = start_date+timedelta(days=6)\n",
    "    next_start_date = start_date+timedelta(days=7)\n",
    "    next_end_date = start_date+timedelta(days=13)\n",
    "    ######################################################## \n",
    "    # prepare X (change this if you want to add SEC emb, etc.)\n",
    "    ########################################################\n",
    "    curr = stock_df[(stock_df.Date>=start_date) & (stock_df.Date<=end_date)]\n",
    "    X = curr.pivot_table(\n",
    "            index=\"Date\", columns=\"ticker_symbol\", values=\"Close\"\n",
    "        ).values.T\n",
    "    X_tensor = torch.tensor(X)\n",
    "    \n",
    "    ########################################################\n",
    "    # prepare y (change this if you want to change labels)\n",
    "    ########################################################\n",
    "\n",
    "    \n",
    "    nxt = stock_df[(stock_df.Date>=next_start_date) & (stock_df.Date<=next_end_date)]\n",
    "    y = nxt.pivot_table(\n",
    "            index=\"Date\", columns=\"ticker_symbol\", values=\"Close\"\n",
    "        ).values.T\n",
    "    y = (y.mean(1) - X.mean(1)) / X.mean(1)\n",
    "    y_tensor = torch.tensor(y)\n",
    "    X_y.append((X_tensor,y_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\n",
    "\n",
    "loader = ChickenpoxDatasetLoader()\n",
    "\n",
    "dataset = loader.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/processed/twitter/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter = ['2016_q4']\n",
    "for i in range(2017, 2022):\n",
    "    for j in range(1, 5):\n",
    "        if i == 2021 and j == 4: break\n",
    "        quarter.append(str(i)+'_q'+str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016_q4',\n",
       " '2017_q1',\n",
       " '2017_q2',\n",
       " '2017_q3',\n",
       " '2017_q4',\n",
       " '2018_q1',\n",
       " '2018_q2',\n",
       " '2018_q3',\n",
       " '2018_q4',\n",
       " '2019_q1',\n",
       " '2019_q2',\n",
       " '2019_q3',\n",
       " '2019_q4',\n",
       " '2020_q1',\n",
       " '2020_q2',\n",
       " '2020_q3',\n",
       " '2020_q4',\n",
       " '2021_q1',\n",
       " '2021_q2',\n",
       " '2021_q3']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for i in quarter:\n",
    "    paths.append(path+i+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/processed/twitter/2016_q4.pt',\n",
       " '../data/processed/twitter/2017_q1.pt',\n",
       " '../data/processed/twitter/2017_q2.pt',\n",
       " '../data/processed/twitter/2017_q3.pt',\n",
       " '../data/processed/twitter/2017_q4.pt',\n",
       " '../data/processed/twitter/2018_q1.pt',\n",
       " '../data/processed/twitter/2018_q2.pt',\n",
       " '../data/processed/twitter/2018_q3.pt',\n",
       " '../data/processed/twitter/2018_q4.pt',\n",
       " '../data/processed/twitter/2019_q1.pt',\n",
       " '../data/processed/twitter/2019_q2.pt',\n",
       " '../data/processed/twitter/2019_q3.pt',\n",
       " '../data/processed/twitter/2019_q4.pt',\n",
       " '../data/processed/twitter/2020_q1.pt',\n",
       " '../data/processed/twitter/2020_q2.pt',\n",
       " '../data/processed/twitter/2020_q3.pt',\n",
       " '../data/processed/twitter/2020_q4.pt',\n",
       " '../data/processed/twitter/2021_q1.pt',\n",
       " '../data/processed/twitter/2021_q2.pt',\n",
       " '../data/processed/twitter/2021_q3.pt']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data(x=[29, 63], edge_index=[2, 760], edge_attr=[760], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 760\n",
      "Average node degree: 26.21\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 62], edge_index=[2, 312], edge_attr=[312], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 312\n",
      "Average node degree: 10.76\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 63], edge_index=[2, 400], edge_attr=[400], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 400\n",
      "Average node degree: 13.79\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 63], edge_index=[2, 552], edge_attr=[552], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 552\n",
      "Average node degree: 19.03\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 63], edge_index=[2, 805], edge_attr=[805], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 805\n",
      "Average node degree: 27.76\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 61], edge_index=[2, 400], edge_attr=[400], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 400\n",
      "Average node degree: 13.79\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 64], edge_index=[2, 616], edge_attr=[616], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 616\n",
      "Average node degree: 21.24\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 63], edge_index=[2, 585], edge_attr=[585], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 585\n",
      "Average node degree: 20.17\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 63], edge_index=[2, 805], edge_attr=[805], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 805\n",
      "Average node degree: 27.76\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 61], edge_index=[2, 552], edge_attr=[552], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 552\n",
      "Average node degree: 19.03\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 63], edge_index=[2, 552], edge_attr=[552], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 552\n",
      "Average node degree: 19.03\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 64], edge_index=[2, 672], edge_attr=[672], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 672\n",
      "Average node degree: 23.17\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 64], edge_index=[2, 777], edge_attr=[777], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 777\n",
      "Average node degree: 26.79\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 62], edge_index=[2, 480], edge_attr=[480], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 480\n",
      "Average node degree: 16.55\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 63], edge_index=[2, 585], edge_attr=[585], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 585\n",
      "Average node degree: 20.17\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 64], edge_index=[2, 697], edge_attr=[697], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 697\n",
      "Average node degree: 24.03\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 64], edge_index=[2, 805], edge_attr=[805], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 805\n",
      "Average node degree: 27.76\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 61], edge_index=[2, 616], edge_attr=[616], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 616\n",
      "Average node degree: 21.24\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 63], edge_index=[2, 720], edge_attr=[720], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 720\n",
      "Average node degree: 24.83\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 64], edge_index=[2, 741], edge_attr=[741], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 741\n",
      "Average node degree: 25.55\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "for path in paths:\n",
    "    dataset = torch.load(path)\n",
    "    data = dataset[0]\n",
    "    data_list.append(transform_and_split(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 62])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[1].x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "edge_indices = [i.edge_index.double() for i in data_list]\n",
    "edge_weights = [i.edge_attr.double() for i in data_list]\n",
    "features = [i.x.double() for i in data_list]\n",
    "targets = [i.y.double() for i in data_list]\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "edge_indices = [i.edge_index.cpu().detach().numpy() for i in data_list]\n",
    "edge_weights = [i.edge_attr.cpu().detach().numpy() for i in data_list]\n",
    "features = [i.x.cpu().detach().numpy() for i in data_list]\n",
    "targets = [i.y.cpu().detach().numpy() for i in data_list]\n",
    "\"\"\"\n",
    "edge_indices = [i.edge_index.numpy() for i in data_list]\n",
    "edge_weights = [i.edge_attr.numpy() for i in data_list]\n",
    "features = [i.x.numpy() for i in data_list]\n",
    "targets = [i.y.numpy() for i in data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 2,  3,  4, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,\n",
       "          2,  2,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,\n",
       "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
       "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10,\n",
       "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12,\n",
       "         12, 12, 12, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15,\n",
       "         15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17,\n",
       "         17, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 20, 20, 20,\n",
       "         20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "         21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23,\n",
       "         23, 23, 23, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25,\n",
       "         25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "         25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27,\n",
       "         27, 27, 28, 28, 28, 28, 28, 28],\n",
       "        [ 4,  9, 10, 21, 22, 25,  4,  9, 10, 21, 22, 25,  4,  9, 10, 21,\n",
       "         22, 25,  4,  9, 10, 21, 22, 25,  0,  1,  2,  3,  4,  5,  6,  7,\n",
       "          8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
       "         24, 25, 26, 27, 28,  4,  9, 10, 21, 22, 25,  4,  9, 10, 21, 22,\n",
       "         25,  4,  9, 10, 21, 22, 25,  4,  9, 10, 21, 22, 25,  0,  1,  2,\n",
       "          3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  0,  1,  2,  3,  4,  5,\n",
       "          6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "         22, 23, 24, 25, 26, 27, 28,  4,  9, 10, 21, 22, 25,  4,  9, 10,\n",
       "         21, 22, 25,  4,  9, 10, 21, 22, 25,  4,  9, 10, 21, 22, 25,  4,\n",
       "          9, 10, 21, 22, 25,  4,  9, 10, 21, 22, 25,  4,  9, 10, 21, 22,\n",
       "         25,  4,  9, 10, 21, 22, 25,  4,  9, 10, 21, 22, 25,  4,  9, 10,\n",
       "         21, 22, 25,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
       "         13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,\n",
       "          0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "         16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  4,  9, 10,\n",
       "         21, 22, 25,  4,  9, 10, 21, 22, 25,  0,  1,  2,  3,  4,  5,  6,\n",
       "          7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "         23, 24, 25, 26, 27, 28,  4,  9, 10, 21, 22, 25,  4,  9, 10, 21,\n",
       "         22, 25,  4,  9, 10, 21, 22, 25]], dtype=int64),\n",
       " array([[ 0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "          3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,\n",
       "          6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "          7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,\n",
       "          8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11,\n",
       "         11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13,\n",
       "         13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15,\n",
       "         15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "         16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "         16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18,\n",
       "         18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22,\n",
       "         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "         22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24,\n",
       "         24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "         25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "         25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
       "         26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28],\n",
       "        [ 3,  4,  7,  9, 16, 22, 25, 26,  3,  4,  7,  9, 16, 22, 25, 26,\n",
       "          3,  4,  7,  9, 16, 22, 25, 26,  0,  1,  2,  3,  4,  5,  6,  7,\n",
       "          8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
       "         24, 25, 26, 27, 28,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,\n",
       "         11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "         27, 28,  3,  4,  7,  9, 16, 22, 25, 26,  3,  4,  7,  9, 16, 22,\n",
       "         25, 26,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "         14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  3,\n",
       "          4,  7,  9, 16, 22, 25, 26,  0,  1,  2,  3,  4,  5,  6,  7,  8,\n",
       "          9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "         25, 26, 27, 28,  3,  4,  7,  9, 16, 22, 25, 26,  3,  4,  7,  9,\n",
       "         16, 22, 25, 26,  3,  4,  7,  9, 16, 22, 25, 26,  3,  4,  7,  9,\n",
       "         16, 22, 25, 26,  3,  4,  7,  9, 16, 22, 25, 26,  3,  4,  7,  9,\n",
       "         16, 22, 25, 26,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11,\n",
       "         12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "         28,  3,  4,  7,  9, 16, 22, 25, 26,  3,  4,  7,  9, 16, 22, 25,\n",
       "         26,  3,  4,  7,  9, 16, 22, 25, 26,  3,  4,  7,  9, 16, 22, 25,\n",
       "         26,  3,  4,  7,  9, 16, 22, 25, 26,  0,  1,  2,  3,  4,  5,  6,\n",
       "          7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "         23, 24, 25, 26, 27, 28,  3,  4,  7,  9, 16, 22, 25, 26,  3,  4,\n",
       "          7,  9, 16, 22, 25, 26,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9,\n",
       "         10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,\n",
       "         26, 27, 28,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
       "         13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,\n",
       "          3,  4,  7,  9, 16, 22, 25, 26,  3,  4,  7,  9, 16, 22, 25, 26]],\n",
       "       dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 3,  4,  5, ..., 22, 25, 26]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 0,  1,  2, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "          3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,\n",
       "          6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,\n",
       "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "          8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
       "         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14,\n",
       "         14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16,\n",
       "         16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18,\n",
       "         18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22,\n",
       "         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23,\n",
       "         23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25,\n",
       "         25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "         25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28],\n",
       "        [ 3,  4,  8,  9, 10, 13, 22, 25,  3,  4,  8,  9, 10, 13, 22, 25,\n",
       "          3,  4,  8,  9, 10, 13, 22, 25,  0,  1,  2,  3,  4,  5,  6,  7,\n",
       "          8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
       "         24, 25, 26, 27, 28,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,\n",
       "         11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "         27, 28,  3,  4,  8,  9, 10, 13, 22, 25,  3,  4,  8,  9, 10, 13,\n",
       "         22, 25,  3,  4,  8,  9, 10, 13, 22, 25,  0,  1,  2,  3,  4,  5,\n",
       "          6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "         22, 23, 24, 25, 26, 27, 28,  0,  1,  2,  3,  4,  5,  6,  7,  8,\n",
       "          9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "         25, 26, 27, 28,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11,\n",
       "         12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "         28,  3,  4,  8,  9, 10, 13, 22, 25,  3,  4,  8,  9, 10, 13, 22,\n",
       "         25,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
       "         15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  3,  4,\n",
       "          8,  9, 10, 13, 22, 25,  3,  4,  8,  9, 10, 13, 22, 25,  3,  4,\n",
       "          8,  9, 10, 13, 22, 25,  3,  4,  8,  9, 10, 13, 22, 25,  3,  4,\n",
       "          8,  9, 10, 13, 22, 25,  3,  4,  8,  9, 10, 13, 22, 25,  3,  4,\n",
       "          8,  9, 10, 13, 22, 25,  3,  4,  8,  9, 10, 13, 22, 25,  0,  1,\n",
       "          2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  3,  4,  8,  9, 10,\n",
       "         13, 22, 25,  3,  4,  8,  9, 10, 13, 22, 25,  0,  1,  2,  3,  4,\n",
       "          5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
       "         21, 22, 23, 24, 25, 26, 27, 28,  3,  4,  8,  9, 10, 13, 22, 25,\n",
       "          3,  4,  8,  9, 10, 13, 22, 25,  3,  4,  8,  9, 10, 13, 22, 25]],\n",
       "       dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 2,  3,  4, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 2,  3,  4, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 0,  1,  2, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 0,  1,  2, ..., 13, 16, 25]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 3,  4,  5, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 0,  1,  2, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 0,  1,  2, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,\n",
       "          4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,\n",
       "          5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "          6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "          6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,\n",
       "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "          8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14,\n",
       "         14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "         15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "         16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17,\n",
       "         17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18,\n",
       "         18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "         21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "         21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "         22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24,\n",
       "         24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "         25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "         25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28],\n",
       "        [ 2,  3,  4,  6,  8,  9, 16, 21, 22, 25,  2,  3,  4,  6,  8,  9,\n",
       "         16, 21, 22, 25,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11,\n",
       "         12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "         28,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
       "         15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  0,  1,\n",
       "          2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  2,  3,  4,  6,  8,\n",
       "          9, 16, 21, 22, 25,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,\n",
       "         11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "         27, 28,  2,  3,  4,  6,  8,  9, 16, 21, 22, 25,  0,  1,  2,  3,\n",
       "          4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "         20, 21, 22, 23, 24, 25, 26, 27, 28,  0,  1,  2,  3,  4,  5,  6,\n",
       "          7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "         23, 24, 25, 26, 27, 28,  2,  3,  4,  6,  8,  9, 16, 21, 22, 25,\n",
       "          2,  3,  4,  6,  8,  9, 16, 21, 22, 25,  2,  3,  4,  6,  8,  9,\n",
       "         16, 21, 22, 25,  2,  3,  4,  6,  8,  9, 16, 21, 22, 25,  2,  3,\n",
       "          4,  6,  8,  9, 16, 21, 22, 25,  2,  3,  4,  6,  8,  9, 16, 21,\n",
       "         22, 25,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "         14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  2,\n",
       "          3,  4,  6,  8,  9, 16, 21, 22, 25,  2,  3,  4,  6,  8,  9, 16,\n",
       "         21, 22, 25,  2,  3,  4,  6,  8,  9, 16, 21, 22, 25,  2,  3,  4,\n",
       "          6,  8,  9, 16, 21, 22, 25,  0,  1,  2,  3,  4,  5,  6,  7,  8,\n",
       "          9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "         25, 26, 27, 28,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11,\n",
       "         12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "         28,  2,  3,  4,  6,  8,  9, 16, 21, 22, 25,  2,  3,  4,  6,  8,\n",
       "          9, 16, 21, 22, 25,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,\n",
       "         11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "         27, 28,  2,  3,  4,  6,  8,  9, 16, 21, 22, 25,  2,  3,  4,  6,\n",
       "          8,  9, 16, 21, 22, 25,  2,  3,  4,  6,  8,  9, 16, 21, 22, 25]],\n",
       "       dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 3,  4,  5, ..., 21, 22, 25]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 3,  4,  5, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 0,  1,  2, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 3,  4,  7, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 0,  1,  2, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 0,  1,  2, ..., 26, 27, 28]], dtype=int64)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import dense_to_sparse\n",
    "edge_idx = []\n",
    "edge_att = []\n",
    "for i in range(260):\n",
    "    edge_index, edge_attr = dense_to_sparse(torch.from_numpy(mat_list[i]))\n",
    "    edge_idx.append(edge_index)\n",
    "    edge_att.append(edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_indices = [i.numpy() for i in edge_idx]\n",
    "edge_weights = [i.numpy() for i in edge_att]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "targets = []\n",
    "for i in range(260):\n",
    "    features.append(normalize(X_y[i][0].numpy(), axis=1, norm='max'))\n",
    "    #features.append(X_y[i][0].numpy())\n",
    "    targets.append([1 if a > 0 else 0 for a in X_y[i][1].numpy()])\n",
    "targets = np.asarray(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 29)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_list[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 5)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_features[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_features = []\n",
    "for i in features:\n",
    "    padded_features.append(np.pad(i, [(0, 0), (0, 5-i.shape[1])], 'mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_features = np.asarray(padded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 29, 773)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_emb = np.asarray([comp_emb.numpy() for i in range(260)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 29, 768)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_features = np.concatenate((padded_features, comp_emb), axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.signal import DynamicGraphTemporalSignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_signal = DynamicGraphTemporalSignal(edge_indices = edge_indices , edge_weights = edge_weights, features = padded_features, targets = targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch_geometric_temporal.signal.dynamic_graph_temporal_signal.DynamicGraphTemporalSignal at 0x25aef39f2b0>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporal_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "\n",
    "train_dataset, test_dataset = temporal_signal_split(temporal_signal, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric_temporal.nn.recurrent import DCRNN\n",
    "from torch_geometric_temporal import GConvGRU\n",
    "from torch_geometric_temporal import EvolveGCNO\n",
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, node_features):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        self.evol = EvolveGCNO(node_features)\n",
    "        self.recurrent = DCRNN(node_features, 16, 1)\n",
    "        self.conv = GConvGRU(node_features, 64, 3)\n",
    "        #self.linear = torch.nn.Linear(16, 1)\n",
    "        self.linear = torch.nn.Linear(64, 2)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "#        h = self.recurrent(x, edge_index, edge_weight)\n",
    "#        h = self.dropout(h)\n",
    "        h = self.conv(x, edge_index, edge_weight)\n",
    "        h = F.relu(h)\n",
    "        h = self.linear(h)\n",
    "        h = torch.sigmoid(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7708675fe369>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msnapshot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msnapshot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msnapshot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msnapshot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msnapshot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model = RecurrentGCN(node_features = 773)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(200)):\n",
    "    loss = 0\n",
    "    for time, snapshot in enumerate(train_dataset):\n",
    "        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "        loss += torch.nn.CrossEntropyLoss()(y_hat, snapshot.y.long())\n",
    "#        loss += torch.mean((y_hat-snapshot.y)**2)\n",
    "#        loss = loss / (time+1)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_l = []\n",
    "model.eval()\n",
    "cost = 0\n",
    "for time, snapshot in enumerate(test_dataset):\n",
    "    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "    #cost = cost + torch.mean((y_hat-snapshot.y)**2)\n",
    "    y_hat_l.append(y_hat)\n",
    "#cost = cost / (time+1)\n",
    "#cost = cost.item()\n",
    "#print(\"MSE: {:.4f}\".format(cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.4215, 0.6706],\n",
       "         [0.4884, 0.7967],\n",
       "         [0.3832, 0.7016],\n",
       "         [0.4128, 0.6792],\n",
       "         [0.4484, 0.6509],\n",
       "         [0.4014, 0.6830],\n",
       "         [0.4192, 0.6690],\n",
       "         [0.4508, 0.6499],\n",
       "         [0.5059, 0.6028],\n",
       "         [0.4601, 0.6401],\n",
       "         [0.4281, 0.6657],\n",
       "         [0.3131, 0.7543],\n",
       "         [0.4349, 0.6609],\n",
       "         [0.4594, 0.6436],\n",
       "         [0.3904, 0.6933],\n",
       "         [0.4468, 0.7603],\n",
       "         [0.4244, 0.6690],\n",
       "         [0.4884, 0.7967],\n",
       "         [0.4508, 0.6468],\n",
       "         [0.4884, 0.7967],\n",
       "         [0.4012, 0.6836],\n",
       "         [0.5265, 0.5900],\n",
       "         [0.4884, 0.7967],\n",
       "         [0.4919, 0.7304],\n",
       "         [0.3989, 0.6871],\n",
       "         [0.4375, 0.6598],\n",
       "         [0.4884, 0.7967],\n",
       "         [0.4311, 0.6632],\n",
       "         [0.3883, 0.6926]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3099, 0.7534],\n",
       "         [0.4536, 0.8277],\n",
       "         [0.2411, 0.8074],\n",
       "         [0.2988, 0.7656],\n",
       "         [0.3509, 0.7250],\n",
       "         [0.2626, 0.7824],\n",
       "         [0.2875, 0.7635],\n",
       "         [0.3396, 0.7355],\n",
       "         [0.4683, 0.6299],\n",
       "         [0.3755, 0.7031],\n",
       "         [0.3251, 0.7426],\n",
       "         [0.4536, 0.8277],\n",
       "         [0.3071, 0.7556],\n",
       "         [0.3757, 0.7090],\n",
       "         [0.2462, 0.7987],\n",
       "         [0.4759, 0.6297],\n",
       "         [0.3050, 0.7578],\n",
       "         [0.4536, 0.8277],\n",
       "         [0.3491, 0.7215],\n",
       "         [0.1917, 0.8449],\n",
       "         [0.2672, 0.7799],\n",
       "         [0.5282, 0.5899],\n",
       "         [0.3119, 0.7589],\n",
       "         [0.4536, 0.8277],\n",
       "         [0.2708, 0.7808],\n",
       "         [0.3433, 0.7315],\n",
       "         [0.4536, 0.8277],\n",
       "         [0.3250, 0.7422],\n",
       "         [0.2620, 0.7833]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4347, 0.6649],\n",
       "         [0.5530, 0.7530],\n",
       "         [0.4203, 0.6768],\n",
       "         [0.4298, 0.6695],\n",
       "         [0.5213, 0.6894],\n",
       "         [0.4284, 0.6686],\n",
       "         [0.4298, 0.6674],\n",
       "         [0.4416, 0.6606],\n",
       "         [0.4635, 0.6420],\n",
       "         [0.4505, 0.6526],\n",
       "         [0.4369, 0.6634],\n",
       "         [0.5033, 0.6602],\n",
       "         [0.4377, 0.6629],\n",
       "         [0.4476, 0.6560],\n",
       "         [0.4206, 0.6753],\n",
       "         [0.5033, 0.6602],\n",
       "         [0.4348, 0.6652],\n",
       "         [0.5530, 0.7530],\n",
       "         [0.4418, 0.6591],\n",
       "         [0.5351, 0.6791],\n",
       "         [0.4250, 0.6713],\n",
       "         [0.4457, 0.7077],\n",
       "         [0.5247, 0.7507],\n",
       "         [0.5599, 0.7245],\n",
       "         [0.5033, 0.6602],\n",
       "         [0.4425, 0.6596],\n",
       "         [0.5530, 0.7530],\n",
       "         [0.4333, 0.6660],\n",
       "         [0.4216, 0.6737]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4302, 0.6680],\n",
       "         [0.4803, 0.7907],\n",
       "         [0.4248, 0.6724],\n",
       "         [0.4287, 0.6694],\n",
       "         [0.4396, 0.6991],\n",
       "         [0.4280, 0.6692],\n",
       "         [0.4293, 0.6682],\n",
       "         [0.4321, 0.6669],\n",
       "         [0.4406, 0.6598],\n",
       "         [0.4359, 0.6636],\n",
       "         [0.4318, 0.6668],\n",
       "         [0.4718, 0.6902],\n",
       "         [0.4334, 0.6655],\n",
       "         [0.4376, 0.6627],\n",
       "         [0.4246, 0.6721],\n",
       "         [0.4696, 0.7320],\n",
       "         [0.4297, 0.6685],\n",
       "         [0.4843, 0.7736],\n",
       "         [0.4654, 0.6963],\n",
       "         [0.4795, 0.7571],\n",
       "         [0.4266, 0.6704],\n",
       "         [0.4367, 0.7431],\n",
       "         [0.4778, 0.7761],\n",
       "         [0.4831, 0.7562],\n",
       "         [0.4268, 0.6705],\n",
       "         [0.4325, 0.6665],\n",
       "         [0.4803, 0.7907],\n",
       "         [0.4307, 0.6676],\n",
       "         [0.4257, 0.6709]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4448, 0.6528],\n",
       "         [0.7191, 0.6307],\n",
       "         [0.4388, 0.6578],\n",
       "         [0.4462, 0.6520],\n",
       "         [0.4966, 0.6417],\n",
       "         [0.4417, 0.6543],\n",
       "         [0.4441, 0.6524],\n",
       "         [0.4487, 0.6502],\n",
       "         [0.4615, 0.6394],\n",
       "         [0.4544, 0.6450],\n",
       "         [0.4473, 0.6508],\n",
       "         [0.5192, 0.6238],\n",
       "         [0.4464, 0.6515],\n",
       "         [0.4543, 0.6458],\n",
       "         [0.4384, 0.6574],\n",
       "         [0.5790, 0.6290],\n",
       "         [0.4458, 0.6520],\n",
       "         [0.7068, 0.6283],\n",
       "         [0.5077, 0.6342],\n",
       "         [0.6844, 0.6184],\n",
       "         [0.4407, 0.6552],\n",
       "         [0.4697, 0.6337],\n",
       "         [0.6698, 0.6287],\n",
       "         [0.6800, 0.6202],\n",
       "         [0.4896, 0.6486],\n",
       "         [0.4966, 0.6417],\n",
       "         [0.7191, 0.6307],\n",
       "         [0.4497, 0.6487],\n",
       "         [0.4386, 0.6568]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3444, 0.7270],\n",
       "         [0.3336, 0.8745],\n",
       "         [0.3406, 0.7303],\n",
       "         [0.3448, 0.7271],\n",
       "         [0.3530, 0.7208],\n",
       "         [0.3405, 0.7294],\n",
       "         [0.3454, 0.7257],\n",
       "         [0.3515, 0.7222],\n",
       "         [0.3598, 0.7152],\n",
       "         [0.3527, 0.7207],\n",
       "         [0.3479, 0.7244],\n",
       "         [0.3707, 0.8097],\n",
       "         [0.3492, 0.7235],\n",
       "         [0.3537, 0.7205],\n",
       "         [0.3411, 0.7294],\n",
       "         [0.3325, 0.7943],\n",
       "         [0.3477, 0.7247],\n",
       "         [0.3333, 0.8687],\n",
       "         [0.3526, 0.7207],\n",
       "         [0.3579, 0.7913],\n",
       "         [0.3405, 0.7294],\n",
       "         [0.3461, 0.7671],\n",
       "         [0.3544, 0.8451],\n",
       "         [0.3460, 0.8407],\n",
       "         [0.3407, 0.7296],\n",
       "         [0.3485, 0.7242],\n",
       "         [0.3336, 0.8745],\n",
       "         [0.3480, 0.7243],\n",
       "         [0.3420, 0.7282]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4305, 0.6664],\n",
       "         [0.5111, 0.7724],\n",
       "         [0.4282, 0.6684],\n",
       "         [0.4309, 0.6662],\n",
       "         [0.4350, 0.6783],\n",
       "         [0.4297, 0.6667],\n",
       "         [0.4295, 0.6666],\n",
       "         [0.4378, 0.6763],\n",
       "         [0.4493, 0.6834],\n",
       "         [0.4358, 0.6623],\n",
       "         [0.4321, 0.6652],\n",
       "         [0.4895, 0.6890],\n",
       "         [0.4328, 0.6646],\n",
       "         [0.4361, 0.6624],\n",
       "         [0.4269, 0.6691],\n",
       "         [0.4660, 0.7164],\n",
       "         [0.4315, 0.6657],\n",
       "         [0.5086, 0.7616],\n",
       "         [0.4618, 0.6989],\n",
       "         [0.4970, 0.7149],\n",
       "         [0.4287, 0.6675],\n",
       "         [0.4510, 0.6791],\n",
       "         [0.5083, 0.7678],\n",
       "         [0.5005, 0.7538],\n",
       "         [0.4417, 0.6711],\n",
       "         [0.4339, 0.6640],\n",
       "         [0.5111, 0.7724],\n",
       "         [0.4318, 0.6653],\n",
       "         [0.4271, 0.6686]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4113, 0.6794],\n",
       "         [0.6362, 0.7032],\n",
       "         [0.4076, 0.6825],\n",
       "         [0.4105, 0.6803],\n",
       "         [0.4144, 0.6772],\n",
       "         [0.4074, 0.6820],\n",
       "         [0.4109, 0.6793],\n",
       "         [0.4144, 0.6774],\n",
       "         [0.4219, 0.6711],\n",
       "         [0.4159, 0.6759],\n",
       "         [0.4381, 0.6774],\n",
       "         [0.5497, 0.6553],\n",
       "         [0.4126, 0.6785],\n",
       "         [0.4162, 0.6760],\n",
       "         [0.4071, 0.6825],\n",
       "         [0.4906, 0.6720],\n",
       "         [0.4116, 0.6792],\n",
       "         [0.6165, 0.6917],\n",
       "         [0.4906, 0.6722],\n",
       "         [0.5849, 0.6655],\n",
       "         [0.4088, 0.6810],\n",
       "         [0.4621, 0.6765],\n",
       "         [0.6213, 0.6881],\n",
       "         [0.6345, 0.6961],\n",
       "         [0.4103, 0.6801],\n",
       "         [0.4142, 0.6774],\n",
       "         [0.6362, 0.7032],\n",
       "         [0.4155, 0.6761],\n",
       "         [0.4072, 0.6822]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3779, 0.7068],\n",
       "         [0.5068, 0.7742],\n",
       "         [0.3731, 0.7107],\n",
       "         [0.3773, 0.7075],\n",
       "         [0.3822, 0.7037],\n",
       "         [0.3769, 0.7070],\n",
       "         [0.3771, 0.7069],\n",
       "         [0.3817, 0.7042],\n",
       "         [0.3896, 0.6977],\n",
       "         [0.3835, 0.7025],\n",
       "         [0.3795, 0.7056],\n",
       "         [0.4913, 0.7385],\n",
       "         [0.3794, 0.7057],\n",
       "         [0.3833, 0.7031],\n",
       "         [0.3736, 0.7099],\n",
       "         [0.4254, 0.7069],\n",
       "         [0.3788, 0.7062],\n",
       "         [0.4965, 0.7640],\n",
       "         [0.4669, 0.7342],\n",
       "         [0.4701, 0.7301],\n",
       "         [0.3750, 0.7085],\n",
       "         [0.4370, 0.7286],\n",
       "         [0.5060, 0.7581],\n",
       "         [0.5068, 0.7742],\n",
       "         [0.3757, 0.7084],\n",
       "         [0.3807, 0.7049],\n",
       "         [0.5068, 0.7742],\n",
       "         [0.3791, 0.7058],\n",
       "         [0.3744, 0.7089]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4117, 0.6947],\n",
       "         [0.4121, 0.8336],\n",
       "         [0.4020, 0.6869],\n",
       "         [0.4035, 0.6858],\n",
       "         [0.4091, 0.6814],\n",
       "         [0.4070, 0.6825],\n",
       "         [0.4054, 0.6837],\n",
       "         [0.4080, 0.6824],\n",
       "         [0.4147, 0.6769],\n",
       "         [0.4105, 0.6802],\n",
       "         [0.4091, 0.6814],\n",
       "         [0.4302, 0.7478],\n",
       "         [0.4077, 0.6825],\n",
       "         [0.4098, 0.6810],\n",
       "         [0.4023, 0.6864],\n",
       "         [0.4079, 0.7434],\n",
       "         [0.4063, 0.6835],\n",
       "         [0.4127, 0.8139],\n",
       "         [0.4166, 0.7290],\n",
       "         [0.4169, 0.7469],\n",
       "         [0.4045, 0.6845],\n",
       "         [0.4089, 0.7424],\n",
       "         [0.4123, 0.8189],\n",
       "         [0.4208, 0.8091],\n",
       "         [0.4040, 0.6852],\n",
       "         [0.4089, 0.6817],\n",
       "         [0.4121, 0.8336],\n",
       "         [0.4047, 0.6846],\n",
       "         [0.4044, 0.6846]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4438, 0.6836],\n",
       "         [0.5546, 0.7705],\n",
       "         [0.4047, 0.6833],\n",
       "         [0.4076, 0.6811],\n",
       "         [0.4105, 0.6788],\n",
       "         [0.4063, 0.6815],\n",
       "         [0.4071, 0.6809],\n",
       "         [0.4093, 0.6798],\n",
       "         [0.4146, 0.6752],\n",
       "         [0.4115, 0.6779],\n",
       "         [0.4086, 0.6801],\n",
       "         [0.4939, 0.6726],\n",
       "         [0.4091, 0.6798],\n",
       "         [0.4120, 0.6778],\n",
       "         [0.4037, 0.6837],\n",
       "         [0.4329, 0.7047],\n",
       "         [0.4079, 0.6807],\n",
       "         [0.5390, 0.7518],\n",
       "         [0.4228, 0.6846],\n",
       "         [0.4973, 0.7015],\n",
       "         [0.4059, 0.6819],\n",
       "         [0.4406, 0.6977],\n",
       "         [0.5355, 0.7468],\n",
       "         [0.5421, 0.7616],\n",
       "         [0.4512, 0.6926],\n",
       "         [0.4103, 0.6789],\n",
       "         [0.5546, 0.7705],\n",
       "         [0.4090, 0.6798],\n",
       "         [0.4051, 0.6824]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4090, 0.6827],\n",
       "         [0.4860, 0.7971],\n",
       "         [0.4057, 0.6855],\n",
       "         [0.4103, 0.6820],\n",
       "         [0.4390, 0.7069],\n",
       "         [0.4068, 0.6840],\n",
       "         [0.4086, 0.6826],\n",
       "         [0.4154, 0.6933],\n",
       "         [0.4171, 0.6931],\n",
       "         [0.4142, 0.6787],\n",
       "         [0.4105, 0.6816],\n",
       "         [0.4680, 0.6999],\n",
       "         [0.4109, 0.6813],\n",
       "         [0.4154, 0.6781],\n",
       "         [0.4048, 0.6858],\n",
       "         [0.4407, 0.7196],\n",
       "         [0.4101, 0.6820],\n",
       "         [0.4817, 0.7873],\n",
       "         [0.4172, 0.6942],\n",
       "         [0.4723, 0.7305],\n",
       "         [0.4066, 0.6842],\n",
       "         [0.4196, 0.7051],\n",
       "         [0.4761, 0.7904],\n",
       "         [0.4854, 0.7780],\n",
       "         [0.4171, 0.6931],\n",
       "         [0.4116, 0.6810],\n",
       "         [0.4860, 0.7971],\n",
       "         [0.4113, 0.6810],\n",
       "         [0.4063, 0.6844]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3875, 0.6975],\n",
       "         [0.5267, 0.7795],\n",
       "         [0.3799, 0.7038],\n",
       "         [0.3873, 0.6982],\n",
       "         [0.4217, 0.7040],\n",
       "         [0.3846, 0.6990],\n",
       "         [0.3871, 0.6970],\n",
       "         [0.3931, 0.6938],\n",
       "         [0.4247, 0.7009],\n",
       "         [0.3970, 0.6902],\n",
       "         [0.3907, 0.6952],\n",
       "         [0.4848, 0.7112],\n",
       "         [0.3905, 0.6954],\n",
       "         [0.3966, 0.6912],\n",
       "         [0.3805, 0.7026],\n",
       "         [0.3896, 0.7250],\n",
       "         [0.3884, 0.6970],\n",
       "         [0.5267, 0.7795],\n",
       "         [0.3947, 0.6918],\n",
       "         [0.4900, 0.7286],\n",
       "         [0.3835, 0.6999],\n",
       "         [0.3896, 0.7250],\n",
       "         [0.5060, 0.7661],\n",
       "         [0.5209, 0.7707],\n",
       "         [0.3836, 0.7003],\n",
       "         [0.3923, 0.6943],\n",
       "         [0.5267, 0.7795],\n",
       "         [0.3909, 0.6949],\n",
       "         [0.3825, 0.7005]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4304, 0.6680],\n",
       "         [0.4735, 0.7932],\n",
       "         [0.4208, 0.6758],\n",
       "         [0.4285, 0.6700],\n",
       "         [0.4347, 0.6650],\n",
       "         [0.4254, 0.6712],\n",
       "         [0.4279, 0.6692],\n",
       "         [0.4344, 0.6655],\n",
       "         [0.4483, 0.6856],\n",
       "         [0.4378, 0.6623],\n",
       "         [0.4323, 0.6667],\n",
       "         [0.4716, 0.7168],\n",
       "         [0.4316, 0.6673],\n",
       "         [0.4370, 0.6635],\n",
       "         [0.4206, 0.6753],\n",
       "         [0.4411, 0.7154],\n",
       "         [0.4295, 0.6689],\n",
       "         [0.4735, 0.7932],\n",
       "         [0.4703, 0.6953],\n",
       "         [0.4812, 0.7109],\n",
       "         [0.4250, 0.6716],\n",
       "         [0.4313, 0.7455],\n",
       "         [0.4735, 0.7932],\n",
       "         [0.4735, 0.7932],\n",
       "         [0.4248, 0.6722],\n",
       "         [0.4327, 0.6666],\n",
       "         [0.4735, 0.7932],\n",
       "         [0.4312, 0.6674],\n",
       "         [0.4232, 0.6728]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3713, 0.7266],\n",
       "         [0.4045, 0.8285],\n",
       "         [0.3499, 0.7284],\n",
       "         [0.3533, 0.7259],\n",
       "         [0.3487, 0.7412],\n",
       "         [0.3526, 0.7258],\n",
       "         [0.3533, 0.7252],\n",
       "         [0.3487, 0.7412],\n",
       "         [0.3487, 0.7412],\n",
       "         [0.3571, 0.7227],\n",
       "         [0.3557, 0.7239],\n",
       "         [0.4246, 0.7613],\n",
       "         [0.3552, 0.7243],\n",
       "         [0.3583, 0.7222],\n",
       "         [0.3501, 0.7280],\n",
       "         [0.3675, 0.7657],\n",
       "         [0.3561, 0.7236],\n",
       "         [0.4077, 0.8149],\n",
       "         [0.3663, 0.7428],\n",
       "         [0.4205, 0.7799],\n",
       "         [0.3520, 0.7263],\n",
       "         [0.3560, 0.7521],\n",
       "         [0.4050, 0.8225],\n",
       "         [0.4069, 0.8214],\n",
       "         [0.3513, 0.7271],\n",
       "         [0.3571, 0.7230],\n",
       "         [0.4045, 0.8285],\n",
       "         [0.3532, 0.7257],\n",
       "         [0.3513, 0.7268]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3835, 0.6998],\n",
       "         [0.5492, 0.7618],\n",
       "         [0.3723, 0.7089],\n",
       "         [0.3813, 0.7021],\n",
       "         [0.3905, 0.6948],\n",
       "         [0.3788, 0.7025],\n",
       "         [0.3816, 0.7001],\n",
       "         [0.4369, 0.6979],\n",
       "         [0.4081, 0.6806],\n",
       "         [0.3927, 0.6926],\n",
       "         [0.3866, 0.6974],\n",
       "         [0.4799, 0.6999],\n",
       "         [0.3862, 0.6979],\n",
       "         [0.3870, 0.6976],\n",
       "         [0.3724, 0.7080],\n",
       "         [0.4885, 0.7218],\n",
       "         [0.3855, 0.6985],\n",
       "         [0.5291, 0.7587],\n",
       "         [0.3906, 0.6940],\n",
       "         [0.5046, 0.7102],\n",
       "         [0.3783, 0.7029],\n",
       "         [0.4170, 0.6748],\n",
       "         [0.5391, 0.7508],\n",
       "         [0.5492, 0.7618],\n",
       "         [0.3796, 0.7025],\n",
       "         [0.3917, 0.6940],\n",
       "         [0.5492, 0.7618],\n",
       "         [0.3837, 0.6996],\n",
       "         [0.3762, 0.7044]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4573, 0.6438],\n",
       "         [0.5177, 0.7842],\n",
       "         [0.4487, 0.6515],\n",
       "         [0.4915, 0.6758],\n",
       "         [0.4755, 0.6886],\n",
       "         [0.4523, 0.6464],\n",
       "         [0.4601, 0.6402],\n",
       "         [0.4745, 0.6313],\n",
       "         [0.4994, 0.6696],\n",
       "         [0.4810, 0.6251],\n",
       "         [0.4647, 0.6382],\n",
       "         [0.4994, 0.6696],\n",
       "         [0.4743, 0.6305],\n",
       "         [0.4822, 0.6248],\n",
       "         [0.4470, 0.6515],\n",
       "         [0.4636, 0.6952],\n",
       "         [0.4639, 0.6390],\n",
       "         [0.5231, 0.7586],\n",
       "         [0.4636, 0.6952],\n",
       "         [0.4316, 0.6651],\n",
       "         [0.4511, 0.6475],\n",
       "         [0.5114, 0.6020],\n",
       "         [0.5013, 0.7717],\n",
       "         [0.5165, 0.7618],\n",
       "         [0.4556, 0.6449],\n",
       "         [0.4714, 0.6335],\n",
       "         [0.5177, 0.7842],\n",
       "         [0.4677, 0.6357],\n",
       "         [0.4493, 0.6488]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.5146, 0.6006],\n",
       "         [0.5816, 0.7286],\n",
       "         [0.4932, 0.6189],\n",
       "         [0.5093, 0.6059],\n",
       "         [0.5226, 0.5949],\n",
       "         [0.4989, 0.6119],\n",
       "         [0.5040, 0.6077],\n",
       "         [0.5216, 0.5961],\n",
       "         [0.5454, 0.5748],\n",
       "         [0.5301, 0.5877],\n",
       "         [0.5165, 0.5993],\n",
       "         [0.5236, 0.6532],\n",
       "         [0.5141, 0.6012],\n",
       "         [0.5252, 0.5932],\n",
       "         [0.4929, 0.6176],\n",
       "         [0.5460, 0.6833],\n",
       "         [0.5098, 0.6049],\n",
       "         [0.5643, 0.7085],\n",
       "         [0.5459, 0.6855],\n",
       "         [0.5395, 0.6439],\n",
       "         [0.4952, 0.6149],\n",
       "         [0.5236, 0.6532],\n",
       "         [0.5816, 0.7286],\n",
       "         [0.5816, 0.7286],\n",
       "         [0.5024, 0.6102],\n",
       "         [0.5198, 0.5972],\n",
       "         [0.5816, 0.7286],\n",
       "         [0.5080, 0.6057],\n",
       "         [0.4978, 0.6126]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3733, 0.7234],\n",
       "         [0.4046, 0.8283],\n",
       "         [0.3549, 0.7238],\n",
       "         [0.3584, 0.7212],\n",
       "         [0.3632, 0.7175],\n",
       "         [0.3575, 0.7213],\n",
       "         [0.3586, 0.7203],\n",
       "         [0.3754, 0.7236],\n",
       "         [0.3690, 0.7129],\n",
       "         [0.3640, 0.7167],\n",
       "         [0.3611, 0.7190],\n",
       "         [0.4203, 0.7804],\n",
       "         [0.3616, 0.7186],\n",
       "         [0.3646, 0.7166],\n",
       "         [0.3563, 0.7225],\n",
       "         [0.3902, 0.7736],\n",
       "         [0.3605, 0.7195],\n",
       "         [0.4052, 0.8223],\n",
       "         [0.3734, 0.7243],\n",
       "         [0.4064, 0.7714],\n",
       "         [0.3578, 0.7211],\n",
       "         [0.3678, 0.7425],\n",
       "         [0.4046, 0.8283],\n",
       "         [0.4137, 0.8053],\n",
       "         [0.3733, 0.7234],\n",
       "         [0.3615, 0.7188],\n",
       "         [0.4046, 0.8283],\n",
       "         [0.3612, 0.7189],\n",
       "         [0.3566, 0.7219]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4481, 0.6812],\n",
       "         [0.5060, 0.7752],\n",
       "         [0.4304, 0.6727],\n",
       "         [0.4375, 0.6673],\n",
       "         [0.4619, 0.7175],\n",
       "         [0.4335, 0.6693],\n",
       "         [0.4380, 0.6658],\n",
       "         [0.4420, 0.6640],\n",
       "         [0.4549, 0.6534],\n",
       "         [0.4458, 0.6604],\n",
       "         [0.4407, 0.6646],\n",
       "         [0.4834, 0.6826],\n",
       "         [0.4415, 0.6640],\n",
       "         [0.4445, 0.6620],\n",
       "         [0.4306, 0.6720],\n",
       "         [0.4605, 0.7355],\n",
       "         [0.4379, 0.6667],\n",
       "         [0.4914, 0.7598],\n",
       "         [0.4481, 0.6812],\n",
       "         [0.4767, 0.7239],\n",
       "         [0.4332, 0.6696],\n",
       "         [0.4544, 0.6792],\n",
       "         [0.5094, 0.7612],\n",
       "         [0.5102, 0.7484],\n",
       "         [0.4801, 0.6830],\n",
       "         [0.4481, 0.6812],\n",
       "         [0.5060, 0.7752],\n",
       "         [0.4399, 0.6650],\n",
       "         [0.4326, 0.6700]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4055, 0.6832],\n",
       "         [0.5409, 0.7679],\n",
       "         [0.3966, 0.6906],\n",
       "         [0.4034, 0.6854],\n",
       "         [0.4096, 0.6803],\n",
       "         [0.4009, 0.6860],\n",
       "         [0.4040, 0.6836],\n",
       "         [0.4088, 0.6812],\n",
       "         [0.4235, 0.6691],\n",
       "         [0.4134, 0.6771],\n",
       "         [0.4070, 0.6821],\n",
       "         [0.4932, 0.7205],\n",
       "         [0.4078, 0.6816],\n",
       "         [0.4132, 0.6779],\n",
       "         [0.3976, 0.6891],\n",
       "         [0.4497, 0.7080],\n",
       "         [0.4055, 0.6834],\n",
       "         [0.5188, 0.7547],\n",
       "         [0.4421, 0.6847],\n",
       "         [0.4734, 0.6909],\n",
       "         [0.4005, 0.6864],\n",
       "         [0.4520, 0.7045],\n",
       "         [0.5409, 0.7679],\n",
       "         [0.5409, 0.7679],\n",
       "         [0.4008, 0.6866],\n",
       "         [0.4096, 0.6805],\n",
       "         [0.5409, 0.7679],\n",
       "         [0.4072, 0.6819],\n",
       "         [0.4008, 0.6860]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4469, 0.6711],\n",
       "         [0.6077, 0.7270],\n",
       "         [0.4164, 0.6751],\n",
       "         [0.4201, 0.6721],\n",
       "         [0.4245, 0.6687],\n",
       "         [0.4205, 0.6712],\n",
       "         [0.4201, 0.6715],\n",
       "         [0.4514, 0.6669],\n",
       "         [0.4324, 0.6622],\n",
       "         [0.4257, 0.6676],\n",
       "         [0.4242, 0.6687],\n",
       "         [0.4838, 0.6605],\n",
       "         [0.4224, 0.6702],\n",
       "         [0.4265, 0.6673],\n",
       "         [0.4163, 0.6748],\n",
       "         [0.4726, 0.6856],\n",
       "         [0.4218, 0.6707],\n",
       "         [0.5960, 0.7189],\n",
       "         [0.4642, 0.6755],\n",
       "         [0.5583, 0.6885],\n",
       "         [0.4185, 0.6728],\n",
       "         [0.4527, 0.6650],\n",
       "         [0.6002, 0.7161],\n",
       "         [0.5903, 0.7141],\n",
       "         [0.4606, 0.6772],\n",
       "         [0.4230, 0.6699],\n",
       "         [0.5986, 0.7253],\n",
       "         [0.4225, 0.6701],\n",
       "         [0.4185, 0.6727]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3670, 0.7131],\n",
       "         [0.6101, 0.7169],\n",
       "         [0.3598, 0.7188],\n",
       "         [0.3648, 0.7151],\n",
       "         [0.4063, 0.7013],\n",
       "         [0.3643, 0.7148],\n",
       "         [0.3650, 0.7142],\n",
       "         [0.4048, 0.7024],\n",
       "         [0.3771, 0.7054],\n",
       "         [0.3710, 0.7101],\n",
       "         [0.3681, 0.7124],\n",
       "         [0.5314, 0.6842],\n",
       "         [0.3669, 0.7133],\n",
       "         [0.3722, 0.7096],\n",
       "         [0.3619, 0.7168],\n",
       "         [0.3932, 0.7108],\n",
       "         [0.3656, 0.7143],\n",
       "         [0.5815, 0.7113],\n",
       "         [0.4359, 0.6964],\n",
       "         [0.5435, 0.6886],\n",
       "         [0.3636, 0.7153],\n",
       "         [0.3966, 0.7083],\n",
       "         [0.6101, 0.7169],\n",
       "         [0.5925, 0.7027],\n",
       "         [0.3932, 0.7108],\n",
       "         [0.4100, 0.6975],\n",
       "         [0.6101, 0.7169],\n",
       "         [0.3672, 0.7130],\n",
       "         [0.3625, 0.7161]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3586, 0.7181],\n",
       "         [0.4112, 0.8358],\n",
       "         [0.3554, 0.7208],\n",
       "         [0.3553, 0.7206],\n",
       "         [0.3621, 0.7156],\n",
       "         [0.3566, 0.7192],\n",
       "         [0.3580, 0.7181],\n",
       "         [0.3614, 0.7163],\n",
       "         [0.3690, 0.7101],\n",
       "         [0.3631, 0.7147],\n",
       "         [0.3597, 0.7173],\n",
       "         [0.4356, 0.7624],\n",
       "         [0.3602, 0.7170],\n",
       "         [0.3630, 0.7151],\n",
       "         [0.3550, 0.7207],\n",
       "         [0.3617, 0.7460],\n",
       "         [0.3601, 0.7171],\n",
       "         [0.4124, 0.8190],\n",
       "         [0.3858, 0.7288],\n",
       "         [0.4227, 0.7615],\n",
       "         [0.3562, 0.7196],\n",
       "         [0.3740, 0.7499],\n",
       "         [0.4132, 0.8294],\n",
       "         [0.4129, 0.8241],\n",
       "         [0.3570, 0.7193],\n",
       "         [0.3609, 0.7166],\n",
       "         [0.4112, 0.8358],\n",
       "         [0.3585, 0.7181],\n",
       "         [0.3552, 0.7202]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3776, 0.7031],\n",
       "         [0.6056, 0.7326],\n",
       "         [0.3733, 0.7067],\n",
       "         [0.3765, 0.7042],\n",
       "         [0.4108, 0.6935],\n",
       "         [0.3748, 0.7049],\n",
       "         [0.3762, 0.7038],\n",
       "         [0.3805, 0.7012],\n",
       "         [0.3865, 0.6963],\n",
       "         [0.3810, 0.7006],\n",
       "         [0.3772, 0.7035],\n",
       "         [0.5478, 0.6764],\n",
       "         [0.3776, 0.7032],\n",
       "         [0.3814, 0.7005],\n",
       "         [0.3726, 0.7069],\n",
       "         [0.4318, 0.7113],\n",
       "         [0.3771, 0.7036],\n",
       "         [0.5961, 0.7243],\n",
       "         [0.4108, 0.6935],\n",
       "         [0.4807, 0.6754],\n",
       "         [0.3748, 0.7050],\n",
       "         [0.3944, 0.7072],\n",
       "         [0.5946, 0.7254],\n",
       "         [0.6015, 0.7280],\n",
       "         [0.4108, 0.6935],\n",
       "         [0.4109, 0.6946],\n",
       "         [0.6056, 0.7326],\n",
       "         [0.3788, 0.7022],\n",
       "         [0.3739, 0.7056]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3840, 0.6967],\n",
       "         [0.4948, 0.8014],\n",
       "         [0.3786, 0.7010],\n",
       "         [0.3829, 0.6978],\n",
       "         [0.3862, 0.6951],\n",
       "         [0.3825, 0.6974],\n",
       "         [0.3821, 0.6976],\n",
       "         [0.3850, 0.6961],\n",
       "         [0.3934, 0.6893],\n",
       "         [0.3887, 0.6930],\n",
       "         [0.3838, 0.6968],\n",
       "         [0.4638, 0.7419],\n",
       "         [0.3840, 0.6967],\n",
       "         [0.3887, 0.6933],\n",
       "         [0.3790, 0.7003],\n",
       "         [0.4232, 0.7131],\n",
       "         [0.3831, 0.6974],\n",
       "         [0.4900, 0.7926],\n",
       "         [0.3963, 0.7029],\n",
       "         [0.4405, 0.7147],\n",
       "         [0.3814, 0.6983],\n",
       "         [0.4261, 0.7240],\n",
       "         [0.4926, 0.7844],\n",
       "         [0.4870, 0.7744],\n",
       "         [0.3810, 0.6988],\n",
       "         [0.3854, 0.6958],\n",
       "         [0.4948, 0.8014],\n",
       "         [0.3850, 0.6959],\n",
       "         [0.3800, 0.6993]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3734, 0.7062],\n",
       "         [0.5492, 0.7648],\n",
       "         [0.3699, 0.7090],\n",
       "         [0.3729, 0.7067],\n",
       "         [0.3964, 0.7036],\n",
       "         [0.3711, 0.7075],\n",
       "         [0.3732, 0.7059],\n",
       "         [0.3760, 0.7044],\n",
       "         [0.3818, 0.6996],\n",
       "         [0.3779, 0.7026],\n",
       "         [0.3746, 0.7053],\n",
       "         [0.5056, 0.7298],\n",
       "         [0.3752, 0.7048],\n",
       "         [0.3777, 0.7031],\n",
       "         [0.3704, 0.7083],\n",
       "         [0.4410, 0.7245],\n",
       "         [0.3744, 0.7055],\n",
       "         [0.5381, 0.7595],\n",
       "         [0.4116, 0.7070],\n",
       "         [0.4812, 0.7181],\n",
       "         [0.3709, 0.7077],\n",
       "         [0.3854, 0.6973],\n",
       "         [0.5492, 0.7648],\n",
       "         [0.5372, 0.7470],\n",
       "         [0.3723, 0.7069],\n",
       "         [0.3754, 0.7048],\n",
       "         [0.5492, 0.7648],\n",
       "         [0.3740, 0.7056],\n",
       "         [0.3709, 0.7077]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4268, 0.6797],\n",
       "         [0.5078, 0.7880],\n",
       "         [0.4141, 0.6781],\n",
       "         [0.4167, 0.6762],\n",
       "         [0.4179, 0.6855],\n",
       "         [0.4149, 0.6771],\n",
       "         [0.4165, 0.6759],\n",
       "         [0.4187, 0.6747],\n",
       "         [0.4204, 0.6842],\n",
       "         [0.4196, 0.6738],\n",
       "         [0.4176, 0.6753],\n",
       "         [0.4824, 0.7219],\n",
       "         [0.4177, 0.6753],\n",
       "         [0.4195, 0.6740],\n",
       "         [0.4144, 0.6777],\n",
       "         [0.4252, 0.7183],\n",
       "         [0.4170, 0.6758],\n",
       "         [0.4898, 0.7746],\n",
       "         [0.4280, 0.6781],\n",
       "         [0.4846, 0.6925],\n",
       "         [0.4148, 0.6772],\n",
       "         [0.4326, 0.7203],\n",
       "         [0.5029, 0.7790],\n",
       "         [0.5078, 0.7646],\n",
       "         [0.4438, 0.6879],\n",
       "         [0.4497, 0.6715],\n",
       "         [0.5078, 0.7880],\n",
       "         [0.4182, 0.6748],\n",
       "         [0.4148, 0.6772]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4255, 0.6919],\n",
       "         [0.4831, 0.7999],\n",
       "         [0.3830, 0.7013],\n",
       "         [0.3887, 0.6970],\n",
       "         [0.3936, 0.6932],\n",
       "         [0.3858, 0.6983],\n",
       "         [0.3879, 0.6966],\n",
       "         [0.4141, 0.7003],\n",
       "         [0.4028, 0.6857],\n",
       "         [0.3949, 0.6918],\n",
       "         [0.3907, 0.6952],\n",
       "         [0.4611, 0.7072],\n",
       "         [0.3914, 0.6947],\n",
       "         [0.3963, 0.6913],\n",
       "         [0.3835, 0.7005],\n",
       "         [0.4182, 0.7188],\n",
       "         [0.3896, 0.6960],\n",
       "         [0.4764, 0.7952],\n",
       "         [0.3927, 0.7159],\n",
       "         [0.4654, 0.7352],\n",
       "         [0.3853, 0.6987],\n",
       "         [0.4035, 0.7284],\n",
       "         [0.4831, 0.7999],\n",
       "         [0.4652, 0.7935],\n",
       "         [0.3848, 0.6995],\n",
       "         [0.3916, 0.6947],\n",
       "         [0.4831, 0.7999],\n",
       "         [0.3918, 0.6943],\n",
       "         [0.3845, 0.6993]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4341, 0.7019],\n",
       "         [0.5756, 0.7322],\n",
       "         [0.3480, 0.7302],\n",
       "         [0.3522, 0.7271],\n",
       "         [0.3563, 0.7239],\n",
       "         [0.3495, 0.7284],\n",
       "         [0.3520, 0.7265],\n",
       "         [0.3674, 0.7319],\n",
       "         [0.3639, 0.7179],\n",
       "         [0.3582, 0.7222],\n",
       "         [0.3542, 0.7253],\n",
       "         [0.5487, 0.7018],\n",
       "         [0.3526, 0.7265],\n",
       "         [0.3591, 0.7221],\n",
       "         [0.3475, 0.7302],\n",
       "         [0.4193, 0.7127],\n",
       "         [0.3525, 0.7267],\n",
       "         [0.5370, 0.7208],\n",
       "         [0.4339, 0.7181],\n",
       "         [0.5408, 0.7087],\n",
       "         [0.3495, 0.7285],\n",
       "         [0.3674, 0.7319],\n",
       "         [0.5631, 0.7321],\n",
       "         [0.5690, 0.7276],\n",
       "         [0.3958, 0.7122],\n",
       "         [0.3544, 0.7254],\n",
       "         [0.5756, 0.7322],\n",
       "         [0.3542, 0.7253],\n",
       "         [0.3487, 0.7289]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3944, 0.6928],\n",
       "         [0.5389, 0.7821],\n",
       "         [0.3633, 0.7176],\n",
       "         [0.3925, 0.6956],\n",
       "         [0.4143, 0.6784],\n",
       "         [0.3808, 0.7007],\n",
       "         [0.3881, 0.6949],\n",
       "         [0.4021, 0.6883],\n",
       "         [0.4493, 0.6495],\n",
       "         [0.4177, 0.6744],\n",
       "         [0.3973, 0.6906],\n",
       "         [0.5017, 0.6992],\n",
       "         [0.3986, 0.6898],\n",
       "         [0.4221, 0.6734],\n",
       "         [0.3670, 0.7128],\n",
       "         [0.5017, 0.6992],\n",
       "         [0.3920, 0.6950],\n",
       "         [0.5286, 0.7431],\n",
       "         [0.4108, 0.6793],\n",
       "         [0.5017, 0.6992],\n",
       "         [0.3831, 0.6991],\n",
       "         [0.4709, 0.6354],\n",
       "         [0.5389, 0.7821],\n",
       "         [0.5286, 0.7431],\n",
       "         [0.3766, 0.7057],\n",
       "         [0.4016, 0.6883],\n",
       "         [0.5389, 0.7821],\n",
       "         [0.3974, 0.6903],\n",
       "         [0.3698, 0.7088]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4247, 0.7614],\n",
       "         [0.4695, 0.8114],\n",
       "         [0.3397, 0.7309],\n",
       "         [0.3534, 0.7207],\n",
       "         [0.3625, 0.7133],\n",
       "         [0.3493, 0.7217],\n",
       "         [0.3489, 0.7216],\n",
       "         [0.3601, 0.7156],\n",
       "         [0.3879, 0.6931],\n",
       "         [0.3659, 0.7100],\n",
       "         [0.3572, 0.7170],\n",
       "         [0.3075, 0.7551],\n",
       "         [0.3582, 0.7164],\n",
       "         [0.3700, 0.7083],\n",
       "         [0.3403, 0.7292],\n",
       "         [0.3861, 0.6954],\n",
       "         [0.3555, 0.7185],\n",
       "         [0.4486, 0.7772],\n",
       "         [0.3639, 0.7114],\n",
       "         [0.4394, 0.7516],\n",
       "         [0.3463, 0.7241],\n",
       "         [0.3968, 0.6877],\n",
       "         [0.4695, 0.8114],\n",
       "         [0.4695, 0.8114],\n",
       "         [0.4247, 0.7614],\n",
       "         [0.3625, 0.7137],\n",
       "         [0.4695, 0.8114],\n",
       "         [0.3580, 0.7163],\n",
       "         [0.3446, 0.7251]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3886, 0.7056],\n",
       "         [0.5578, 0.7226],\n",
       "         [0.3724, 0.7186],\n",
       "         [0.3888, 0.7064],\n",
       "         [0.4026, 0.6959],\n",
       "         [0.3793, 0.7109],\n",
       "         [0.3858, 0.7060],\n",
       "         [0.4810, 0.6937],\n",
       "         [0.4348, 0.6702],\n",
       "         [0.4094, 0.6898],\n",
       "         [0.3985, 0.6982],\n",
       "         [0.5218, 0.7085],\n",
       "         [0.3952, 0.7010],\n",
       "         [0.4109, 0.6901],\n",
       "         [0.3707, 0.7184],\n",
       "         [0.4810, 0.6937],\n",
       "         [0.3900, 0.7049],\n",
       "         [0.5578, 0.7226],\n",
       "         [0.4715, 0.7010],\n",
       "         [0.5578, 0.7226],\n",
       "         [0.3784, 0.7118],\n",
       "         [0.4715, 0.7010],\n",
       "         [0.5218, 0.7085],\n",
       "         [0.5578, 0.7226],\n",
       "         [0.3819, 0.7103],\n",
       "         [0.3984, 0.6992],\n",
       "         [0.5578, 0.7226],\n",
       "         [0.3938, 0.7017],\n",
       "         [0.3756, 0.7136]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4211, 0.6642],\n",
       "         [0.5350, 0.7960],\n",
       "         [0.3847, 0.6948],\n",
       "         [0.4130, 0.6725],\n",
       "         [0.4628, 0.6327],\n",
       "         [0.3925, 0.6825],\n",
       "         [0.4164, 0.6638],\n",
       "         [0.4606, 0.6354],\n",
       "         [0.5124, 0.5897],\n",
       "         [0.4724, 0.6230],\n",
       "         [0.4383, 0.6509],\n",
       "         [0.2975, 0.7609],\n",
       "         [0.4372, 0.6522],\n",
       "         [0.4643, 0.6330],\n",
       "         [0.3832, 0.6924],\n",
       "         [0.5279, 0.5802],\n",
       "         [0.4255, 0.6615],\n",
       "         [0.5350, 0.7960],\n",
       "         [0.4579, 0.6339],\n",
       "         [0.3401, 0.7297],\n",
       "         [0.3975, 0.6794],\n",
       "         [0.5560, 0.5581],\n",
       "         [0.5350, 0.7960],\n",
       "         [0.5350, 0.7960],\n",
       "         [0.4026, 0.6777],\n",
       "         [0.4441, 0.6482],\n",
       "         [0.5350, 0.7960],\n",
       "         [0.4366, 0.6518],\n",
       "         [0.3883, 0.6856]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4733, 0.6297],\n",
       "         [0.4910, 0.8038],\n",
       "         [0.4622, 0.6391],\n",
       "         [0.4678, 0.6346],\n",
       "         [0.4789, 0.6257],\n",
       "         [0.4661, 0.6344],\n",
       "         [0.4709, 0.6307],\n",
       "         [0.4791, 0.6258],\n",
       "         [0.4641, 0.6763],\n",
       "         [0.4823, 0.6225],\n",
       "         [0.4746, 0.6288],\n",
       "         [0.4965, 0.7115],\n",
       "         [0.4757, 0.6280],\n",
       "         [0.4825, 0.6231],\n",
       "         [0.4632, 0.6375],\n",
       "         [0.4803, 0.7643],\n",
       "         [0.4722, 0.6308],\n",
       "         [0.4956, 0.7873],\n",
       "         [0.4794, 0.6247],\n",
       "         [0.4739, 0.6989],\n",
       "         [0.4668, 0.6341],\n",
       "         [0.4859, 0.6570],\n",
       "         [0.4956, 0.7873],\n",
       "         [0.4967, 0.7115],\n",
       "         [0.4671, 0.6345],\n",
       "         [0.4781, 0.6265],\n",
       "         [0.4910, 0.8038],\n",
       "         [0.4767, 0.6269],\n",
       "         [0.4645, 0.6357]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4126, 0.6775],\n",
       "         [0.4915, 0.8011],\n",
       "         [0.4056, 0.6833],\n",
       "         [0.4126, 0.6779],\n",
       "         [0.4187, 0.6731],\n",
       "         [0.4089, 0.6797],\n",
       "         [0.4107, 0.6781],\n",
       "         [0.4221, 0.7235],\n",
       "         [0.4196, 0.7022],\n",
       "         [0.4206, 0.6712],\n",
       "         [0.4159, 0.6750],\n",
       "         [0.4847, 0.6812],\n",
       "         [0.4148, 0.6758],\n",
       "         [0.4212, 0.6713],\n",
       "         [0.4057, 0.6826],\n",
       "         [0.4009, 0.7131],\n",
       "         [0.4130, 0.6773],\n",
       "         [0.4915, 0.8011],\n",
       "         [0.4183, 0.6728],\n",
       "         [0.4742, 0.6884],\n",
       "         [0.4081, 0.6803],\n",
       "         [0.4009, 0.7131],\n",
       "         [0.4915, 0.8011],\n",
       "         [0.4895, 0.7808],\n",
       "         [0.4196, 0.7022],\n",
       "         [0.4166, 0.6747],\n",
       "         [0.4915, 0.8011],\n",
       "         [0.4139, 0.6764],\n",
       "         [0.4071, 0.6810]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3857, 0.7147],\n",
       "         [0.5581, 0.7577],\n",
       "         [0.3549, 0.7204],\n",
       "         [0.3583, 0.7178],\n",
       "         [0.3640, 0.7134],\n",
       "         [0.3559, 0.7190],\n",
       "         [0.3573, 0.7179],\n",
       "         [0.3610, 0.7158],\n",
       "         [0.3682, 0.7099],\n",
       "         [0.3642, 0.7130],\n",
       "         [0.3599, 0.7164],\n",
       "         [0.4720, 0.7004],\n",
       "         [0.3594, 0.7167],\n",
       "         [0.3630, 0.7143],\n",
       "         [0.3540, 0.7207],\n",
       "         [0.4018, 0.7189],\n",
       "         [0.3587, 0.7173],\n",
       "         [0.5581, 0.7577],\n",
       "         [0.3619, 0.7147],\n",
       "         [0.4983, 0.7277],\n",
       "         [0.3554, 0.7194],\n",
       "         [0.3812, 0.7166],\n",
       "         [0.5581, 0.7577],\n",
       "         [0.5505, 0.7552],\n",
       "         [0.3812, 0.7166],\n",
       "         [0.3897, 0.7112],\n",
       "         [0.5581, 0.7577],\n",
       "         [0.3590, 0.7170],\n",
       "         [0.3552, 0.7195]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4417, 0.6572],\n",
       "         [0.6425, 0.6914],\n",
       "         [0.4390, 0.6595],\n",
       "         [0.4422, 0.6571],\n",
       "         [0.4463, 0.6537],\n",
       "         [0.4400, 0.6582],\n",
       "         [0.4419, 0.6566],\n",
       "         [0.4702, 0.6510],\n",
       "         [0.4514, 0.6495],\n",
       "         [0.4477, 0.6525],\n",
       "         [0.4436, 0.6557],\n",
       "         [0.5547, 0.6417],\n",
       "         [0.4443, 0.6553],\n",
       "         [0.4471, 0.6533],\n",
       "         [0.4383, 0.6597],\n",
       "         [0.5126, 0.6760],\n",
       "         [0.4428, 0.6564],\n",
       "         [0.6266, 0.6821],\n",
       "         [0.5013, 0.6573],\n",
       "         [0.5919, 0.6465],\n",
       "         [0.4395, 0.6586],\n",
       "         [0.5085, 0.6514],\n",
       "         [0.6425, 0.6914],\n",
       "         [0.6425, 0.6914],\n",
       "         [0.4404, 0.6582],\n",
       "         [0.4630, 0.6567],\n",
       "         [0.6425, 0.6914],\n",
       "         [0.4442, 0.6552],\n",
       "         [0.4396, 0.6585]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3995, 0.7044],\n",
       "         [0.4664, 0.8096],\n",
       "         [0.3856, 0.7007],\n",
       "         [0.3888, 0.6984],\n",
       "         [0.4054, 0.6992],\n",
       "         [0.3881, 0.6983],\n",
       "         [0.3894, 0.6972],\n",
       "         [0.3920, 0.6960],\n",
       "         [0.4019, 0.7031],\n",
       "         [0.3935, 0.6945],\n",
       "         [0.3909, 0.6965],\n",
       "         [0.4712, 0.7338],\n",
       "         [0.3913, 0.6963],\n",
       "         [0.3942, 0.6943],\n",
       "         [0.3862, 0.7000],\n",
       "         [0.4115, 0.7346],\n",
       "         [0.3903, 0.6971],\n",
       "         [0.4519, 0.7832],\n",
       "         [0.3997, 0.7191],\n",
       "         [0.4525, 0.7281],\n",
       "         [0.3875, 0.6988],\n",
       "         [0.4093, 0.7362],\n",
       "         [0.4578, 0.8035],\n",
       "         [0.4558, 0.7933],\n",
       "         [0.4321, 0.6950],\n",
       "         [0.3918, 0.6960],\n",
       "         [0.4639, 0.8055],\n",
       "         [0.3908, 0.6966],\n",
       "         [0.3867, 0.6993]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3951, 0.6911],\n",
       "         [0.4779, 0.8120],\n",
       "         [0.3914, 0.6942],\n",
       "         [0.3951, 0.6914],\n",
       "         [0.3986, 0.6886],\n",
       "         [0.3936, 0.6920],\n",
       "         [0.3947, 0.6910],\n",
       "         [0.4057, 0.6965],\n",
       "         [0.4149, 0.6912],\n",
       "         [0.3999, 0.6874],\n",
       "         [0.3964, 0.6902],\n",
       "         [0.4637, 0.7298],\n",
       "         [0.3974, 0.6895],\n",
       "         [0.4005, 0.6873],\n",
       "         [0.3914, 0.6939],\n",
       "         [0.4169, 0.7032],\n",
       "         [0.3961, 0.6905],\n",
       "         [0.4693, 0.7824],\n",
       "         [0.4155, 0.7200],\n",
       "         [0.4456, 0.7097],\n",
       "         [0.3930, 0.6924],\n",
       "         [0.4194, 0.7396],\n",
       "         [0.4724, 0.7990],\n",
       "         [0.4773, 0.7960],\n",
       "         [0.3933, 0.6925],\n",
       "         [0.3976, 0.6894],\n",
       "         [0.4787, 0.8066],\n",
       "         [0.3989, 0.6882],\n",
       "         [0.3924, 0.6929]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4089, 0.6840],\n",
       "         [0.4914, 0.7883],\n",
       "         [0.4055, 0.6868],\n",
       "         [0.4082, 0.6847],\n",
       "         [0.4310, 0.6815],\n",
       "         [0.4076, 0.6846],\n",
       "         [0.4083, 0.6841],\n",
       "         [0.4113, 0.6824],\n",
       "         [0.4172, 0.6775],\n",
       "         [0.4133, 0.6806],\n",
       "         [0.4103, 0.6830],\n",
       "         [0.4813, 0.7412],\n",
       "         [0.4101, 0.6831],\n",
       "         [0.4134, 0.6808],\n",
       "         [0.4054, 0.6865],\n",
       "         [0.4393, 0.7123],\n",
       "         [0.4093, 0.6837],\n",
       "         [0.4909, 0.7828],\n",
       "         [0.4447, 0.7094],\n",
       "         [0.4663, 0.7253],\n",
       "         [0.4070, 0.6851],\n",
       "         [0.4468, 0.7478],\n",
       "         [0.4877, 0.7793],\n",
       "         [0.4891, 0.7650],\n",
       "         [0.4073, 0.6852],\n",
       "         [0.4113, 0.6824],\n",
       "         [0.4914, 0.7883],\n",
       "         [0.4102, 0.6830],\n",
       "         [0.4062, 0.6857]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4192, 0.7107],\n",
       "         [0.5717, 0.7414],\n",
       "         [0.3568, 0.7231],\n",
       "         [0.3601, 0.7206],\n",
       "         [0.3875, 0.7106],\n",
       "         [0.3584, 0.7215],\n",
       "         [0.3588, 0.7211],\n",
       "         [0.3721, 0.7219],\n",
       "         [0.3662, 0.7158],\n",
       "         [0.3632, 0.7181],\n",
       "         [0.3601, 0.7204],\n",
       "         [0.4898, 0.6864],\n",
       "         [0.3605, 0.7202],\n",
       "         [0.3631, 0.7184],\n",
       "         [0.3567, 0.7229],\n",
       "         [0.3950, 0.7275],\n",
       "         [0.3590, 0.7213],\n",
       "         [0.5616, 0.7324],\n",
       "         [0.4207, 0.7090],\n",
       "         [0.5170, 0.7001],\n",
       "         [0.3575, 0.7221],\n",
       "         [0.3750, 0.7201],\n",
       "         [0.5693, 0.7376],\n",
       "         [0.5687, 0.7327],\n",
       "         [0.4352, 0.7187],\n",
       "         [0.4393, 0.6943],\n",
       "         [0.5711, 0.7366],\n",
       "         [0.3607, 0.7200],\n",
       "         [0.3569, 0.7225]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4155, 0.6894],\n",
       "         [0.4129, 0.8319],\n",
       "         [0.4019, 0.6883],\n",
       "         [0.4043, 0.6864],\n",
       "         [0.4028, 0.6992],\n",
       "         [0.4032, 0.6868],\n",
       "         [0.4045, 0.6858],\n",
       "         [0.4072, 0.6843],\n",
       "         [0.4030, 0.7001],\n",
       "         [0.4086, 0.6830],\n",
       "         [0.4063, 0.6848],\n",
       "         [0.4515, 0.7076],\n",
       "         [0.4065, 0.6847],\n",
       "         [0.4096, 0.6825],\n",
       "         [0.4022, 0.6878],\n",
       "         [0.4009, 0.7234],\n",
       "         [0.4057, 0.6853],\n",
       "         [0.4166, 0.8174],\n",
       "         [0.4186, 0.6990],\n",
       "         [0.4197, 0.7801],\n",
       "         [0.4031, 0.6870],\n",
       "         [0.4108, 0.7524],\n",
       "         [0.4185, 0.8070],\n",
       "         [0.4198, 0.8062],\n",
       "         [0.4292, 0.7127],\n",
       "         [0.4071, 0.6843],\n",
       "         [0.4129, 0.8319],\n",
       "         [0.4063, 0.6848],\n",
       "         [0.4032, 0.6869]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4123, 0.6952],\n",
       "         [0.5499, 0.7653],\n",
       "         [0.3949, 0.6913],\n",
       "         [0.3975, 0.6892],\n",
       "         [0.4022, 0.6856],\n",
       "         [0.3969, 0.6890],\n",
       "         [0.3981, 0.6881],\n",
       "         [0.4009, 0.6867],\n",
       "         [0.4088, 0.6802],\n",
       "         [0.4035, 0.6843],\n",
       "         [0.4003, 0.6869],\n",
       "         [0.4720, 0.6839],\n",
       "         [0.4005, 0.6868],\n",
       "         [0.4036, 0.6846],\n",
       "         [0.3946, 0.6911],\n",
       "         [0.4487, 0.7135],\n",
       "         [0.3991, 0.6878],\n",
       "         [0.5490, 0.7594],\n",
       "         [0.4272, 0.6998],\n",
       "         [0.5052, 0.7204],\n",
       "         [0.3965, 0.6894],\n",
       "         [0.4150, 0.6924],\n",
       "         [0.5415, 0.7566],\n",
       "         [0.5318, 0.7557],\n",
       "         [0.4681, 0.6856],\n",
       "         [0.4014, 0.6862],\n",
       "         [0.5499, 0.7653],\n",
       "         [0.3999, 0.6872],\n",
       "         [0.3955, 0.6901]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4016, 0.6825],\n",
       "         [0.5126, 0.8060],\n",
       "         [0.3848, 0.6964],\n",
       "         [0.3972, 0.6869],\n",
       "         [0.4143, 0.6733],\n",
       "         [0.3887, 0.6907],\n",
       "         [0.3991, 0.6825],\n",
       "         [0.4124, 0.6752],\n",
       "         [0.4387, 0.6530],\n",
       "         [0.4177, 0.6695],\n",
       "         [0.4063, 0.6789],\n",
       "         [0.4759, 0.6923],\n",
       "         [0.4068, 0.6786],\n",
       "         [0.4207, 0.6689],\n",
       "         [0.3853, 0.6945],\n",
       "         [0.4263, 0.7270],\n",
       "         [0.4038, 0.6811],\n",
       "         [0.5092, 0.7831],\n",
       "         [0.4156, 0.6709],\n",
       "         [0.4652, 0.6998],\n",
       "         [0.3904, 0.6897],\n",
       "         [0.4524, 0.6440],\n",
       "         [0.5092, 0.7831],\n",
       "         [0.5092, 0.7831],\n",
       "         [0.3951, 0.6870],\n",
       "         [0.4098, 0.6770],\n",
       "         [0.5126, 0.8060],\n",
       "         [0.4064, 0.6786],\n",
       "         [0.3867, 0.6922]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4721, 0.6312],\n",
       "         [0.4256, 0.8348],\n",
       "         [0.4634, 0.6389],\n",
       "         [0.4712, 0.6327],\n",
       "         [0.4810, 0.6245],\n",
       "         [0.4661, 0.6348],\n",
       "         [0.4697, 0.6318],\n",
       "         [0.4807, 0.6252],\n",
       "         [0.5017, 0.6071],\n",
       "         [0.4866, 0.6195],\n",
       "         [0.4775, 0.6270],\n",
       "         [0.4304, 0.6645],\n",
       "         [0.4772, 0.6274],\n",
       "         [0.4894, 0.6184],\n",
       "         [0.4605, 0.6400],\n",
       "         [0.4200, 0.7538],\n",
       "         [0.4746, 0.6295],\n",
       "         [0.4268, 0.7999],\n",
       "         [0.4296, 0.7104],\n",
       "         [0.4850, 0.7126],\n",
       "         [0.4652, 0.6356],\n",
       "         [0.4154, 0.7563],\n",
       "         [0.4175, 0.8231],\n",
       "         [0.4256, 0.8348],\n",
       "         [0.4683, 0.6340],\n",
       "         [0.4837, 0.6226],\n",
       "         [0.4256, 0.8348],\n",
       "         [0.4750, 0.6287],\n",
       "         [0.4626, 0.6374]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4592, 0.6925],\n",
       "         [0.4297, 0.7630],\n",
       "         [0.1780, 0.8878],\n",
       "         [0.5631, 0.6321],\n",
       "         [0.8204, 0.3667],\n",
       "         [0.1658, 0.8669],\n",
       "         [0.1325, 0.8856],\n",
       "         [0.7915, 0.4077],\n",
       "         [0.9556, 0.1157],\n",
       "         [0.8662, 0.2839],\n",
       "         [0.5672, 0.6121],\n",
       "         [0.0021, 0.9976],\n",
       "         [0.6443, 0.5482],\n",
       "         [0.8582, 0.3247],\n",
       "         [0.1111, 0.9146],\n",
       "         [0.9401, 0.1580],\n",
       "         [0.4801, 0.6858],\n",
       "         [0.4821, 0.6784],\n",
       "         [0.6791, 0.4938],\n",
       "         [0.0143, 0.9883],\n",
       "         [0.1409, 0.8849],\n",
       "         [0.9673, 0.0997],\n",
       "         [0.4814, 0.7196],\n",
       "         [0.0289, 0.9811],\n",
       "         [0.1626, 0.8822],\n",
       "         [0.7021, 0.5108],\n",
       "         [0.8868, 0.3353],\n",
       "         [0.6296, 0.5551],\n",
       "         [0.1184, 0.8991]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3286, 0.7385],\n",
       "         [0.5786, 0.7441],\n",
       "         [0.2901, 0.7686],\n",
       "         [0.3227, 0.7447],\n",
       "         [0.3498, 0.7237],\n",
       "         [0.3048, 0.7535],\n",
       "         [0.3201, 0.7416],\n",
       "         [0.3436, 0.7293],\n",
       "         [0.3994, 0.6835],\n",
       "         [0.3562, 0.7170],\n",
       "         [0.3389, 0.7310],\n",
       "         [0.5786, 0.7441],\n",
       "         [0.3376, 0.7323],\n",
       "         [0.3617, 0.7159],\n",
       "         [0.3009, 0.7584],\n",
       "         [0.4001, 0.6853],\n",
       "         [0.3313, 0.7371],\n",
       "         [0.5786, 0.7441],\n",
       "         [0.3500, 0.7214],\n",
       "         [0.2676, 0.7862],\n",
       "         [0.3114, 0.7490],\n",
       "         [0.4319, 0.6622],\n",
       "         [0.5786, 0.7441],\n",
       "         [0.5786, 0.7441],\n",
       "         [0.3143, 0.7487],\n",
       "         [0.3454, 0.7275],\n",
       "         [0.5786, 0.7441],\n",
       "         [0.3358, 0.7330],\n",
       "         [0.3067, 0.7520]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3142, 0.7550],\n",
       "         [0.5497, 0.7566],\n",
       "         [0.2755, 0.7862],\n",
       "         [0.3063, 0.7641],\n",
       "         [0.3629, 0.7214],\n",
       "         [0.2896, 0.7681],\n",
       "         [0.3051, 0.7561],\n",
       "         [0.3521, 0.7309],\n",
       "         [0.4548, 0.6467],\n",
       "         [0.3852, 0.7016],\n",
       "         [0.3327, 0.7417],\n",
       "         [0.5497, 0.7566],\n",
       "         [0.3359, 0.7398],\n",
       "         [0.3867, 0.7055],\n",
       "         [0.2619, 0.7918],\n",
       "         [0.4712, 0.6390],\n",
       "         [0.3187, 0.7527],\n",
       "         [0.5497, 0.7566],\n",
       "         [0.3648, 0.7157],\n",
       "         [0.2066, 0.8368],\n",
       "         [0.2875, 0.7705],\n",
       "         [0.5191, 0.6026],\n",
       "         [0.5497, 0.7566],\n",
       "         [0.5497, 0.7566],\n",
       "         [0.2846, 0.7757],\n",
       "         [0.3600, 0.7238],\n",
       "         [0.5497, 0.7566],\n",
       "         [0.3166, 0.7524],\n",
       "         [0.2728, 0.7804]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4104, 0.6870],\n",
       "         [0.5649, 0.7186],\n",
       "         [0.3893, 0.7039],\n",
       "         [0.4046, 0.6925],\n",
       "         [0.4937, 0.6849],\n",
       "         [0.3992, 0.6939],\n",
       "         [0.4041, 0.6900],\n",
       "         [0.4178, 0.6827],\n",
       "         [0.4959, 0.6835],\n",
       "         [0.4279, 0.6736],\n",
       "         [0.4113, 0.6865],\n",
       "         [0.5423, 0.6955],\n",
       "         [0.4131, 0.6853],\n",
       "         [0.4263, 0.6763],\n",
       "         [0.3913, 0.7007],\n",
       "         [0.4937, 0.6849],\n",
       "         [0.4082, 0.6890],\n",
       "         [0.5649, 0.7186],\n",
       "         [0.5290, 0.7083],\n",
       "         [0.5423, 0.6955],\n",
       "         [0.3967, 0.6960],\n",
       "         [0.4625, 0.6486],\n",
       "         [0.5290, 0.7083],\n",
       "         [0.5649, 0.7186],\n",
       "         [0.4003, 0.6942],\n",
       "         [0.4169, 0.6829],\n",
       "         [0.5649, 0.7186],\n",
       "         [0.4138, 0.6843],\n",
       "         [0.3928, 0.6987]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4376, 0.6852],\n",
       "         [0.5507, 0.7512],\n",
       "         [0.3828, 0.7042],\n",
       "         [0.3882, 0.7002],\n",
       "         [0.4216, 0.6981],\n",
       "         [0.3851, 0.7016],\n",
       "         [0.3887, 0.6989],\n",
       "         [0.3928, 0.6968],\n",
       "         [0.4573, 0.6925],\n",
       "         [0.3972, 0.6930],\n",
       "         [0.3908, 0.6980],\n",
       "         [0.4870, 0.6895],\n",
       "         [0.3916, 0.6974],\n",
       "         [0.3960, 0.6944],\n",
       "         [0.3835, 0.7032],\n",
       "         [0.4254, 0.7145],\n",
       "         [0.3904, 0.6983],\n",
       "         [0.5388, 0.7375],\n",
       "         [0.4472, 0.7182],\n",
       "         [0.5471, 0.7323],\n",
       "         [0.3851, 0.7017],\n",
       "         [0.4082, 0.6852],\n",
       "         [0.5434, 0.7461],\n",
       "         [0.5400, 0.7363],\n",
       "         [0.4137, 0.7022],\n",
       "         [0.3925, 0.6969],\n",
       "         [0.5507, 0.7512],\n",
       "         [0.3899, 0.6985],\n",
       "         [0.3844, 0.7021]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3463, 0.7318],\n",
       "         [0.4079, 0.8266],\n",
       "         [0.3376, 0.7385],\n",
       "         [0.3438, 0.7340],\n",
       "         [0.3913, 0.7426],\n",
       "         [0.3401, 0.7356],\n",
       "         [0.3454, 0.7317],\n",
       "         [0.3489, 0.7302],\n",
       "         [0.3514, 0.7521],\n",
       "         [0.3534, 0.7264],\n",
       "         [0.3490, 0.7298],\n",
       "         [0.4290, 0.7837],\n",
       "         [0.3485, 0.7302],\n",
       "         [0.3540, 0.7265],\n",
       "         [0.3406, 0.7358],\n",
       "         [0.3684, 0.7377],\n",
       "         [0.3480, 0.7306],\n",
       "         [0.4079, 0.8266],\n",
       "         [0.3511, 0.7503],\n",
       "         [0.4012, 0.7705],\n",
       "         [0.3427, 0.7339],\n",
       "         [0.3511, 0.7503],\n",
       "         [0.4023, 0.8198],\n",
       "         [0.4119, 0.8146],\n",
       "         [0.3446, 0.7329],\n",
       "         [0.3782, 0.7308],\n",
       "         [0.4079, 0.8266],\n",
       "         [0.3490, 0.7298],\n",
       "         [0.3421, 0.7343]], grad_fn=<SigmoidBackward>)]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_l = [list(np.squeeze(i.detach().numpy())) for i in y_hat_l]\n",
    "y_hat_l = [z for y in y_hat_l for z in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_l = [y[1] for y in y_hat_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.67061865,\n",
       " 0.7966896,\n",
       " 0.70161974,\n",
       " 0.67922944,\n",
       " 0.6509174,\n",
       " 0.68296516,\n",
       " 0.66900086,\n",
       " 0.64990574,\n",
       " 0.6028081,\n",
       " 0.6401323,\n",
       " 0.66565436,\n",
       " 0.75427705,\n",
       " 0.66087997,\n",
       " 0.64360666,\n",
       " 0.69332373,\n",
       " 0.7603342,\n",
       " 0.6689626,\n",
       " 0.7966896,\n",
       " 0.6467989,\n",
       " 0.7966896,\n",
       " 0.68361264,\n",
       " 0.5899542,\n",
       " 0.7966896,\n",
       " 0.7304265,\n",
       " 0.6871339,\n",
       " 0.65983117,\n",
       " 0.7966896,\n",
       " 0.6631817,\n",
       " 0.69259286,\n",
       " 0.7534302,\n",
       " 0.82766867,\n",
       " 0.8074023,\n",
       " 0.7656217,\n",
       " 0.7249651,\n",
       " 0.78235877,\n",
       " 0.7635128,\n",
       " 0.7355015,\n",
       " 0.62989587,\n",
       " 0.7031404,\n",
       " 0.742613,\n",
       " 0.82766867,\n",
       " 0.75561064,\n",
       " 0.70897025,\n",
       " 0.7987101,\n",
       " 0.62970644,\n",
       " 0.75784475,\n",
       " 0.82766867,\n",
       " 0.72149986,\n",
       " 0.8448559,\n",
       " 0.7798597,\n",
       " 0.58986133,\n",
       " 0.75889605,\n",
       " 0.82766867,\n",
       " 0.78079814,\n",
       " 0.7315216,\n",
       " 0.82766867,\n",
       " 0.74217516,\n",
       " 0.7833304,\n",
       " 0.66491127,\n",
       " 0.7529848,\n",
       " 0.6768209,\n",
       " 0.6695302,\n",
       " 0.68941957,\n",
       " 0.6685791,\n",
       " 0.667408,\n",
       " 0.6605773,\n",
       " 0.6420391,\n",
       " 0.6525958,\n",
       " 0.6633989,\n",
       " 0.66022336,\n",
       " 0.6628683,\n",
       " 0.65599704,\n",
       " 0.675304,\n",
       " 0.66022336,\n",
       " 0.6651538,\n",
       " 0.7529848,\n",
       " 0.6590826,\n",
       " 0.6791147,\n",
       " 0.6713051,\n",
       " 0.7077246,\n",
       " 0.7506908,\n",
       " 0.72445786,\n",
       " 0.66022336,\n",
       " 0.6595785,\n",
       " 0.7529849,\n",
       " 0.66601723,\n",
       " 0.6736615,\n",
       " 0.6679597,\n",
       " 0.7907068,\n",
       " 0.67244375,\n",
       " 0.66943645,\n",
       " 0.69913656,\n",
       " 0.66924936,\n",
       " 0.6681977,\n",
       " 0.66686213,\n",
       " 0.65976685,\n",
       " 0.6635674,\n",
       " 0.66677266,\n",
       " 0.69019467,\n",
       " 0.6655482,\n",
       " 0.66270655,\n",
       " 0.67213494,\n",
       " 0.7319661,\n",
       " 0.6684512,\n",
       " 0.77355766,\n",
       " 0.69626063,\n",
       " 0.7570651,\n",
       " 0.67036897,\n",
       " 0.743058,\n",
       " 0.7761354,\n",
       " 0.7562153,\n",
       " 0.67049545,\n",
       " 0.66649103,\n",
       " 0.7907068,\n",
       " 0.66758907,\n",
       " 0.6709424,\n",
       " 0.6527584,\n",
       " 0.6306597,\n",
       " 0.65775293,\n",
       " 0.65200573,\n",
       " 0.6416904,\n",
       " 0.6543019,\n",
       " 0.6524009,\n",
       " 0.6501638,\n",
       " 0.6393929,\n",
       " 0.6450236,\n",
       " 0.65079296,\n",
       " 0.6238257,\n",
       " 0.65153295,\n",
       " 0.64577204,\n",
       " 0.6574387,\n",
       " 0.62903357,\n",
       " 0.6519968,\n",
       " 0.62834245,\n",
       " 0.6342449,\n",
       " 0.61840034,\n",
       " 0.655222,\n",
       " 0.63370544,\n",
       " 0.62874764,\n",
       " 0.6201505,\n",
       " 0.648642,\n",
       " 0.6416903,\n",
       " 0.63065964,\n",
       " 0.64871854,\n",
       " 0.65683234,\n",
       " 0.72699773,\n",
       " 0.8744779,\n",
       " 0.7302669,\n",
       " 0.7270767,\n",
       " 0.72081184,\n",
       " 0.729372,\n",
       " 0.7256537,\n",
       " 0.7222052,\n",
       " 0.71523094,\n",
       " 0.72067463,\n",
       " 0.7244411,\n",
       " 0.8097228,\n",
       " 0.72353923,\n",
       " 0.72052586,\n",
       " 0.7293735,\n",
       " 0.79431814,\n",
       " 0.72472554,\n",
       " 0.8686776,\n",
       " 0.72068244,\n",
       " 0.7913393,\n",
       " 0.72942656,\n",
       " 0.7670799,\n",
       " 0.8450623,\n",
       " 0.84068835,\n",
       " 0.729611,\n",
       " 0.72424686,\n",
       " 0.8744779,\n",
       " 0.7242832,\n",
       " 0.7282336,\n",
       " 0.66640174,\n",
       " 0.7724207,\n",
       " 0.66835517,\n",
       " 0.66625,\n",
       " 0.6783268,\n",
       " 0.6666543,\n",
       " 0.66664857,\n",
       " 0.6762815,\n",
       " 0.6833644,\n",
       " 0.66229963,\n",
       " 0.66522944,\n",
       " 0.68901944,\n",
       " 0.66464937,\n",
       " 0.6623704,\n",
       " 0.6690794,\n",
       " 0.7164414,\n",
       " 0.6656716,\n",
       " 0.7616495,\n",
       " 0.6989245,\n",
       " 0.7148676,\n",
       " 0.66747206,\n",
       " 0.6790739,\n",
       " 0.7677643,\n",
       " 0.7538212,\n",
       " 0.6710584,\n",
       " 0.6639733,\n",
       " 0.7724207,\n",
       " 0.6653148,\n",
       " 0.66858834,\n",
       " 0.6794456,\n",
       " 0.7031675,\n",
       " 0.68252164,\n",
       " 0.6802776,\n",
       " 0.6772472,\n",
       " 0.68198913,\n",
       " 0.6792597,\n",
       " 0.6773579,\n",
       " 0.6710826,\n",
       " 0.6758592,\n",
       " 0.67735195,\n",
       " 0.6552504,\n",
       " 0.6785053,\n",
       " 0.67599976,\n",
       " 0.6825126,\n",
       " 0.67197293,\n",
       " 0.67924345,\n",
       " 0.69171226,\n",
       " 0.67216533,\n",
       " 0.6654887,\n",
       " 0.68101,\n",
       " 0.6765168,\n",
       " 0.68806714,\n",
       " 0.6961432,\n",
       " 0.68009883,\n",
       " 0.6774344,\n",
       " 0.70316744,\n",
       " 0.6760932,\n",
       " 0.68215674,\n",
       " 0.7068154,\n",
       " 0.77422637,\n",
       " 0.7106824,\n",
       " 0.70750177,\n",
       " 0.7037088,\n",
       " 0.7070132,\n",
       " 0.7068764,\n",
       " 0.7041777,\n",
       " 0.6977107,\n",
       " 0.7024518,\n",
       " 0.7055904,\n",
       " 0.7384907,\n",
       " 0.7056623,\n",
       " 0.7030647,\n",
       " 0.70993114,\n",
       " 0.7068928,\n",
       " 0.70618117,\n",
       " 0.7639773,\n",
       " 0.73420405,\n",
       " 0.7301206,\n",
       " 0.70854247,\n",
       " 0.72860366,\n",
       " 0.75812006,\n",
       " 0.7742263,\n",
       " 0.70835215,\n",
       " 0.70492893,\n",
       " 0.7742263,\n",
       " 0.7058447,\n",
       " 0.7089211,\n",
       " 0.69471616,\n",
       " 0.8336005,\n",
       " 0.6869314,\n",
       " 0.6857665,\n",
       " 0.6814209,\n",
       " 0.6825406,\n",
       " 0.68374556,\n",
       " 0.6824053,\n",
       " 0.6769146,\n",
       " 0.68021107,\n",
       " 0.6814125,\n",
       " 0.74784493,\n",
       " 0.6824548,\n",
       " 0.68104404,\n",
       " 0.6863837,\n",
       " 0.7434236,\n",
       " 0.68350595,\n",
       " 0.813928,\n",
       " 0.72896016,\n",
       " 0.746944,\n",
       " 0.68453246,\n",
       " 0.74235517,\n",
       " 0.81894773,\n",
       " 0.80911183,\n",
       " 0.6851561,\n",
       " 0.68166924,\n",
       " 0.83360064,\n",
       " 0.68456024,\n",
       " 0.6846034,\n",
       " 0.6836097,\n",
       " 0.77054584,\n",
       " 0.6833292,\n",
       " 0.6811099,\n",
       " 0.67878234,\n",
       " 0.6815154,\n",
       " 0.6808903,\n",
       " 0.67980105,\n",
       " 0.6751631,\n",
       " 0.67786413,\n",
       " 0.6801273,\n",
       " 0.6726219,\n",
       " 0.67981124,\n",
       " 0.67778206,\n",
       " 0.68373615,\n",
       " 0.70473164,\n",
       " 0.68070316,\n",
       " 0.75179577,\n",
       " 0.68457454,\n",
       " 0.7015295,\n",
       " 0.6818817,\n",
       " 0.6977422,\n",
       " 0.74678224,\n",
       " 0.761608,\n",
       " 0.69260126,\n",
       " 0.6789465,\n",
       " 0.7705459,\n",
       " 0.67976093,\n",
       " 0.6824208,\n",
       " 0.6826929,\n",
       " 0.79709685,\n",
       " 0.6855246,\n",
       " 0.68201256,\n",
       " 0.7068624,\n",
       " 0.68398756,\n",
       " 0.6826043,\n",
       " 0.69332033,\n",
       " 0.6930894,\n",
       " 0.6787202,\n",
       " 0.68160605,\n",
       " 0.6999495,\n",
       " 0.6813123,\n",
       " 0.6781209,\n",
       " 0.68579686,\n",
       " 0.71963227,\n",
       " 0.6819557,\n",
       " 0.7873488,\n",
       " 0.69423085,\n",
       " 0.7304943,\n",
       " 0.6841996,\n",
       " 0.70513755,\n",
       " 0.7903544,\n",
       " 0.7780269,\n",
       " 0.6930895,\n",
       " 0.68098253,\n",
       " 0.79709685,\n",
       " 0.6809606,\n",
       " 0.68435866,\n",
       " 0.6974849,\n",
       " 0.7795192,\n",
       " 0.7037657,\n",
       " 0.6982111,\n",
       " 0.7039864,\n",
       " 0.69897187,\n",
       " 0.69695306,\n",
       " 0.69383925,\n",
       " 0.70086765,\n",
       " 0.6901736,\n",
       " 0.69518554,\n",
       " 0.71123064,\n",
       " 0.695373,\n",
       " 0.69121337,\n",
       " 0.70263726,\n",
       " 0.7250268,\n",
       " 0.69702137,\n",
       " 0.7795192,\n",
       " 0.69176966,\n",
       " 0.72859216,\n",
       " 0.6998944,\n",
       " 0.7250268,\n",
       " 0.766061,\n",
       " 0.7706605,\n",
       " 0.7003463,\n",
       " 0.69425505,\n",
       " 0.7795192,\n",
       " 0.69492584,\n",
       " 0.7005466,\n",
       " 0.6680407,\n",
       " 0.7931945,\n",
       " 0.67580914,\n",
       " 0.6699705,\n",
       " 0.6650266,\n",
       " 0.67116797,\n",
       " 0.66918194,\n",
       " 0.66553533,\n",
       " 0.685632,\n",
       " 0.66227967,\n",
       " 0.66665566,\n",
       " 0.7168015,\n",
       " 0.6672558,\n",
       " 0.6634751,\n",
       " 0.6753088,\n",
       " 0.71542317,\n",
       " 0.66887474,\n",
       " 0.7931945,\n",
       " 0.69528043,\n",
       " 0.7108789,\n",
       " 0.67158103,\n",
       " 0.7454651,\n",
       " 0.7931945,\n",
       " 0.7931945,\n",
       " 0.67216384,\n",
       " 0.6666334,\n",
       " 0.7931945,\n",
       " 0.6674494,\n",
       " 0.6727876,\n",
       " 0.7265874,\n",
       " 0.8284957,\n",
       " 0.72841156,\n",
       " 0.7258973,\n",
       " 0.7412485,\n",
       " 0.72581476,\n",
       " 0.72522473,\n",
       " 0.7412485,\n",
       " 0.7412485,\n",
       " 0.72270733,\n",
       " 0.7239091,\n",
       " 0.7612745,\n",
       " 0.72428286,\n",
       " 0.7222277,\n",
       " 0.7279707,\n",
       " 0.7657184,\n",
       " 0.7236101,\n",
       " 0.8149352,\n",
       " 0.7427615,\n",
       " 0.77994335,\n",
       " 0.7263484,\n",
       " 0.75209874,\n",
       " 0.82247967,\n",
       " 0.8214315,\n",
       " 0.72709376,\n",
       " 0.7230306,\n",
       " 0.82849556,\n",
       " 0.7256614,\n",
       " 0.72677517,\n",
       " 0.6997624,\n",
       " 0.7617843,\n",
       " 0.7088507,\n",
       " 0.70208615,\n",
       " 0.69483703,\n",
       " 0.7025,\n",
       " 0.70014185,\n",
       " 0.6978566,\n",
       " 0.6805951,\n",
       " 0.69261354,\n",
       " 0.6973966,\n",
       " 0.69991827,\n",
       " 0.6978765,\n",
       " 0.6975609,\n",
       " 0.7079554,\n",
       " 0.72175497,\n",
       " 0.6984613,\n",
       " 0.7586513,\n",
       " 0.69402903,\n",
       " 0.71022296,\n",
       " 0.70294905,\n",
       " 0.6748334,\n",
       " 0.7508009,\n",
       " 0.7617843,\n",
       " 0.70252985,\n",
       " 0.6939732,\n",
       " 0.7617843,\n",
       " 0.69955164,\n",
       " 0.70437247,\n",
       " 0.64375114,\n",
       " 0.7841586,\n",
       " 0.6514859,\n",
       " 0.67583805,\n",
       " 0.6885742,\n",
       " 0.6463914,\n",
       " 0.64019984,\n",
       " 0.6313324,\n",
       " 0.6696173,\n",
       " 0.6250645,\n",
       " 0.6382123,\n",
       " 0.6696173,\n",
       " 0.6304544,\n",
       " 0.6248266,\n",
       " 0.65154964,\n",
       " 0.6951591,\n",
       " 0.63904876,\n",
       " 0.7586491,\n",
       " 0.69515914,\n",
       " 0.66509885,\n",
       " 0.6474988,\n",
       " 0.60195017,\n",
       " 0.77166706,\n",
       " 0.7618074,\n",
       " 0.6449369,\n",
       " 0.63352734,\n",
       " 0.7841585,\n",
       " 0.63570654,\n",
       " 0.64875966,\n",
       " 0.6006219,\n",
       " 0.7286207,\n",
       " 0.6189115,\n",
       " 0.6059369,\n",
       " 0.5948533,\n",
       " 0.6118864,\n",
       " 0.6076595,\n",
       " 0.596103,\n",
       " 0.5748274,\n",
       " 0.5877336,\n",
       " 0.5992546,\n",
       " 0.65324336,\n",
       " 0.60121673,\n",
       " 0.5931564,\n",
       " 0.61760056,\n",
       " 0.6832675,\n",
       " 0.60489774,\n",
       " 0.7084829,\n",
       " 0.6855471,\n",
       " 0.64388806,\n",
       " 0.6149113,\n",
       " 0.65324336,\n",
       " 0.72862077,\n",
       " 0.72862077,\n",
       " 0.61020046,\n",
       " 0.59719914,\n",
       " 0.72862077,\n",
       " 0.60572165,\n",
       " 0.6125586,\n",
       " 0.72335774,\n",
       " 0.8282976,\n",
       " 0.7238025,\n",
       " 0.72123796,\n",
       " 0.71751225,\n",
       " 0.7212675,\n",
       " 0.7203294,\n",
       " 0.72357595,\n",
       " 0.7128794,\n",
       " 0.71672446,\n",
       " 0.7190241,\n",
       " 0.78043765,\n",
       " 0.71863973,\n",
       " 0.71661466,\n",
       " 0.722468,\n",
       " 0.77358574,\n",
       " 0.71945417,\n",
       " 0.8222707,\n",
       " 0.72433525,\n",
       " 0.7714297,\n",
       " 0.72110605,\n",
       " 0.74250346,\n",
       " 0.8282976,\n",
       " 0.80533683,\n",
       " 0.7233577,\n",
       " 0.71882296,\n",
       " 0.8282976,\n",
       " 0.71890074,\n",
       " 0.72190905,\n",
       " 0.6812053,\n",
       " 0.7751849,\n",
       " 0.6727487,\n",
       " 0.6673178,\n",
       " 0.7175343,\n",
       " 0.66934913,\n",
       " 0.6658232,\n",
       " 0.6639894,\n",
       " 0.6533532,\n",
       " 0.6604312,\n",
       " 0.6645764,\n",
       " 0.68264276,\n",
       " 0.66400564,\n",
       " 0.66197425,\n",
       " 0.671994,\n",
       " 0.7354904,\n",
       " 0.6667236,\n",
       " 0.7597675,\n",
       " 0.6812053,\n",
       " 0.7239476,\n",
       " 0.669639,\n",
       " 0.67920196,\n",
       " 0.76115066,\n",
       " 0.74843264,\n",
       " 0.68299854,\n",
       " 0.6812053,\n",
       " 0.7751849,\n",
       " 0.6649858,\n",
       " 0.66998494,\n",
       " 0.68323594,\n",
       " 0.76787156,\n",
       " 0.6905538,\n",
       " 0.68540686,\n",
       " 0.68034166,\n",
       " 0.6860365,\n",
       " 0.68355334,\n",
       " 0.6811821,\n",
       " 0.66908,\n",
       " 0.6770874,\n",
       " 0.6821288,\n",
       " 0.72049886,\n",
       " 0.6816144,\n",
       " 0.6778902,\n",
       " 0.6890825,\n",
       " 0.70796645,\n",
       " 0.68339324,\n",
       " 0.754653,\n",
       " 0.6846933,\n",
       " 0.6909205,\n",
       " 0.6864305,\n",
       " 0.70450807,\n",
       " 0.7678716,\n",
       " 0.7678716,\n",
       " 0.68664664,\n",
       " 0.6804529,\n",
       " 0.7678716,\n",
       " 0.68188673,\n",
       " 0.68595946,\n",
       " 0.6710743,\n",
       " 0.7269852,\n",
       " 0.6750617,\n",
       " 0.6721207,\n",
       " 0.66871953,\n",
       " 0.6712262,\n",
       " 0.6714505,\n",
       " 0.66693753,\n",
       " 0.66224754,\n",
       " 0.6675673,\n",
       " 0.6687456,\n",
       " 0.6605022,\n",
       " 0.67021996,\n",
       " 0.6672941,\n",
       " 0.6747608,\n",
       " 0.68555003,\n",
       " 0.6707095,\n",
       " 0.71891636,\n",
       " 0.67550296,\n",
       " 0.6885269,\n",
       " 0.6727954,\n",
       " 0.6650282,\n",
       " 0.71609086,\n",
       " 0.7141025,\n",
       " 0.6772324,\n",
       " 0.66987157,\n",
       " 0.72534716,\n",
       " 0.67009455,\n",
       " 0.6727078,\n",
       " 0.71311706,\n",
       " 0.71685535,\n",
       " 0.7188086,\n",
       " 0.7151267,\n",
       " 0.70133245,\n",
       " 0.7147685,\n",
       " 0.7141692,\n",
       " 0.7024393,\n",
       " 0.70540273,\n",
       " 0.7101025,\n",
       " 0.71235704,\n",
       " 0.6842152,\n",
       " 0.7133152,\n",
       " 0.7095555,\n",
       " 0.7168463,\n",
       " 0.7108038,\n",
       " 0.7142811,\n",
       " 0.71134514,\n",
       " 0.69638824,\n",
       " 0.6886146,\n",
       " 0.71531785,\n",
       " 0.7083398,\n",
       " 0.71685535,\n",
       " 0.7026665,\n",
       " 0.71080387,\n",
       " 0.6974855,\n",
       " 0.71685535,\n",
       " 0.7130377,\n",
       " 0.7161237,\n",
       " 0.718095,\n",
       " 0.835758,\n",
       " 0.7207903,\n",
       " 0.7206371,\n",
       " 0.71560514,\n",
       " 0.7192468,\n",
       " 0.7181137,\n",
       " 0.71629083,\n",
       " 0.71012276,\n",
       " 0.7146743,\n",
       " 0.71727806,\n",
       " 0.76243603,\n",
       " 0.7170074,\n",
       " 0.7150621,\n",
       " 0.7207048,\n",
       " 0.74600756,\n",
       " 0.7170788,\n",
       " 0.8189908,\n",
       " 0.7287946,\n",
       " 0.7614823,\n",
       " 0.7196342,\n",
       " 0.74986833,\n",
       " 0.8294264,\n",
       " 0.8241186,\n",
       " 0.7192568,\n",
       " 0.7165691,\n",
       " 0.8357579,\n",
       " 0.71813244,\n",
       " 0.7202447,\n",
       " 0.7031444,\n",
       " 0.7325519,\n",
       " 0.706671,\n",
       " 0.70424587,\n",
       " 0.69349533,\n",
       " 0.7049178,\n",
       " 0.7037828,\n",
       " 0.7012343,\n",
       " 0.6962513,\n",
       " 0.7005573,\n",
       " 0.70345926,\n",
       " 0.6763658,\n",
       " 0.7032229,\n",
       " 0.70052695,\n",
       " 0.7068805,\n",
       " 0.7113441,\n",
       " 0.7036379,\n",
       " 0.72432095,\n",
       " 0.69349533,\n",
       " 0.6753812,\n",
       " 0.70495206,\n",
       " 0.70722044,\n",
       " 0.7253758,\n",
       " 0.7280388,\n",
       " 0.69349533,\n",
       " 0.69462717,\n",
       " 0.73255193,\n",
       " 0.70221126,\n",
       " 0.70558953,\n",
       " 0.6966599,\n",
       " 0.80135936,\n",
       " 0.7009649,\n",
       " 0.6977628,\n",
       " 0.6951087,\n",
       " 0.69740456,\n",
       " 0.6975585,\n",
       " 0.69614536,\n",
       " 0.68927425,\n",
       " 0.69299847,\n",
       " 0.69679284,\n",
       " 0.74185294,\n",
       " 0.69665796,\n",
       " 0.6933496,\n",
       " 0.7002863,\n",
       " 0.71314925,\n",
       " 0.69735706,\n",
       " 0.7925959,\n",
       " 0.70294094,\n",
       " 0.71469665,\n",
       " 0.6982981,\n",
       " 0.72397995,\n",
       " 0.78443456,\n",
       " 0.77439404,\n",
       " 0.69883114,\n",
       " 0.69577485,\n",
       " 0.80135936,\n",
       " 0.6958749,\n",
       " 0.6992916,\n",
       " 0.70615435,\n",
       " 0.7648009,\n",
       " 0.7090188,\n",
       " 0.7067,\n",
       " 0.7036136,\n",
       " 0.7075099,\n",
       " 0.70589954,\n",
       " 0.70437044,\n",
       " 0.69963527,\n",
       " 0.70262843,\n",
       " 0.70525676,\n",
       " 0.72979856,\n",
       " 0.7047862,\n",
       " 0.7031119,\n",
       " 0.70831096,\n",
       " 0.72453886,\n",
       " 0.70545435,\n",
       " 0.7594891,\n",
       " 0.706981,\n",
       " 0.7181045,\n",
       " 0.7077365,\n",
       " 0.6972947,\n",
       " 0.7648009,\n",
       " 0.7469584,\n",
       " 0.7068621,\n",
       " 0.7048009,\n",
       " 0.7648009,\n",
       " 0.7056175,\n",
       " 0.7076783,\n",
       " 0.6797285,\n",
       " 0.78803045,\n",
       " 0.6781268,\n",
       " 0.6761583,\n",
       " 0.6854942,\n",
       " 0.67708963,\n",
       " 0.6759167,\n",
       " 0.6746536,\n",
       " 0.6842184,\n",
       " 0.6737751,\n",
       " 0.6753298,\n",
       " 0.7218507,\n",
       " 0.6752567,\n",
       " 0.6740409,\n",
       " 0.67770123,\n",
       " 0.7183062,\n",
       " 0.6758112,\n",
       " 0.77456677,\n",
       " 0.67808515,\n",
       " 0.6924793,\n",
       " 0.67720646,\n",
       " 0.7203088,\n",
       " 0.77904755,\n",
       " 0.76458937,\n",
       " 0.6879323,\n",
       " 0.6714974,\n",
       " 0.78803045,\n",
       " 0.6748316,\n",
       " 0.6772427,\n",
       " 0.6919072,\n",
       " 0.7999328,\n",
       " 0.7012862,\n",
       " 0.6970283,\n",
       " 0.6931841,\n",
       " 0.698282,\n",
       " 0.69659245,\n",
       " 0.70030266,\n",
       " 0.6856694,\n",
       " 0.69182205,\n",
       " 0.6951658,\n",
       " 0.7072221,\n",
       " 0.69467705,\n",
       " 0.69134665,\n",
       " 0.70046103,\n",
       " 0.7187973,\n",
       " 0.69604605,\n",
       " 0.7951827,\n",
       " 0.71591336,\n",
       " 0.73519504,\n",
       " 0.698721,\n",
       " 0.72840947,\n",
       " 0.7999328,\n",
       " 0.79346997,\n",
       " 0.6994932,\n",
       " 0.6947237,\n",
       " 0.7999328,\n",
       " 0.69425195,\n",
       " 0.6992641,\n",
       " 0.7018754,\n",
       " 0.7321886,\n",
       " 0.73021895,\n",
       " 0.7271281,\n",
       " 0.7239342,\n",
       " 0.72841674,\n",
       " 0.72646534,\n",
       " 0.73192614,\n",
       " 0.7179376,\n",
       " 0.7222411,\n",
       " 0.7253462,\n",
       " 0.70179456,\n",
       " 0.7265333,\n",
       " 0.72207236,\n",
       " 0.730179,\n",
       " 0.7126516,\n",
       " 0.7266683,\n",
       " 0.7208071,\n",
       " 0.7180734,\n",
       " 0.7086712,\n",
       " 0.728458,\n",
       " 0.73192614,\n",
       " 0.7320976,\n",
       " 0.72764796,\n",
       " 0.712165,\n",
       " 0.7254012,\n",
       " 0.7321886,\n",
       " 0.7252873,\n",
       " 0.72894734,\n",
       " 0.6927511,\n",
       " 0.7820999,\n",
       " 0.7176369,\n",
       " 0.69564855,\n",
       " 0.67839503,\n",
       " 0.70070654,\n",
       " 0.6948829,\n",
       " 0.6883131,\n",
       " 0.6494624,\n",
       " 0.67441463,\n",
       " 0.69062364,\n",
       " 0.6992322,\n",
       " 0.6897945,\n",
       " 0.6733924,\n",
       " 0.7127519,\n",
       " 0.6992322,\n",
       " 0.69495064,\n",
       " 0.743124,\n",
       " 0.67933685,\n",
       " 0.6992322,\n",
       " 0.6990955,\n",
       " 0.6354127,\n",
       " 0.7820999,\n",
       " 0.743124,\n",
       " 0.7056571,\n",
       " 0.68832076,\n",
       " 0.7820999,\n",
       " 0.6902604,\n",
       " 0.70883965,\n",
       " 0.76143515,\n",
       " 0.81136745,\n",
       " 0.73085415,\n",
       " 0.72066396,\n",
       " 0.7132823,\n",
       " 0.72165483,\n",
       " 0.7215646,\n",
       " 0.7156107,\n",
       " 0.69313097,\n",
       " 0.7099694,\n",
       " 0.7170165,\n",
       " 0.7550939,\n",
       " 0.7163668,\n",
       " 0.7082799,\n",
       " 0.729242,\n",
       " 0.69537187,\n",
       " 0.7185042,\n",
       " 0.77721334,\n",
       " 0.711354,\n",
       " 0.7516152,\n",
       " 0.7241193,\n",
       " 0.68767583,\n",
       " 0.8113675,\n",
       " 0.81136745,\n",
       " 0.76143515,\n",
       " 0.7136598,\n",
       " 0.8113675,\n",
       " 0.71630883,\n",
       " 0.7251148,\n",
       " 0.70556146,\n",
       " 0.7225778,\n",
       " 0.71856856,\n",
       " 0.7064021,\n",
       " 0.69587404,\n",
       " 0.71091545,\n",
       " 0.70599985,\n",
       " 0.6937111,\n",
       " 0.67021424,\n",
       " 0.68981034,\n",
       " 0.6982454,\n",
       " 0.7084912,\n",
       " 0.70099616,\n",
       " 0.69006455,\n",
       " 0.7184085,\n",
       " 0.69371104,\n",
       " 0.70491415,\n",
       " 0.7225778,\n",
       " 0.7010117,\n",
       " 0.72257787,\n",
       " 0.71181554,\n",
       " 0.7010117,\n",
       " 0.7084912,\n",
       " 0.72257787,\n",
       " 0.7102534,\n",
       " 0.69916064,\n",
       " 0.72257787,\n",
       " 0.7016603,\n",
       " 0.7136134,\n",
       " 0.664178,\n",
       " 0.7959556,\n",
       " 0.69484496,\n",
       " 0.6725131,\n",
       " 0.6327103,\n",
       " 0.6824701,\n",
       " 0.66375875,\n",
       " 0.6353878,\n",
       " 0.58974755,\n",
       " 0.623002,\n",
       " 0.65093654,\n",
       " 0.76085407,\n",
       " 0.652175,\n",
       " 0.63304454,\n",
       " 0.6923904,\n",
       " 0.58023673,\n",
       " 0.6615222,\n",
       " 0.79595566,\n",
       " 0.6338641,\n",
       " 0.7297132,\n",
       " 0.6793582,\n",
       " 0.55808234,\n",
       " 0.79595566,\n",
       " 0.79595566,\n",
       " 0.6777225,\n",
       " 0.64822954,\n",
       " 0.79595566,\n",
       " 0.6518454,\n",
       " 0.6855987,\n",
       " 0.6297369,\n",
       " 0.80377364,\n",
       " 0.63912296,\n",
       " 0.63455826,\n",
       " 0.6256752,\n",
       " 0.63437384,\n",
       " 0.63069284,\n",
       " 0.6257854,\n",
       " 0.6763272,\n",
       " 0.6224814,\n",
       " 0.6288187,\n",
       " 0.7114539,\n",
       " 0.6280312,\n",
       " 0.6231042,\n",
       " ...]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5106382408679204"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "y = true_label\n",
    "pred = np.array(y_hat_l)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_list = [1 if x > 0.70 else 0 for x in y_hat_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = []\n",
    "for time, snapshot in enumerate(test_dataset):\n",
    "    true_label.append(list(snapshot.y.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = [int(z) for y in true_label for z in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.45      0.51      0.48       661\n",
      "     class 1       0.57      0.51      0.54       847\n",
      "\n",
      "    accuracy                           0.51      1508\n",
      "   macro avg       0.51      0.51      0.51      1508\n",
      "weighted avg       0.52      0.51      0.51      1508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = true_label\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_true, y_hat_list, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
