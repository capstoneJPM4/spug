{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e095685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d91cc0d",
   "metadata": {},
   "source": [
    "# (change this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f659df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these variables\n",
    "DATA_ARCHIVE_DIR = \"./data_archive\"\n",
    "news_or_twitter = \"twitter\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe750fa",
   "metadata": {},
   "source": [
    "# convert each .npy file to torch trainable\n",
    "\n",
    "You need basically four things: \n",
    "1. **X** (the nodes' attr: right now is current month avg stock prices). You could modify it at cell 5\n",
    "2. **y** (the next period of time's stock prices: right now is next month avg stock prices). You could modify it at cell 5\n",
    "3. **edge_index** (graph edges: right now is based on co-mentions). You could modify it at cell 6\n",
    "4. **edge_attr** (edges' attr: right now is # of co-mentions during that month). You could modify it at cell 6\n",
    "\n",
    "\n",
    "**dataset** would be a list of Data object that could be directly used for training.\n",
    "\n",
    "modify cell 8&9 to update **model architecture** (right now is simple) and **training process** (e.g. threshold, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05fb2a6",
   "metadata": {},
   "source": [
    "# sort the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f91ed33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order the .npy files by time \n",
    "fps = {}\n",
    "for fp in os.listdir(os.path.join(DATA_ARCHIVE_DIR,news_or_twitter)):\n",
    "    if fp.split(\".\")[-1] == \"npy\":\n",
    "        new_name = fp.split(\".\")[0]\n",
    "        if len(new_name.split(\"_\")[1]) == 1:\n",
    "            new_name = new_name.split(\"_\")[0] + \"_\" + \"0\" + new_name.split(\"_\")[1]\n",
    "        fps[fp] = new_name\n",
    "sorted_fps = [k for k, v in sorted(fps.items(), key=lambda item: item[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a4b4a",
   "metadata": {},
   "source": [
    "# get stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b14aeb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the stock prices\n",
    "stock_df = pd.read_csv(\n",
    "            os.path.join(DATA_ARCHIVE_DIR,\"stock\",\"raw.csv\"),\n",
    "            usecols=[\"ticker_symbol\", \"Date\", \"Close\"],\n",
    "            parse_dates=[\"Date\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f45ce",
   "metadata": {},
   "source": [
    "# Prepare X and y (change this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00198d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y = []\n",
    "for fp in sorted_fps:\n",
    "    full_fp = os.path.join(DATA_ARCHIVE_DIR,news_or_twitter,fp)\n",
    "    yr_month = fp.split(\".\")[0]\n",
    "    yr = int(yr_month.split(\"_\")[0])\n",
    "    month = int(yr_month.split(\"_\")[1])\n",
    "    # no further data available\n",
    "    if yr == 2021 and month == 10: \n",
    "        continue\n",
    "    \n",
    "    ######################################################## \n",
    "    # prepare X (change this if you want to add SEC emb, etc.)\n",
    "    ########################################################\n",
    "    curr = stock_df[(stock_df.Date.dt.year == yr) & (stock_df.Date.dt.month == month)]\n",
    "    X = curr.pivot_table(\n",
    "            index=\"Date\", columns=\"ticker_symbol\", values=\"Close\"\n",
    "        ).values.T\n",
    "    X_tensor = torch.tensor(X)\n",
    "    \n",
    "    ########################################################\n",
    "    # prepare y (change this if you want to change labels)\n",
    "    ########################################################\n",
    "    if month == 12:\n",
    "        y_yr = yr+1\n",
    "        y_month = 1\n",
    "    else:\n",
    "        y_yr = yr\n",
    "        y_month = month + 1\n",
    "    \n",
    "    nxt = stock_df[(stock_df.Date.dt.year == y_yr) & (stock_df.Date.dt.month == y_month)]\n",
    "    y = nxt.pivot_table(\n",
    "            index=\"Date\", columns=\"ticker_symbol\", values=\"Close\"\n",
    "        ).values.T\n",
    "    y = (y.mean(1) - X.mean(1)) / X.mean(1)\n",
    "    y_tensor = torch.tensor(y)\n",
    "    X_y.append((X_tensor,y_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1065e501",
   "metadata": {},
   "source": [
    "# prepare edges index and attr (change this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1b728b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_lst = []\n",
    "for fp in sorted_fps:\n",
    "    full_fp = os.path.join(DATA_ARCHIVE_DIR,news_or_twitter,fp)\n",
    "    yr_month = fp.split(\".\")[0]\n",
    "    yr = int(yr_month.split(\"_\")[0])\n",
    "    month = int(yr_month.split(\"_\")[1])\n",
    "    # no further data available\n",
    "    if yr == 2021 and month == 10: \n",
    "        continue\n",
    "        \n",
    "        \n",
    "    edges = np.load(full_fp)\n",
    "    edge_index, edge_attr = dense_to_sparse(torch.from_numpy(edges))\n",
    "    ######################################################## \n",
    "    # modify edge index and edge attr (change this if you want to change graph structure)\n",
    "    ########################################################\n",
    "    edges_lst.append((edge_index,edge_attr))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85897c16",
   "metadata": {},
   "source": [
    "## create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "463fcdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for i in range(len(X_y)):\n",
    "    X = X_y[i][0]\n",
    "    y = X_y[i][1]\n",
    "    edge_index = edges_lst[i][0]\n",
    "    edge_attr = edges_lst[i][1]\n",
    "    data = Data(x=X, y=y, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    dataset.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f8a37a",
   "metadata": {},
   "source": [
    "## model (change this for different NN structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f595a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, feature_size, output_size):\n",
    "        super().__init__()\n",
    "        self.conv = GCNConv(in_channels=input_size, out_channels=feature_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc = nn.Linear(in_features=feature_size, out_features=output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, data):\n",
    "        node_attr = F.normalize(data.x.float(), dim=0)\n",
    "        num_pad = self.conv.in_channels - node_attr.shape[1]\n",
    "        node_attr = torch.cat(\n",
    "            (\n",
    "                node_attr,\n",
    "                torch.zeros((node_attr.shape[0], num_pad))\n",
    "            ),\n",
    "            -1\n",
    "        )\n",
    "        edge_index = data.edge_index.long()\n",
    "        edge_weight = F.normalize(data.edge_attr.float().reshape(-1, 1), dim=0)\n",
    "        x = self.conv(\n",
    "                    x=node_attr, \n",
    "                    edge_index=edge_index, \n",
    "                    edge_weight=edge_weight\n",
    "                    )\n",
    "        x = self.activation(x)\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52752c",
   "metadata": {},
   "source": [
    "# training starts below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83bfc55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "class Trainer:\n",
    "    def __init__(self, model, dataset, args):\n",
    "        self.device = args.device\n",
    "        self.model = model.to(self.device)\n",
    "        self.dataset = dataset\n",
    "        self.epochs = args.num_epochs\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = args.learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "        self.val_idx = int(len(self.dataset) * (1-args.val_size))\n",
    "        self.best_model_weights = self.model.state_dict()\n",
    "        self.best_epoch = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.threshold = args.threshold\n",
    "    def train(self):\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            train_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "            for i, data in enumerate(self.dataset):\n",
    "                if i < self.val_idx:\n",
    "                    loss = self._train_step(self.model, data)\n",
    "                    train_loss += loss / self.val_idx\n",
    "                else:\n",
    "                    loss = self._val_step(self.model, data)\n",
    "                    val_loss += loss / (len(self.dataset) - self.val_idx)\n",
    "            # if epoch % 20 == 0 or epoch == self.epochs:\n",
    "            #     print(f\"\"\"\n",
    "            #         epoch {epoch}:\n",
    "            #             train loss: {train_loss},\n",
    "            #             val loss: {val_loss}\n",
    "            #     \"\"\")\n",
    "            if self.best_val_loss > val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_epoch = epoch\n",
    "                self.best_model_weights = model.state_dict()\n",
    "        self.model.load_state_dict(self.best_model_weights)\n",
    "        print(\n",
    "            f\"\"\"\n",
    "            best model loss is:\n",
    "                val loss: {self.best_val_loss} @ epoch: {self.best_epoch}\n",
    "            \"\"\"\n",
    "        )\n",
    "        self._benchmark()\n",
    "        return self.model\n",
    "\n",
    "    def _train_step(self, model, data):\n",
    "        self.optimizer.zero_grad()\n",
    "        logits, target = self._shared_step(model, data)\n",
    "        loss = self.criterion(logits, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    def _val_step(self, model, data):\n",
    "        with torch.no_grad():\n",
    "            logits, target = self._shared_step(model, data)\n",
    "            loss = self.criterion(logits, target)\n",
    "            return loss.item()\n",
    "    def _shared_step(self, model, data):\n",
    "        data.x = data.x.to(self.device)\n",
    "        data.edge_index = data.edge_index.to(self.device)\n",
    "        data.edge_attr = data.edge_attr.to(self.device)\n",
    "        target = data.y.long().to(self.device)\n",
    "        logits = model(data)\n",
    "        return logits, target\n",
    "    def _benchmark(self):\n",
    "        train_preds = []\n",
    "        train_trues = []\n",
    "        val_preds = []\n",
    "        val_trues = []\n",
    "        for i, data in enumerate(self.dataset):\n",
    "            logits, target = self._shared_step(self.model, data)\n",
    "            pred = logits.argmax(-1).cpu().numpy()\n",
    "            target = target.cpu().numpy()\n",
    "            if i < self.val_idx:\n",
    "                train_preds.append(pred)\n",
    "                train_trues.append(target)\n",
    "            else:\n",
    "                val_preds.append(pred)\n",
    "                val_trues.append(target)\n",
    "        train_preds = np.hstack(train_preds)\n",
    "        train_trues = np.hstack(train_trues)\n",
    "        val_preds = np.hstack(val_preds)\n",
    "        val_trues = np.hstack(val_trues)\n",
    "        print(\n",
    "            f\"\"\"\n",
    "                best model performance is:\n",
    "                    train acc: {metrics.accuracy_score(train_trues, train_preds)}\n",
    "                    val acc: {metrics.accuracy_score(val_trues, val_preds)}\n",
    "\n",
    "                    train f1 score {metrics.f1_score(train_trues, train_preds)}\n",
    "                    val f1 score {metrics.f1_score(val_trues, val_preds)}\n",
    "\n",
    "                    train precision score {metrics.precision_score(train_trues, train_preds)}\n",
    "                    val precision score {metrics.precision_score(val_trues, val_preds)}\n",
    "\n",
    "                    train recall score {metrics.recall_score(train_trues, train_preds)}\n",
    "                    val recall score {metrics.recall_score(val_trues, val_preds)}\n",
    "\n",
    "                    num of pos prediction in training set {train_preds[train_preds == 1].shape[0]}\n",
    "                    num of neg prediction in training set {train_preds[train_preds == 0].shape[0]}\n",
    "                    num of pos prediction in val set {val_preds[val_preds == 1].shape[0]}\n",
    "                    num of neg prediction in val set {val_preds[val_preds == 0].shape[0]}\n",
    "            \"\"\"\n",
    "        )\n",
    "        print(\n",
    "            metrics.classification_report(val_trues, val_preds)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bcadaa",
   "metadata": {},
   "source": [
    "# hyperparams (change this e.g. change logit threshold for predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e87cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(70, 32, 2)\n",
    "args = dict(\n",
    "    num_epochs = 100,\n",
    "    learning_rate = 2e-5,\n",
    "    device = \"cpu\",\n",
    "    val_size = .2,\n",
    "    ###########################\n",
    "    # not implemented\n",
    "    ###########################\n",
    "    threshold = 0 \n",
    ")\n",
    "\n",
    "args = argparse.Namespace(**args)\n",
    "trainer = Trainer(model, dataset, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eefdf313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            best model loss is:\n",
      "                val loss: 0.48381937046845763 @ epoch: 100\n",
      "            \n",
      "\n",
      "                best model performance is:\n",
      "                    train acc: 1.0\n",
      "                    val acc: 1.0\n",
      "\n",
      "                    train f1 score 0.0\n",
      "                    val f1 score 0.0\n",
      "\n",
      "                    train precision score 0.0\n",
      "                    val precision score 0.0\n",
      "\n",
      "                    train recall score 0.0\n",
      "                    val recall score 0.0\n",
      "\n",
      "                    num of pos prediction in training set 0\n",
      "                    num of neg prediction in training set 1392\n",
      "                    num of pos prediction in val set 0\n",
      "                    num of neg prediction in val set 348\n",
      "            \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       348\n",
      "\n",
      "    accuracy                           1.00       348\n",
      "   macro avg       1.00      1.00      1.00       348\n",
      "weighted avg       1.00      1.00      1.00       348\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/capstone/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/opt/anaconda3/envs/capstone/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "/opt/anaconda3/envs/capstone/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/capstone/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/capstone/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/capstone/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be87e41a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
