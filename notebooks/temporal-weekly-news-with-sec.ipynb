{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use pytorch geometric temporal, make sure you have torch 1.9.0 installed (uninstall 1.10.0 before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cpu.html\n",
      "Requirement already satisfied: torch-scatter in c:\\users\\erdon\\anaconda3\\lib\\site-packages (2.0.9)\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cpu.html\n",
      "Requirement already satisfied: torch-sparse in c:\\users\\erdon\\anaconda3\\lib\\site-packages (0.6.12)\n",
      "Requirement already satisfied: scipy in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-sparse) (1.6.2)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from scipy->torch-sparse) (1.20.1)\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cpu.html\n",
      "Requirement already satisfied: torch-cluster in c:\\users\\erdon\\anaconda3\\lib\\site-packages (1.5.9)\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cpu.html\n",
      "Requirement already satisfied: torch-spline-conv in c:\\users\\erdon\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: torch-geometric in c:\\users\\erdon\\anaconda3\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric) (2.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric) (1.6.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric) (2.11.3)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric) (2.4.7)\n",
      "Requirement already satisfied: yacs in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric) (0.1.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric) (4.59.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric) (0.24.1)\n",
      "Requirement already satisfied: rdflib in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric) (6.0.2)\n",
      "Requirement already satisfied: googledrivedownloader in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric) (0.4)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric) (5.4.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric) (1.2.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric) (1.20.1)\n",
      "Requirement already satisfied: requests in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric) (2.25.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from jinja2->torch-geometric) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from networkx->torch-geometric) (4.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from pandas->torch-geometric) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from pandas->torch-geometric) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from rdflib->torch-geometric) (52.0.0.post20210125)\n",
      "Requirement already satisfied: isodate in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from rdflib->torch-geometric) (0.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from requests->torch-geometric) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from requests->torch-geometric) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from requests->torch-geometric) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from requests->torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from scikit-learn->torch-geometric) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from scikit-learn->torch-geometric) (2.1.0)\n",
      "Requirement already satisfied: torch-geometric-temporal in c:\\users\\erdon\\anaconda3\\lib\\site-packages (0.41)\n",
      "Requirement already satisfied: decorator==4.4.2 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric-temporal) (4.4.2)\n",
      "Requirement already satisfied: torch in c:\\users\\erdon\\appdata\\roaming\\python\\python38\\site-packages (from torch-geometric-temporal) (1.9.0)\n",
      "Requirement already satisfied: torch-spline-conv in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric-temporal) (1.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric-temporal) (1.20.1)\n",
      "Requirement already satisfied: torch-scatter in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric-temporal) (2.0.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric-temporal) (4.59.0)\n",
      "Requirement already satisfied: torch-geometric in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric-temporal) (2.0.2)\n",
      "Requirement already satisfied: torch-cluster in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric-temporal) (1.5.9)\n",
      "Requirement already satisfied: torch-sparse in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric-temporal) (0.6.12)\n",
      "Requirement already satisfied: six in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric-temporal) (1.15.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric-temporal) (1.6.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch->torch-geometric-temporal) (3.7.4.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric->torch-geometric-temporal) (2.11.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric->torch-geometric-temporal) (1.2.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric->torch-geometric-temporal) (0.24.1)\n",
      "Requirement already satisfied: rdflib in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric->torch-geometric-temporal) (6.0.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric->torch-geometric-temporal) (2.5)\n",
      "Requirement already satisfied: googledrivedownloader in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric->torch-geometric-temporal) (0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric->torch-geometric-temporal) (2.25.1)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric->torch-geometric-temporal) (2.4.7)\n",
      "Requirement already satisfied: yacs in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric->torch-geometric-temporal) (0.1.8)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from torch-geometric->torch-geometric-temporal) (5.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from jinja2->torch-geometric->torch-geometric-temporal) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from pandas->torch-geometric->torch-geometric-temporal) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from pandas->torch-geometric->torch-geometric-temporal) (2021.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from rdflib->torch-geometric->torch-geometric-temporal) (52.0.0.post20210125)\n",
      "Requirement already satisfied: isodate in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from rdflib->torch-geometric->torch-geometric-temporal) (0.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from requests->torch-geometric->torch-geometric-temporal) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from requests->torch-geometric->torch-geometric-temporal) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from requests->torch-geometric->torch-geometric-temporal) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from requests->torch-geometric->torch-geometric-temporal) (2020.12.5)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from scikit-learn->torch-geometric->torch-geometric-temporal) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\erdon\\anaconda3\\lib\\site-packages (from scikit-learn->torch-geometric->torch-geometric-temporal) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cpu.html\n",
    "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cpu.html\n",
    "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.9.0+cpu.html\n",
    "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.9.0+cpu.html\n",
    "!pip install torch-geometric\n",
    "!pip install torch-geometric-temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def transform_and_split(data):\n",
    "    # Normalize node features and transform data type\n",
    "    data.x = normalize(data.x, axis=1, norm='max')\n",
    "    data.x = torch.from_numpy(data.x).to(torch.float64)\n",
    "    data.y = data.y.apply_(lambda x:  1 if (x > 0) else 0) # Change y into {0, 1} for binary classification\n",
    "    data.y = data.y.to(torch.float64)    \n",
    "    data.edge_attr = data.edge_attr.to(torch.double)\n",
    "\n",
    "\n",
    "    # Split into train/test set\n",
    "#    split = nodeSplit(split=\"train_rest\", num_splits = 1, num_val = 0.0, num_test= 0.2)\n",
    "#    masked_data = split(data)\n",
    "\n",
    "#    print(\"Training samples:\", torch.sum(masked_data.train_mask).item())\n",
    "#    print(\"Validation samples:\", torch.sum(masked_data.val_mask ).item())\n",
    "#    print(\"Test samples:\", torch.sum(masked_data.test_mask ).item())\n",
    "    print_basic_info(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_basic_info(data):\n",
    "    print()\n",
    "    print(data)\n",
    "    print('===========================================================================================================')\n",
    "\n",
    "    print(f'Number of nodes: {data.num_nodes}')\n",
    "    print(f'Number of edges: {data.num_edges}')\n",
    "    print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "    print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "    print(f'Has self-loops: {data.has_self_loops()}')\n",
    "    print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data(x=[29, 61], edge_index=[2, 400], edge_attr=[400], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 400\n",
      "Average node degree: 13.79\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/processed/twitter/2018_q1.pt\" # Customize...\n",
    "dataset = torch.load(path)\n",
    "data = dataset[0]\n",
    "transformed_data = transform_and_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/news/news_data_weekly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>url</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/04/opinion/the...</td>\n",
       "      <td>Donald Trump is a thug. He’s a thug who talks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/08/us/politics...</td>\n",
       "      <td>In lucrative paid speeches that Hillary Clint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/07/technology/...</td>\n",
       "      <td>SAN FRANCISCO — Marc Benioff, the founder and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/05/business/pr...</td>\n",
       "      <td>Prepaid debit cards are a financial lifeline ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/04/world/ameri...</td>\n",
       "      <td>RIO DE JANEIRO — It was not a banner day for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>2021-10-02</td>\n",
       "      <td>https://www.nytimes.com/2021/09/30/sports/socc...</td>\n",
       "      <td>Looking to expand its global footprint beyond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>2021-10-02</td>\n",
       "      <td>https://www.nytimes.com/2021/10/01/business/cr...</td>\n",
       "      <td>Despite the popularity of mobile apps promisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>2021-10-02</td>\n",
       "      <td>https://www.nytimes.com/2021/10/02/your-money/...</td>\n",
       "      <td>Introducing your child to the real-world use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>2021-10-02</td>\n",
       "      <td>https://www.nytimes.com/2021/09/26/fashion/wat...</td>\n",
       "      <td>Like their counterparts in industries such as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2744</th>\n",
       "      <td>2021-10-02</td>\n",
       "      <td>https://www.nytimes.com/2021/09/27/business/ca...</td>\n",
       "      <td>LOS ANGELES — Creative Artists Agency announc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3636 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date                                                url  \\\n",
       "2746  2016-10-08  https://www.nytimes.com/2016/10/04/opinion/the...   \n",
       "33    2016-10-08  https://www.nytimes.com/2016/10/08/us/politics...   \n",
       "32    2016-10-08  https://www.nytimes.com/2016/10/07/technology/...   \n",
       "31    2016-10-08  https://www.nytimes.com/2016/10/05/business/pr...   \n",
       "30    2016-10-08  https://www.nytimes.com/2016/10/04/world/ameri...   \n",
       "...          ...                                                ...   \n",
       "1518  2021-10-02  https://www.nytimes.com/2021/09/30/sports/socc...   \n",
       "1517  2021-10-02  https://www.nytimes.com/2021/10/01/business/cr...   \n",
       "1516  2021-10-02  https://www.nytimes.com/2021/10/02/your-money/...   \n",
       "2736  2021-10-02  https://www.nytimes.com/2021/09/26/fashion/wat...   \n",
       "2744  2021-10-02  https://www.nytimes.com/2021/09/27/business/ca...   \n",
       "\n",
       "                                                  texts  \n",
       "2746   Donald Trump is a thug. He’s a thug who talks...  \n",
       "33     In lucrative paid speeches that Hillary Clint...  \n",
       "32     SAN FRANCISCO — Marc Benioff, the founder and...  \n",
       "31     Prepaid debit cards are a financial lifeline ...  \n",
       "30     RIO DE JANEIRO — It was not a banner day for ...  \n",
       "...                                                 ...  \n",
       "1518   Looking to expand its global footprint beyond...  \n",
       "1517   Despite the popularity of mobile apps promisi...  \n",
       "1516   Introducing your child to the real-world use ...  \n",
       "2736   Like their counterparts in industries such as...  \n",
       "2744   LOS ANGELES — Creative Artists Agency announc...  \n",
       "\n",
       "[3636 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [part for _, part in df.groupby(pd.Grouper(key='Date', freq='W-MON'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>url</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/04/opinion/the...</td>\n",
       "      <td>Donald Trump is a thug. He’s a thug who talks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/08/us/politics...</td>\n",
       "      <td>In lucrative paid speeches that Hillary Clint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/07/technology/...</td>\n",
       "      <td>SAN FRANCISCO — Marc Benioff, the founder and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/05/business/pr...</td>\n",
       "      <td>Prepaid debit cards are a financial lifeline ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/04/world/ameri...</td>\n",
       "      <td>RIO DE JANEIRO — It was not a banner day for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/07/business/de...</td>\n",
       "      <td>WASHINGTON — Nearly five years after Jon S. C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/08/business/in...</td>\n",
       "      <td>LONDON — As Europe has grappled with the trau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/08/world/europ...</td>\n",
       "      <td>LONDON — For those blithely inclined toward t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/04/business/de...</td>\n",
       "      <td>The Janus Capital Group and the Henderson Gro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/09/world/middl...</td>\n",
       "      <td>TEHRAN — Rushing for a plane to Tehran becaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/06/us/politics...</td>\n",
       "      <td>Following is a transcript of the vice-preside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>2016-10-08</td>\n",
       "      <td>https://www.nytimes.com/2016/10/06/t-magazine/...</td>\n",
       "      <td>IN THE RANK OF UNFLATTERING monikers for an a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date                                                url  \\\n",
       "2746 2016-10-08  https://www.nytimes.com/2016/10/04/opinion/the...   \n",
       "33   2016-10-08  https://www.nytimes.com/2016/10/08/us/politics...   \n",
       "32   2016-10-08  https://www.nytimes.com/2016/10/07/technology/...   \n",
       "31   2016-10-08  https://www.nytimes.com/2016/10/05/business/pr...   \n",
       "30   2016-10-08  https://www.nytimes.com/2016/10/04/world/ameri...   \n",
       "36   2016-10-08  https://www.nytimes.com/2016/10/07/business/de...   \n",
       "37   2016-10-08  https://www.nytimes.com/2016/10/08/business/in...   \n",
       "38   2016-10-08  https://www.nytimes.com/2016/10/08/world/europ...   \n",
       "34   2016-10-08  https://www.nytimes.com/2016/10/04/business/de...   \n",
       "2747 2016-10-08  https://www.nytimes.com/2016/10/09/world/middl...   \n",
       "35   2016-10-08  https://www.nytimes.com/2016/10/06/us/politics...   \n",
       "3252 2016-10-08  https://www.nytimes.com/2016/10/06/t-magazine/...   \n",
       "\n",
       "                                                  texts  \n",
       "2746   Donald Trump is a thug. He’s a thug who talks...  \n",
       "33     In lucrative paid speeches that Hillary Clint...  \n",
       "32     SAN FRANCISCO — Marc Benioff, the founder and...  \n",
       "31     Prepaid debit cards are a financial lifeline ...  \n",
       "30     RIO DE JANEIRO — It was not a banner day for ...  \n",
       "36     WASHINGTON — Nearly five years after Jon S. C...  \n",
       "37     LONDON — As Europe has grappled with the trau...  \n",
       "38     LONDON — For those blithely inclined toward t...  \n",
       "34     The Janus Capital Group and the Henderson Gro...  \n",
       "2747   TEHRAN — Rushing for a plane to Tehran becaus...  \n",
       "35     Following is a transcript of the vice-preside...  \n",
       "3252   IN THE RANK OF UNFLATTERING monikers for an a...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'companies': [{'wba': {'alias': ['$wba', 'wba', 'walgreen boots alliance inc', 'walgreen boots alliance', 'walgreenbootsalliance']}}, {'v': {'alias': ['$v', 'v', 'visa inc class a', 'visa']}}, {'crm': {'alias': ['$crm', 'crm', 'salesforce.com inc', 'salesforce']}}, {'cvx': {'alias': ['$cvx', 'cvx', 'chevron corp', 'chevron']}}, {'pg': {'alias': ['$pg', 'pg', 'procter & gamble', 'procter&gamble']}}, {'vz': {'alias': ['$vz', 'vz', 'verizon communications inc', 'verizon']}}, {'wmt': {'alias': ['$wmt', 'wmt', 'walmart stores inc', 'walmart stores', 'walmart']}}, {'unh': {'alias': ['$unh', 'unh', 'unitedhealth group inc', 'unitedhealth group', 'unitedhealthgroup']}}, {'trv': {'alias': ['$trv', 'trv', 'travelers companies inc', 'travelers companies', 'travelers', 'travelerscompanies']}}, {'mcd': {'alias': ['$mcd', 'mcd', 'mcdonalds corp', 'mcdonalds']}}, {'mmm': {'alias': ['$mmm', 'mmm', '3m', '3m']}}, {'nke': {'alias': ['$nke', 'nke', 'nike inc class b', 'nike']}}, {'mrk': {'alias': ['$mrk', 'mrk', 'merck & co inc', 'merck & co', 'merck&co']}}, {'msft': {'alias': ['$msft', 'msft', 'microsoft corp', 'microsoft']}}, {'jpm': {'alias': ['$jpm', 'jpm', 'jp morgan chase & co', 'jp morgan chase', 'jpmorgan', ' j.p. morgan', ' jp morgan', 'jpm']}}, {'ko': {'alias': ['$ko', 'ko', 'coca-cola', 'cocacola']}}, {'jnj': {'alias': ['$jnj', 'jnj', 'johnson & johnson', 'johnson&johnson']}}, {'gs': {'alias': ['$gs', 'gs', 'goldman sachs group inc', 'goldman sachs group', 'goldman sachs', 'goldman']}}, {'hd': {'alias': ['$hd', 'hd', 'home depot inc', 'home depot']}}, {'hon': {'alias': ['$hon', 'hon', 'honeywell international inc', 'honeywell international', 'honeywell']}}, {'ibm': {'alias': ['$ibm', 'ibm', 'international business machines co', 'international business machines', 'internationalbusinessmachines']}}, {'intc': {'alias': ['$intc', 'intc', 'intel corporation corp', 'intel corporation', 'intel']}}, {'dis': {'alias': ['$dis', 'dis', 'walt disney', 'disney', 'waltdisney']}}, {'cat': {'alias': ['$cat', 'cat', 'caterpillar inc', 'caterpillar inc', 'caterpillarinc']}}, {'csco': {'alias': ['$csco', 'csco', 'cisco systems inc', 'cisco systems', 'cisco']}}, {'axp': {'alias': ['$axp', 'axp', 'american express', 'american express', 'amex', 'americanexpress']}}, {'ba': {'alias': ['$ba', 'ba', 'boeing', 'boeing']}}, {'amgn': {'alias': ['$amgn', 'amgn', 'amgen inc', 'amgen']}}, {'aapl': {'alias': ['$aapl', 'aapl', 'apple inc', 'apple inc', 'appleinc']}}], 'data_root': './data', 'start_date': datetime.date(2016, 10, 1), 'end_date': datetime.date(2021, 10, 1), 'data_pipeline_config': {'components': ['sec'], 'redownload': False}, 'twitter_conifg': {'sleep_time': 5}, 'etf_config': {'external_path': './data/external/etf'}, 'sec_config': {'parser_config': {'directory': './data/raw/sec', 'amount': 5, 'items': ['1A', '1B', '7A', '8'], 'external_fp': './data/external/sec'}, 'emb_config': {'top_k': 20, 'model_name': 'all-mpnet-base-v2'}}, 'news_config': {'sleep_time': 2, 'nyt_key': 'MN3m4QX4XDbGsaBFDBpJG4waHymmqZ3O', 'base_url': 'https://api.nytimes.com/svc/search/v2/articlesearch.json?', 'search_keyword': ['3M', 'American+Express', 'Amgen', 'Apple', 'Boeing', 'Caterpillar', 'Chevron', 'Cisco', 'Coca+Cola', 'Dow', 'Goldman+Sachs', 'Home+Depot', 'Honeywell', 'IBM', 'Intel', 'Johnson+Johnson', 'JPMorgan', 'McDonald', 'Merck', 'Microsoft', 'Nike', 'Procter+Gamble', 'Salesforce', 'Travelers', 'UnitedHealth', 'Verizon', 'Visa', 'Walgreens', 'Walmart', 'Disney']}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open('../configs/dow_jones.yaml') as f:\n",
    "    \n",
    "    data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix(df):\n",
    "    companies = [list(com.keys())[0] for com in data['companies']]\n",
    "    alias = list(map(lambda x: list(x.items())[0][1][\"alias\"], data['companies']))\n",
    "    res = pd.DataFrame(0, index=companies, columns=companies)\n",
    "    for company1, search_items1 in zip(companies, alias):\n",
    "        for company2, search_items2 in zip(companies, alias):\n",
    "            if company1 != company2:\n",
    "                search_items = search_items1 + search_items2\n",
    "            else:\n",
    "                search_items = search_items1\n",
    "            pat = \"|\".join(search_items)\n",
    "            res[company1][company2] += df.texts.str.contains(\n",
    "                pat\n",
    "            ).sum()\n",
    "    return res.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_list = [get_matrix(df) for df in df_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "week = 0\n",
    "for i in range(261):\n",
    "    np.save('../data/raw/news/week_'+str(week)+'.npy', mat_list[i])\n",
    "    week += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "stock_df = pd.read_csv(\n",
    "            os.path.join('../data/raw',\"stock\",\"raw.csv\"),\n",
    "            usecols=[\"ticker_symbol\", \"Date\", \"Close\"],\n",
    "            parse_dates=[\"Date\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>ticker_symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-10-03</td>\n",
       "      <td>28.129999</td>\n",
       "      <td>aapl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-10-04</td>\n",
       "      <td>28.250000</td>\n",
       "      <td>aapl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-10-05</td>\n",
       "      <td>28.262501</td>\n",
       "      <td>aapl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-10-06</td>\n",
       "      <td>28.472500</td>\n",
       "      <td>aapl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-10-07</td>\n",
       "      <td>28.514999</td>\n",
       "      <td>aapl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36506</th>\n",
       "      <td>2021-09-27</td>\n",
       "      <td>142.250000</td>\n",
       "      <td>wmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36507</th>\n",
       "      <td>2021-09-28</td>\n",
       "      <td>140.500000</td>\n",
       "      <td>wmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36508</th>\n",
       "      <td>2021-09-29</td>\n",
       "      <td>140.440002</td>\n",
       "      <td>wmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36509</th>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>139.380005</td>\n",
       "      <td>wmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36510</th>\n",
       "      <td>2021-10-01</td>\n",
       "      <td>137.050003</td>\n",
       "      <td>wmt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36511 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Close ticker_symbol\n",
       "0     2016-10-03   28.129999          aapl\n",
       "1     2016-10-04   28.250000          aapl\n",
       "2     2016-10-05   28.262501          aapl\n",
       "3     2016-10-06   28.472500          aapl\n",
       "4     2016-10-07   28.514999          aapl\n",
       "...          ...         ...           ...\n",
       "36506 2021-09-27  142.250000           wmt\n",
       "36507 2021-09-28  140.500000           wmt\n",
       "36508 2021-09-29  140.440002           wmt\n",
       "36509 2021-09-30  139.380005           wmt\n",
       "36510 2021-10-01  137.050003           wmt\n",
       "\n",
       "[36511 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "start = datetime.strptime('2016-10-02', \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = [start]\n",
    "cur = start\n",
    "for i in range(261):\n",
    "    cur = cur + timedelta(days=7)\n",
    "    date_list.append(cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2016, 10, 2, 0, 0),\n",
       " datetime.datetime(2016, 10, 9, 0, 0),\n",
       " datetime.datetime(2016, 10, 16, 0, 0),\n",
       " datetime.datetime(2016, 10, 23, 0, 0),\n",
       " datetime.datetime(2016, 10, 30, 0, 0),\n",
       " datetime.datetime(2016, 11, 6, 0, 0),\n",
       " datetime.datetime(2016, 11, 13, 0, 0),\n",
       " datetime.datetime(2016, 11, 20, 0, 0),\n",
       " datetime.datetime(2016, 11, 27, 0, 0),\n",
       " datetime.datetime(2016, 12, 4, 0, 0),\n",
       " datetime.datetime(2016, 12, 11, 0, 0),\n",
       " datetime.datetime(2016, 12, 18, 0, 0),\n",
       " datetime.datetime(2016, 12, 25, 0, 0),\n",
       " datetime.datetime(2017, 1, 1, 0, 0),\n",
       " datetime.datetime(2017, 1, 8, 0, 0),\n",
       " datetime.datetime(2017, 1, 15, 0, 0),\n",
       " datetime.datetime(2017, 1, 22, 0, 0),\n",
       " datetime.datetime(2017, 1, 29, 0, 0),\n",
       " datetime.datetime(2017, 2, 5, 0, 0),\n",
       " datetime.datetime(2017, 2, 12, 0, 0),\n",
       " datetime.datetime(2017, 2, 19, 0, 0),\n",
       " datetime.datetime(2017, 2, 26, 0, 0),\n",
       " datetime.datetime(2017, 3, 5, 0, 0),\n",
       " datetime.datetime(2017, 3, 12, 0, 0),\n",
       " datetime.datetime(2017, 3, 19, 0, 0),\n",
       " datetime.datetime(2017, 3, 26, 0, 0),\n",
       " datetime.datetime(2017, 4, 2, 0, 0),\n",
       " datetime.datetime(2017, 4, 9, 0, 0),\n",
       " datetime.datetime(2017, 4, 16, 0, 0),\n",
       " datetime.datetime(2017, 4, 23, 0, 0),\n",
       " datetime.datetime(2017, 4, 30, 0, 0),\n",
       " datetime.datetime(2017, 5, 7, 0, 0),\n",
       " datetime.datetime(2017, 5, 14, 0, 0),\n",
       " datetime.datetime(2017, 5, 21, 0, 0),\n",
       " datetime.datetime(2017, 5, 28, 0, 0),\n",
       " datetime.datetime(2017, 6, 4, 0, 0),\n",
       " datetime.datetime(2017, 6, 11, 0, 0),\n",
       " datetime.datetime(2017, 6, 18, 0, 0),\n",
       " datetime.datetime(2017, 6, 25, 0, 0),\n",
       " datetime.datetime(2017, 7, 2, 0, 0),\n",
       " datetime.datetime(2017, 7, 9, 0, 0),\n",
       " datetime.datetime(2017, 7, 16, 0, 0),\n",
       " datetime.datetime(2017, 7, 23, 0, 0),\n",
       " datetime.datetime(2017, 7, 30, 0, 0),\n",
       " datetime.datetime(2017, 8, 6, 0, 0),\n",
       " datetime.datetime(2017, 8, 13, 0, 0),\n",
       " datetime.datetime(2017, 8, 20, 0, 0),\n",
       " datetime.datetime(2017, 8, 27, 0, 0),\n",
       " datetime.datetime(2017, 9, 3, 0, 0),\n",
       " datetime.datetime(2017, 9, 10, 0, 0),\n",
       " datetime.datetime(2017, 9, 17, 0, 0),\n",
       " datetime.datetime(2017, 9, 24, 0, 0),\n",
       " datetime.datetime(2017, 10, 1, 0, 0),\n",
       " datetime.datetime(2017, 10, 8, 0, 0),\n",
       " datetime.datetime(2017, 10, 15, 0, 0),\n",
       " datetime.datetime(2017, 10, 22, 0, 0),\n",
       " datetime.datetime(2017, 10, 29, 0, 0),\n",
       " datetime.datetime(2017, 11, 5, 0, 0),\n",
       " datetime.datetime(2017, 11, 12, 0, 0),\n",
       " datetime.datetime(2017, 11, 19, 0, 0),\n",
       " datetime.datetime(2017, 11, 26, 0, 0),\n",
       " datetime.datetime(2017, 12, 3, 0, 0),\n",
       " datetime.datetime(2017, 12, 10, 0, 0),\n",
       " datetime.datetime(2017, 12, 17, 0, 0),\n",
       " datetime.datetime(2017, 12, 24, 0, 0),\n",
       " datetime.datetime(2017, 12, 31, 0, 0),\n",
       " datetime.datetime(2018, 1, 7, 0, 0),\n",
       " datetime.datetime(2018, 1, 14, 0, 0),\n",
       " datetime.datetime(2018, 1, 21, 0, 0),\n",
       " datetime.datetime(2018, 1, 28, 0, 0),\n",
       " datetime.datetime(2018, 2, 4, 0, 0),\n",
       " datetime.datetime(2018, 2, 11, 0, 0),\n",
       " datetime.datetime(2018, 2, 18, 0, 0),\n",
       " datetime.datetime(2018, 2, 25, 0, 0),\n",
       " datetime.datetime(2018, 3, 4, 0, 0),\n",
       " datetime.datetime(2018, 3, 11, 0, 0),\n",
       " datetime.datetime(2018, 3, 18, 0, 0),\n",
       " datetime.datetime(2018, 3, 25, 0, 0),\n",
       " datetime.datetime(2018, 4, 1, 0, 0),\n",
       " datetime.datetime(2018, 4, 8, 0, 0),\n",
       " datetime.datetime(2018, 4, 15, 0, 0),\n",
       " datetime.datetime(2018, 4, 22, 0, 0),\n",
       " datetime.datetime(2018, 4, 29, 0, 0),\n",
       " datetime.datetime(2018, 5, 6, 0, 0),\n",
       " datetime.datetime(2018, 5, 13, 0, 0),\n",
       " datetime.datetime(2018, 5, 20, 0, 0),\n",
       " datetime.datetime(2018, 5, 27, 0, 0),\n",
       " datetime.datetime(2018, 6, 3, 0, 0),\n",
       " datetime.datetime(2018, 6, 10, 0, 0),\n",
       " datetime.datetime(2018, 6, 17, 0, 0),\n",
       " datetime.datetime(2018, 6, 24, 0, 0),\n",
       " datetime.datetime(2018, 7, 1, 0, 0),\n",
       " datetime.datetime(2018, 7, 8, 0, 0),\n",
       " datetime.datetime(2018, 7, 15, 0, 0),\n",
       " datetime.datetime(2018, 7, 22, 0, 0),\n",
       " datetime.datetime(2018, 7, 29, 0, 0),\n",
       " datetime.datetime(2018, 8, 5, 0, 0),\n",
       " datetime.datetime(2018, 8, 12, 0, 0),\n",
       " datetime.datetime(2018, 8, 19, 0, 0),\n",
       " datetime.datetime(2018, 8, 26, 0, 0),\n",
       " datetime.datetime(2018, 9, 2, 0, 0),\n",
       " datetime.datetime(2018, 9, 9, 0, 0),\n",
       " datetime.datetime(2018, 9, 16, 0, 0),\n",
       " datetime.datetime(2018, 9, 23, 0, 0),\n",
       " datetime.datetime(2018, 9, 30, 0, 0),\n",
       " datetime.datetime(2018, 10, 7, 0, 0),\n",
       " datetime.datetime(2018, 10, 14, 0, 0),\n",
       " datetime.datetime(2018, 10, 21, 0, 0),\n",
       " datetime.datetime(2018, 10, 28, 0, 0),\n",
       " datetime.datetime(2018, 11, 4, 0, 0),\n",
       " datetime.datetime(2018, 11, 11, 0, 0),\n",
       " datetime.datetime(2018, 11, 18, 0, 0),\n",
       " datetime.datetime(2018, 11, 25, 0, 0),\n",
       " datetime.datetime(2018, 12, 2, 0, 0),\n",
       " datetime.datetime(2018, 12, 9, 0, 0),\n",
       " datetime.datetime(2018, 12, 16, 0, 0),\n",
       " datetime.datetime(2018, 12, 23, 0, 0),\n",
       " datetime.datetime(2018, 12, 30, 0, 0),\n",
       " datetime.datetime(2019, 1, 6, 0, 0),\n",
       " datetime.datetime(2019, 1, 13, 0, 0),\n",
       " datetime.datetime(2019, 1, 20, 0, 0),\n",
       " datetime.datetime(2019, 1, 27, 0, 0),\n",
       " datetime.datetime(2019, 2, 3, 0, 0),\n",
       " datetime.datetime(2019, 2, 10, 0, 0),\n",
       " datetime.datetime(2019, 2, 17, 0, 0),\n",
       " datetime.datetime(2019, 2, 24, 0, 0),\n",
       " datetime.datetime(2019, 3, 3, 0, 0),\n",
       " datetime.datetime(2019, 3, 10, 0, 0),\n",
       " datetime.datetime(2019, 3, 17, 0, 0),\n",
       " datetime.datetime(2019, 3, 24, 0, 0),\n",
       " datetime.datetime(2019, 3, 31, 0, 0),\n",
       " datetime.datetime(2019, 4, 7, 0, 0),\n",
       " datetime.datetime(2019, 4, 14, 0, 0),\n",
       " datetime.datetime(2019, 4, 21, 0, 0),\n",
       " datetime.datetime(2019, 4, 28, 0, 0),\n",
       " datetime.datetime(2019, 5, 5, 0, 0),\n",
       " datetime.datetime(2019, 5, 12, 0, 0),\n",
       " datetime.datetime(2019, 5, 19, 0, 0),\n",
       " datetime.datetime(2019, 5, 26, 0, 0),\n",
       " datetime.datetime(2019, 6, 2, 0, 0),\n",
       " datetime.datetime(2019, 6, 9, 0, 0),\n",
       " datetime.datetime(2019, 6, 16, 0, 0),\n",
       " datetime.datetime(2019, 6, 23, 0, 0),\n",
       " datetime.datetime(2019, 6, 30, 0, 0),\n",
       " datetime.datetime(2019, 7, 7, 0, 0),\n",
       " datetime.datetime(2019, 7, 14, 0, 0),\n",
       " datetime.datetime(2019, 7, 21, 0, 0),\n",
       " datetime.datetime(2019, 7, 28, 0, 0),\n",
       " datetime.datetime(2019, 8, 4, 0, 0),\n",
       " datetime.datetime(2019, 8, 11, 0, 0),\n",
       " datetime.datetime(2019, 8, 18, 0, 0),\n",
       " datetime.datetime(2019, 8, 25, 0, 0),\n",
       " datetime.datetime(2019, 9, 1, 0, 0),\n",
       " datetime.datetime(2019, 9, 8, 0, 0),\n",
       " datetime.datetime(2019, 9, 15, 0, 0),\n",
       " datetime.datetime(2019, 9, 22, 0, 0),\n",
       " datetime.datetime(2019, 9, 29, 0, 0),\n",
       " datetime.datetime(2019, 10, 6, 0, 0),\n",
       " datetime.datetime(2019, 10, 13, 0, 0),\n",
       " datetime.datetime(2019, 10, 20, 0, 0),\n",
       " datetime.datetime(2019, 10, 27, 0, 0),\n",
       " datetime.datetime(2019, 11, 3, 0, 0),\n",
       " datetime.datetime(2019, 11, 10, 0, 0),\n",
       " datetime.datetime(2019, 11, 17, 0, 0),\n",
       " datetime.datetime(2019, 11, 24, 0, 0),\n",
       " datetime.datetime(2019, 12, 1, 0, 0),\n",
       " datetime.datetime(2019, 12, 8, 0, 0),\n",
       " datetime.datetime(2019, 12, 15, 0, 0),\n",
       " datetime.datetime(2019, 12, 22, 0, 0),\n",
       " datetime.datetime(2019, 12, 29, 0, 0),\n",
       " datetime.datetime(2020, 1, 5, 0, 0),\n",
       " datetime.datetime(2020, 1, 12, 0, 0),\n",
       " datetime.datetime(2020, 1, 19, 0, 0),\n",
       " datetime.datetime(2020, 1, 26, 0, 0),\n",
       " datetime.datetime(2020, 2, 2, 0, 0),\n",
       " datetime.datetime(2020, 2, 9, 0, 0),\n",
       " datetime.datetime(2020, 2, 16, 0, 0),\n",
       " datetime.datetime(2020, 2, 23, 0, 0),\n",
       " datetime.datetime(2020, 3, 1, 0, 0),\n",
       " datetime.datetime(2020, 3, 8, 0, 0),\n",
       " datetime.datetime(2020, 3, 15, 0, 0),\n",
       " datetime.datetime(2020, 3, 22, 0, 0),\n",
       " datetime.datetime(2020, 3, 29, 0, 0),\n",
       " datetime.datetime(2020, 4, 5, 0, 0),\n",
       " datetime.datetime(2020, 4, 12, 0, 0),\n",
       " datetime.datetime(2020, 4, 19, 0, 0),\n",
       " datetime.datetime(2020, 4, 26, 0, 0),\n",
       " datetime.datetime(2020, 5, 3, 0, 0),\n",
       " datetime.datetime(2020, 5, 10, 0, 0),\n",
       " datetime.datetime(2020, 5, 17, 0, 0),\n",
       " datetime.datetime(2020, 5, 24, 0, 0),\n",
       " datetime.datetime(2020, 5, 31, 0, 0),\n",
       " datetime.datetime(2020, 6, 7, 0, 0),\n",
       " datetime.datetime(2020, 6, 14, 0, 0),\n",
       " datetime.datetime(2020, 6, 21, 0, 0),\n",
       " datetime.datetime(2020, 6, 28, 0, 0),\n",
       " datetime.datetime(2020, 7, 5, 0, 0),\n",
       " datetime.datetime(2020, 7, 12, 0, 0),\n",
       " datetime.datetime(2020, 7, 19, 0, 0),\n",
       " datetime.datetime(2020, 7, 26, 0, 0),\n",
       " datetime.datetime(2020, 8, 2, 0, 0),\n",
       " datetime.datetime(2020, 8, 9, 0, 0),\n",
       " datetime.datetime(2020, 8, 16, 0, 0),\n",
       " datetime.datetime(2020, 8, 23, 0, 0),\n",
       " datetime.datetime(2020, 8, 30, 0, 0),\n",
       " datetime.datetime(2020, 9, 6, 0, 0),\n",
       " datetime.datetime(2020, 9, 13, 0, 0),\n",
       " datetime.datetime(2020, 9, 20, 0, 0),\n",
       " datetime.datetime(2020, 9, 27, 0, 0),\n",
       " datetime.datetime(2020, 10, 4, 0, 0),\n",
       " datetime.datetime(2020, 10, 11, 0, 0),\n",
       " datetime.datetime(2020, 10, 18, 0, 0),\n",
       " datetime.datetime(2020, 10, 25, 0, 0),\n",
       " datetime.datetime(2020, 11, 1, 0, 0),\n",
       " datetime.datetime(2020, 11, 8, 0, 0),\n",
       " datetime.datetime(2020, 11, 15, 0, 0),\n",
       " datetime.datetime(2020, 11, 22, 0, 0),\n",
       " datetime.datetime(2020, 11, 29, 0, 0),\n",
       " datetime.datetime(2020, 12, 6, 0, 0),\n",
       " datetime.datetime(2020, 12, 13, 0, 0),\n",
       " datetime.datetime(2020, 12, 20, 0, 0),\n",
       " datetime.datetime(2020, 12, 27, 0, 0),\n",
       " datetime.datetime(2021, 1, 3, 0, 0),\n",
       " datetime.datetime(2021, 1, 10, 0, 0),\n",
       " datetime.datetime(2021, 1, 17, 0, 0),\n",
       " datetime.datetime(2021, 1, 24, 0, 0),\n",
       " datetime.datetime(2021, 1, 31, 0, 0),\n",
       " datetime.datetime(2021, 2, 7, 0, 0),\n",
       " datetime.datetime(2021, 2, 14, 0, 0),\n",
       " datetime.datetime(2021, 2, 21, 0, 0),\n",
       " datetime.datetime(2021, 2, 28, 0, 0),\n",
       " datetime.datetime(2021, 3, 7, 0, 0),\n",
       " datetime.datetime(2021, 3, 14, 0, 0),\n",
       " datetime.datetime(2021, 3, 21, 0, 0),\n",
       " datetime.datetime(2021, 3, 28, 0, 0),\n",
       " datetime.datetime(2021, 4, 4, 0, 0),\n",
       " datetime.datetime(2021, 4, 11, 0, 0),\n",
       " datetime.datetime(2021, 4, 18, 0, 0),\n",
       " datetime.datetime(2021, 4, 25, 0, 0),\n",
       " datetime.datetime(2021, 5, 2, 0, 0),\n",
       " datetime.datetime(2021, 5, 9, 0, 0),\n",
       " datetime.datetime(2021, 5, 16, 0, 0),\n",
       " datetime.datetime(2021, 5, 23, 0, 0),\n",
       " datetime.datetime(2021, 5, 30, 0, 0),\n",
       " datetime.datetime(2021, 6, 6, 0, 0),\n",
       " datetime.datetime(2021, 6, 13, 0, 0),\n",
       " datetime.datetime(2021, 6, 20, 0, 0),\n",
       " datetime.datetime(2021, 6, 27, 0, 0),\n",
       " datetime.datetime(2021, 7, 4, 0, 0),\n",
       " datetime.datetime(2021, 7, 11, 0, 0),\n",
       " datetime.datetime(2021, 7, 18, 0, 0),\n",
       " datetime.datetime(2021, 7, 25, 0, 0),\n",
       " datetime.datetime(2021, 8, 1, 0, 0),\n",
       " datetime.datetime(2021, 8, 8, 0, 0),\n",
       " datetime.datetime(2021, 8, 15, 0, 0),\n",
       " datetime.datetime(2021, 8, 22, 0, 0),\n",
       " datetime.datetime(2021, 8, 29, 0, 0),\n",
       " datetime.datetime(2021, 9, 5, 0, 0),\n",
       " datetime.datetime(2021, 9, 12, 0, 0),\n",
       " datetime.datetime(2021, 9, 19, 0, 0),\n",
       " datetime.datetime(2021, 9, 26, 0, 0),\n",
       " datetime.datetime(2021, 10, 3, 0, 0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_emb = []\n",
    "for fp in sorted(os.listdir(\"../data/raw/sec/\")):\n",
    "    full_path = os.path.join(\"../data/raw\", \"sec\", fp)\n",
    "    if fp.split(\".\")[-1]=='npy':\n",
    "        comp_emb.append(torch.from_numpy(np.load(full_path)))\n",
    "comp_emb = torch.stack(comp_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y = []\n",
    "for i in range(260):\n",
    "    start_date = date_list[i]\n",
    "    end_date = start_date+timedelta(days=6)\n",
    "    next_start_date = start_date+timedelta(days=7)\n",
    "    next_end_date = start_date+timedelta(days=13)\n",
    "    ######################################################## \n",
    "    # prepare X (change this if you want to add SEC emb, etc.)\n",
    "    ########################################################\n",
    "    curr = stock_df[(stock_df.Date>=start_date) & (stock_df.Date<=end_date)]\n",
    "    X = curr.pivot_table(\n",
    "            index=\"Date\", columns=\"ticker_symbol\", values=\"Close\"\n",
    "        ).values.T\n",
    "    X_tensor = torch.tensor(X)\n",
    "    \n",
    "    ########################################################\n",
    "    # prepare y (change this if you want to change labels)\n",
    "    ########################################################\n",
    "\n",
    "    \n",
    "    nxt = stock_df[(stock_df.Date>=next_start_date) & (stock_df.Date<=next_end_date)]\n",
    "    y = nxt.pivot_table(\n",
    "            index=\"Date\", columns=\"ticker_symbol\", values=\"Close\"\n",
    "        ).values.T\n",
    "    y = (y.mean(1) - X.mean(1)) / X.mean(1)\n",
    "    y_tensor = torch.tensor(y)\n",
    "    X_y.append((X_tensor,y_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/processed/twitter/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter = ['2016_q4']\n",
    "for i in range(2017, 2022):\n",
    "    for j in range(1, 5):\n",
    "        if i == 2021 and j == 4: break\n",
    "        quarter.append(str(i)+'_q'+str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016_q4',\n",
       " '2017_q1',\n",
       " '2017_q2',\n",
       " '2017_q3',\n",
       " '2017_q4',\n",
       " '2018_q1',\n",
       " '2018_q2',\n",
       " '2018_q3',\n",
       " '2018_q4',\n",
       " '2019_q1',\n",
       " '2019_q2',\n",
       " '2019_q3',\n",
       " '2019_q4',\n",
       " '2020_q1',\n",
       " '2020_q2',\n",
       " '2020_q3',\n",
       " '2020_q4',\n",
       " '2021_q1',\n",
       " '2021_q2',\n",
       " '2021_q3']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for i in quarter:\n",
    "    paths.append(path+i+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/processed/twitter/2016_q4.pt',\n",
       " '../data/processed/twitter/2017_q1.pt',\n",
       " '../data/processed/twitter/2017_q2.pt',\n",
       " '../data/processed/twitter/2017_q3.pt',\n",
       " '../data/processed/twitter/2017_q4.pt',\n",
       " '../data/processed/twitter/2018_q1.pt',\n",
       " '../data/processed/twitter/2018_q2.pt',\n",
       " '../data/processed/twitter/2018_q3.pt',\n",
       " '../data/processed/twitter/2018_q4.pt',\n",
       " '../data/processed/twitter/2019_q1.pt',\n",
       " '../data/processed/twitter/2019_q2.pt',\n",
       " '../data/processed/twitter/2019_q3.pt',\n",
       " '../data/processed/twitter/2019_q4.pt',\n",
       " '../data/processed/twitter/2020_q1.pt',\n",
       " '../data/processed/twitter/2020_q2.pt',\n",
       " '../data/processed/twitter/2020_q3.pt',\n",
       " '../data/processed/twitter/2020_q4.pt',\n",
       " '../data/processed/twitter/2021_q1.pt',\n",
       " '../data/processed/twitter/2021_q2.pt',\n",
       " '../data/processed/twitter/2021_q3.pt']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data(x=[29, 63], edge_index=[2, 760], edge_attr=[760], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 760\n",
      "Average node degree: 26.21\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 62], edge_index=[2, 312], edge_attr=[312], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 312\n",
      "Average node degree: 10.76\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 63], edge_index=[2, 400], edge_attr=[400], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 400\n",
      "Average node degree: 13.79\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 63], edge_index=[2, 552], edge_attr=[552], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 552\n",
      "Average node degree: 19.03\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 63], edge_index=[2, 805], edge_attr=[805], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 805\n",
      "Average node degree: 27.76\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 61], edge_index=[2, 400], edge_attr=[400], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 400\n",
      "Average node degree: 13.79\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 64], edge_index=[2, 616], edge_attr=[616], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 616\n",
      "Average node degree: 21.24\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 63], edge_index=[2, 585], edge_attr=[585], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 585\n",
      "Average node degree: 20.17\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 63], edge_index=[2, 805], edge_attr=[805], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 805\n",
      "Average node degree: 27.76\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 61], edge_index=[2, 552], edge_attr=[552], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 552\n",
      "Average node degree: 19.03\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 63], edge_index=[2, 552], edge_attr=[552], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 552\n",
      "Average node degree: 19.03\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 64], edge_index=[2, 672], edge_attr=[672], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 672\n",
      "Average node degree: 23.17\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 64], edge_index=[2, 777], edge_attr=[777], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 777\n",
      "Average node degree: 26.79\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 62], edge_index=[2, 480], edge_attr=[480], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 480\n",
      "Average node degree: 16.55\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 63], edge_index=[2, 585], edge_attr=[585], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 585\n",
      "Average node degree: 20.17\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 64], edge_index=[2, 697], edge_attr=[697], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 697\n",
      "Average node degree: 24.03\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 64], edge_index=[2, 805], edge_attr=[805], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 805\n",
      "Average node degree: 27.76\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 61], edge_index=[2, 616], edge_attr=[616], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 616\n",
      "Average node degree: 21.24\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 63], edge_index=[2, 720], edge_attr=[720], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 720\n",
      "Average node degree: 24.83\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n",
      "\n",
      "Data(x=[29, 64], edge_index=[2, 741], edge_attr=[741], y=[29])\n",
      "===========================================================================================================\n",
      "Number of nodes: 29\n",
      "Number of edges: 741\n",
      "Average node degree: 25.55\n",
      "Has isolated nodes: False\n",
      "Has self-loops: True\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "for path in paths:\n",
    "    dataset = torch.load(path)\n",
    "    data = dataset[0]\n",
    "    data_list.append(transform_and_split(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 62])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[1].x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "edge_indices = [i.edge_index.double() for i in data_list]\n",
    "edge_weights = [i.edge_attr.double() for i in data_list]\n",
    "features = [i.x.double() for i in data_list]\n",
    "targets = [i.y.double() for i in data_list]\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "edge_indices = [i.edge_index.cpu().detach().numpy() for i in data_list]\n",
    "edge_weights = [i.edge_attr.cpu().detach().numpy() for i in data_list]\n",
    "features = [i.x.cpu().detach().numpy() for i in data_list]\n",
    "targets = [i.y.cpu().detach().numpy() for i in data_list]\n",
    "\"\"\"\n",
    "edge_indices = [i.edge_index.numpy() for i in data_list]\n",
    "edge_weights = [i.edge_attr.numpy() for i in data_list]\n",
    "features = [i.x.numpy() for i in data_list]\n",
    "targets = [i.y.numpy() for i in data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 2,  3,  4, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,\n",
       "          2,  2,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,\n",
       "          6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  9,  9,  9,\n",
       "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10,\n",
       "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12, 12, 12,\n",
       "         12, 12, 12, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15,\n",
       "         15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17,\n",
       "         17, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 20, 20, 20,\n",
       "         20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "         21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23,\n",
       "         23, 23, 23, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25,\n",
       "         25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "         25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27,\n",
       "         27, 27, 28, 28, 28, 28, 28, 28],\n",
       "        [ 4,  9, 10, 21, 22, 25,  4,  9, 10, 21, 22, 25,  4,  9, 10, 21,\n",
       "         22, 25,  4,  9, 10, 21, 22, 25,  0,  1,  2,  3,  4,  5,  6,  7,\n",
       "          8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
       "         24, 25, 26, 27, 28,  4,  9, 10, 21, 22, 25,  4,  9, 10, 21, 22,\n",
       "         25,  4,  9, 10, 21, 22, 25,  4,  9, 10, 21, 22, 25,  0,  1,  2,\n",
       "          3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  0,  1,  2,  3,  4,  5,\n",
       "          6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "         22, 23, 24, 25, 26, 27, 28,  4,  9, 10, 21, 22, 25,  4,  9, 10,\n",
       "         21, 22, 25,  4,  9, 10, 21, 22, 25,  4,  9, 10, 21, 22, 25,  4,\n",
       "          9, 10, 21, 22, 25,  4,  9, 10, 21, 22, 25,  4,  9, 10, 21, 22,\n",
       "         25,  4,  9, 10, 21, 22, 25,  4,  9, 10, 21, 22, 25,  4,  9, 10,\n",
       "         21, 22, 25,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
       "         13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,\n",
       "          0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "         16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  4,  9, 10,\n",
       "         21, 22, 25,  4,  9, 10, 21, 22, 25,  0,  1,  2,  3,  4,  5,  6,\n",
       "          7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "         23, 24, 25, 26, 27, 28,  4,  9, 10, 21, 22, 25,  4,  9, 10, 21,\n",
       "         22, 25,  4,  9, 10, 21, 22, 25]], dtype=int64),\n",
       " array([[ 0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "          3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,\n",
       "          6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "          7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,\n",
       "          8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11,\n",
       "         11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13,\n",
       "         13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15,\n",
       "         15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "         16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "         16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18,\n",
       "         18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20,\n",
       "         20, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22,\n",
       "         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "         22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24,\n",
       "         24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "         25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "         25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
       "         26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28],\n",
       "        [ 3,  4,  7,  9, 16, 22, 25, 26,  3,  4,  7,  9, 16, 22, 25, 26,\n",
       "          3,  4,  7,  9, 16, 22, 25, 26,  0,  1,  2,  3,  4,  5,  6,  7,\n",
       "          8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
       "         24, 25, 26, 27, 28,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,\n",
       "         11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "         27, 28,  3,  4,  7,  9, 16, 22, 25, 26,  3,  4,  7,  9, 16, 22,\n",
       "         25, 26,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "         14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  3,\n",
       "          4,  7,  9, 16, 22, 25, 26,  0,  1,  2,  3,  4,  5,  6,  7,  8,\n",
       "          9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "         25, 26, 27, 28,  3,  4,  7,  9, 16, 22, 25, 26,  3,  4,  7,  9,\n",
       "         16, 22, 25, 26,  3,  4,  7,  9, 16, 22, 25, 26,  3,  4,  7,  9,\n",
       "         16, 22, 25, 26,  3,  4,  7,  9, 16, 22, 25, 26,  3,  4,  7,  9,\n",
       "         16, 22, 25, 26,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11,\n",
       "         12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "         28,  3,  4,  7,  9, 16, 22, 25, 26,  3,  4,  7,  9, 16, 22, 25,\n",
       "         26,  3,  4,  7,  9, 16, 22, 25, 26,  3,  4,  7,  9, 16, 22, 25,\n",
       "         26,  3,  4,  7,  9, 16, 22, 25, 26,  0,  1,  2,  3,  4,  5,  6,\n",
       "          7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "         23, 24, 25, 26, 27, 28,  3,  4,  7,  9, 16, 22, 25, 26,  3,  4,\n",
       "          7,  9, 16, 22, 25, 26,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9,\n",
       "         10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,\n",
       "         26, 27, 28,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
       "         13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,\n",
       "          3,  4,  7,  9, 16, 22, 25, 26,  3,  4,  7,  9, 16, 22, 25, 26]],\n",
       "       dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 3,  4,  5, ..., 22, 25, 26]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 0,  1,  2, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "          3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,\n",
       "          6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,\n",
       "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "          8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         10, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12,\n",
       "         12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
       "         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14,\n",
       "         14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16,\n",
       "         16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18,\n",
       "         18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22,\n",
       "         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23,\n",
       "         23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25,\n",
       "         25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "         25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26,\n",
       "         27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28],\n",
       "        [ 3,  4,  8,  9, 10, 13, 22, 25,  3,  4,  8,  9, 10, 13, 22, 25,\n",
       "          3,  4,  8,  9, 10, 13, 22, 25,  0,  1,  2,  3,  4,  5,  6,  7,\n",
       "          8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
       "         24, 25, 26, 27, 28,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,\n",
       "         11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "         27, 28,  3,  4,  8,  9, 10, 13, 22, 25,  3,  4,  8,  9, 10, 13,\n",
       "         22, 25,  3,  4,  8,  9, 10, 13, 22, 25,  0,  1,  2,  3,  4,  5,\n",
       "          6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "         22, 23, 24, 25, 26, 27, 28,  0,  1,  2,  3,  4,  5,  6,  7,  8,\n",
       "          9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "         25, 26, 27, 28,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11,\n",
       "         12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "         28,  3,  4,  8,  9, 10, 13, 22, 25,  3,  4,  8,  9, 10, 13, 22,\n",
       "         25,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
       "         15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  3,  4,\n",
       "          8,  9, 10, 13, 22, 25,  3,  4,  8,  9, 10, 13, 22, 25,  3,  4,\n",
       "          8,  9, 10, 13, 22, 25,  3,  4,  8,  9, 10, 13, 22, 25,  3,  4,\n",
       "          8,  9, 10, 13, 22, 25,  3,  4,  8,  9, 10, 13, 22, 25,  3,  4,\n",
       "          8,  9, 10, 13, 22, 25,  3,  4,  8,  9, 10, 13, 22, 25,  0,  1,\n",
       "          2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  3,  4,  8,  9, 10,\n",
       "         13, 22, 25,  3,  4,  8,  9, 10, 13, 22, 25,  0,  1,  2,  3,  4,\n",
       "          5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
       "         21, 22, 23, 24, 25, 26, 27, 28,  3,  4,  8,  9, 10, 13, 22, 25,\n",
       "          3,  4,  8,  9, 10, 13, 22, 25,  3,  4,  8,  9, 10, 13, 22, 25]],\n",
       "       dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 2,  3,  4, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 2,  3,  4, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 0,  1,  2, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 0,  1,  2, ..., 13, 16, 25]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 3,  4,  5, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 0,  1,  2, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 0,  1,  2, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,\n",
       "          1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "          2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "          3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,\n",
       "          4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
       "          4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,\n",
       "          5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "          6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "          6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,\n",
       "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "          8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "          9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "         11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12,\n",
       "         12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14,\n",
       "         14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "         15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "         16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17,\n",
       "         17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18,\n",
       "         18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "         21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "         21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "         22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
       "         22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24,\n",
       "         24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "         25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
       "         25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27,\n",
       "         27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28],\n",
       "        [ 2,  3,  4,  6,  8,  9, 16, 21, 22, 25,  2,  3,  4,  6,  8,  9,\n",
       "         16, 21, 22, 25,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11,\n",
       "         12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "         28,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
       "         15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  0,  1,\n",
       "          2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  2,  3,  4,  6,  8,\n",
       "          9, 16, 21, 22, 25,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,\n",
       "         11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "         27, 28,  2,  3,  4,  6,  8,  9, 16, 21, 22, 25,  0,  1,  2,  3,\n",
       "          4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
       "         20, 21, 22, 23, 24, 25, 26, 27, 28,  0,  1,  2,  3,  4,  5,  6,\n",
       "          7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "         23, 24, 25, 26, 27, 28,  2,  3,  4,  6,  8,  9, 16, 21, 22, 25,\n",
       "          2,  3,  4,  6,  8,  9, 16, 21, 22, 25,  2,  3,  4,  6,  8,  9,\n",
       "         16, 21, 22, 25,  2,  3,  4,  6,  8,  9, 16, 21, 22, 25,  2,  3,\n",
       "          4,  6,  8,  9, 16, 21, 22, 25,  2,  3,  4,  6,  8,  9, 16, 21,\n",
       "         22, 25,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
       "         14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,  2,\n",
       "          3,  4,  6,  8,  9, 16, 21, 22, 25,  2,  3,  4,  6,  8,  9, 16,\n",
       "         21, 22, 25,  2,  3,  4,  6,  8,  9, 16, 21, 22, 25,  2,  3,  4,\n",
       "          6,  8,  9, 16, 21, 22, 25,  0,  1,  2,  3,  4,  5,  6,  7,  8,\n",
       "          9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "         25, 26, 27, 28,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11,\n",
       "         12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "         28,  2,  3,  4,  6,  8,  9, 16, 21, 22, 25,  2,  3,  4,  6,  8,\n",
       "          9, 16, 21, 22, 25,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10,\n",
       "         11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n",
       "         27, 28,  2,  3,  4,  6,  8,  9, 16, 21, 22, 25,  2,  3,  4,  6,\n",
       "          8,  9, 16, 21, 22, 25,  2,  3,  4,  6,  8,  9, 16, 21, 22, 25]],\n",
       "       dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 3,  4,  5, ..., 21, 22, 25]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 3,  4,  5, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 0,  1,  2, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 3,  4,  7, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 0,  1,  2, ..., 26, 27, 28]], dtype=int64),\n",
       " array([[ 0,  0,  0, ..., 28, 28, 28],\n",
       "        [ 0,  1,  2, ..., 26, 27, 28]], dtype=int64)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import dense_to_sparse\n",
    "edge_idx = []\n",
    "edge_att = []\n",
    "for i in range(260):\n",
    "    edge_index, edge_attr = dense_to_sparse(torch.from_numpy(mat_list[i]))\n",
    "    edge_idx.append(edge_index)\n",
    "    edge_att.append(edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_indices = [i.numpy() for i in edge_idx]\n",
    "edge_weights = [i.numpy() for i in edge_att]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "targets = []\n",
    "for i in range(260):\n",
    "    features.append(normalize(X_y[i][0].numpy(), axis=1, norm='max'))\n",
    "    #features.append(X_y[i][0].numpy())\n",
    "    targets.append([1 if a > 0 else 0 for a in X_y[i][1].numpy()])\n",
    "targets = np.asarray(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 29)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_list[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_features = []\n",
    "for i in features:\n",
    "    padded_features.append(np.pad(i, [(0, 0), (0, 5-i.shape[1])], 'mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_features = np.asarray(padded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 29, 5)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_emb = np.asarray([comp_emb.numpy() for i in range(260)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 29, 768)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_features = np.concatenate((padded_features, comp_emb), axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.signal import DynamicGraphTemporalSignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_signal = DynamicGraphTemporalSignal(edge_indices = edge_indices , edge_weights = edge_weights, features = padded_features, targets = targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch_geometric_temporal.signal.dynamic_graph_temporal_signal.DynamicGraphTemporalSignal at 0x22de69f7f10>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporal_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "\n",
    "train_dataset, test_dataset = temporal_signal_split(temporal_signal, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric_temporal.nn.recurrent import DCRNN\n",
    "from torch_geometric_temporal import TemporalConv\n",
    "from torch_geometric_temporal import EvolveGCNO\n",
    "from torch_geometric_temporal import GConvGRU\n",
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, node_features):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        self.evol = EvolveGCNO(node_features)\n",
    "        self.recurrent = DCRNN(node_features, 16, 1)\n",
    "        self.conv = GConvGRU(node_features, 64, 3)\n",
    "        #self.linear = torch.nn.Linear(16, 1)\n",
    "        self.linear = torch.nn.Linear(64, 2)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "#        h = self.recurrent(x, edge_index, edge_weight)\n",
    "#        h = self.dropout(h)\n",
    "        h = self.conv(x, edge_index, edge_weight)\n",
    "        h = F.relu(h)\n",
    "        h = self.linear(h)\n",
    "        h = torch.sigmoid(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [28:51<00:00,  8.66s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model = RecurrentGCN(node_features = 773)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(200)):\n",
    "    loss = 0\n",
    "    for time, snapshot in enumerate(train_dataset):\n",
    "        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "        loss += torch.nn.CrossEntropyLoss()(y_hat, snapshot.y.long())\n",
    "#        loss += torch.mean((y_hat-snapshot.y)**2)\n",
    "#        loss = loss / (time+1)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_l = []\n",
    "model.eval()\n",
    "cost = 0\n",
    "for time, snapshot in enumerate(test_dataset):\n",
    "    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "    #cost = cost + torch.mean((y_hat-snapshot.y)**2)\n",
    "    y_hat_l.append(y_hat)\n",
    "#cost = cost / (time+1)\n",
    "#cost = cost.item()\n",
    "#print(\"MSE: {:.4f}\".format(cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.2463, 0.7511],\n",
       "         [0.6364, 0.3697],\n",
       "         [0.1962, 0.8088],\n",
       "         [0.2867, 0.7180],\n",
       "         [0.3509, 0.6552],\n",
       "         [0.0968, 0.9059],\n",
       "         [0.2954, 0.6922],\n",
       "         [0.4180, 0.5805],\n",
       "         [0.3080, 0.6721],\n",
       "         [0.3419, 0.6665],\n",
       "         [0.3282, 0.6772],\n",
       "         [0.3104, 0.6879],\n",
       "         [0.3382, 0.6655],\n",
       "         [0.3535, 0.6595],\n",
       "         [0.4030, 0.6023],\n",
       "         [0.2265, 0.7795],\n",
       "         [0.3195, 0.6725],\n",
       "         [0.2009, 0.8185],\n",
       "         [0.4203, 0.5928],\n",
       "         [0.2005, 0.8159],\n",
       "         [0.1237, 0.8772],\n",
       "         [0.4547, 0.5522],\n",
       "         [0.5536, 0.4180],\n",
       "         [0.8471, 0.1337],\n",
       "         [0.0891, 0.9063],\n",
       "         [0.1991, 0.8201],\n",
       "         [0.7791, 0.2162],\n",
       "         [0.3796, 0.6143],\n",
       "         [0.1670, 0.8344]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1557, 0.8454],\n",
       "         [0.5533, 0.4568],\n",
       "         [0.0758, 0.9287],\n",
       "         [0.1706, 0.8340],\n",
       "         [0.1782, 0.8324],\n",
       "         [0.0193, 0.9826],\n",
       "         [0.0698, 0.9306],\n",
       "         [0.3539, 0.6482],\n",
       "         [0.2330, 0.7521],\n",
       "         [0.1687, 0.8385],\n",
       "         [0.1825, 0.8250],\n",
       "         [0.1152, 0.8941],\n",
       "         [0.1062, 0.8994],\n",
       "         [0.1940, 0.8210],\n",
       "         [0.1905, 0.8170],\n",
       "         [0.0530, 0.9501],\n",
       "         [0.1188, 0.8828],\n",
       "         [0.0888, 0.9221],\n",
       "         [0.1758, 0.8339],\n",
       "         [0.8300, 0.1547],\n",
       "         [0.0310, 0.9709],\n",
       "         [0.3948, 0.6161],\n",
       "         [0.5127, 0.4804],\n",
       "         [0.4184, 0.5873],\n",
       "         [0.0495, 0.9508],\n",
       "         [0.1370, 0.8771],\n",
       "         [0.6754, 0.3245],\n",
       "         [0.2030, 0.7972],\n",
       "         [0.0980, 0.9031]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3230, 0.6680],\n",
       "         [0.4262, 0.5921],\n",
       "         [0.4227, 0.5808],\n",
       "         [0.4643, 0.5294],\n",
       "         [0.6287, 0.3348],\n",
       "         [0.4277, 0.5590],\n",
       "         [0.4268, 0.5458],\n",
       "         [0.5311, 0.4611],\n",
       "         [0.5218, 0.4458],\n",
       "         [0.5795, 0.4158],\n",
       "         [0.4910, 0.5098],\n",
       "         [0.3573, 0.6204],\n",
       "         [0.5691, 0.4275],\n",
       "         [0.4720, 0.5329],\n",
       "         [0.5614, 0.4403],\n",
       "         [0.6020, 0.3961],\n",
       "         [0.4874, 0.4976],\n",
       "         [0.2772, 0.7437],\n",
       "         [0.5006, 0.5009],\n",
       "         [0.4288, 0.5633],\n",
       "         [0.3348, 0.6503],\n",
       "         [0.2783, 0.7181],\n",
       "         [0.3174, 0.6664],\n",
       "         [0.4200, 0.5745],\n",
       "         [0.4567, 0.4849],\n",
       "         [0.2736, 0.7452],\n",
       "         [0.4549, 0.5553],\n",
       "         [0.5385, 0.4525],\n",
       "         [0.4087, 0.5812]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2868, 0.7052],\n",
       "         [0.5817, 0.4289],\n",
       "         [0.4466, 0.5563],\n",
       "         [0.4611, 0.5330],\n",
       "         [0.5793, 0.3846],\n",
       "         [0.4750, 0.5092],\n",
       "         [0.5082, 0.4624],\n",
       "         [0.4453, 0.5515],\n",
       "         [0.4173, 0.5539],\n",
       "         [0.5155, 0.4812],\n",
       "         [0.5246, 0.4750],\n",
       "         [0.3611, 0.6178],\n",
       "         [0.7060, 0.2869],\n",
       "         [0.7045, 0.2902],\n",
       "         [0.5030, 0.5020],\n",
       "         [0.2775, 0.7299],\n",
       "         [0.4057, 0.5823],\n",
       "         [0.2989, 0.7219],\n",
       "         [0.5058, 0.4717],\n",
       "         [0.2466, 0.7635],\n",
       "         [0.3313, 0.6540],\n",
       "         [0.2593, 0.7446],\n",
       "         [0.4953, 0.4740],\n",
       "         [0.5049, 0.4763],\n",
       "         [0.2295, 0.7404],\n",
       "         [0.2275, 0.7904],\n",
       "         [0.4887, 0.5175],\n",
       "         [0.5673, 0.4236],\n",
       "         [0.4005, 0.5907]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2478, 0.7461],\n",
       "         [0.5391, 0.4761],\n",
       "         [0.3703, 0.6348],\n",
       "         [0.6416, 0.3465],\n",
       "         [0.4791, 0.4922],\n",
       "         [0.2391, 0.7576],\n",
       "         [0.3453, 0.6324],\n",
       "         [0.4292, 0.5690],\n",
       "         [0.4262, 0.5432],\n",
       "         [0.4794, 0.5215],\n",
       "         [0.4091, 0.5942],\n",
       "         [0.3602, 0.6217],\n",
       "         [0.3466, 0.6573],\n",
       "         [0.4378, 0.5677],\n",
       "         [0.4913, 0.5116],\n",
       "         [0.4022, 0.6003],\n",
       "         [0.3654, 0.6247],\n",
       "         [0.3443, 0.6781],\n",
       "         [0.4849, 0.5036],\n",
       "         [0.2421, 0.7706],\n",
       "         [0.2081, 0.7856],\n",
       "         [0.4775, 0.5258],\n",
       "         [0.5037, 0.4553],\n",
       "         [0.5592, 0.4198],\n",
       "         [0.4231, 0.5212],\n",
       "         [0.1552, 0.8558],\n",
       "         [0.5728, 0.4345],\n",
       "         [0.6402, 0.3479],\n",
       "         [0.2942, 0.6995]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.0958, 0.9060],\n",
       "         [0.4900, 0.5268],\n",
       "         [0.1996, 0.8077],\n",
       "         [0.1801, 0.8237],\n",
       "         [0.4239, 0.5804],\n",
       "         [0.0259, 0.9759],\n",
       "         [0.1733, 0.8178],\n",
       "         [0.4478, 0.5493],\n",
       "         [0.3275, 0.6488],\n",
       "         [0.3110, 0.6993],\n",
       "         [0.2275, 0.7792],\n",
       "         [0.2141, 0.7879],\n",
       "         [0.3209, 0.6843],\n",
       "         [0.3193, 0.6911],\n",
       "         [0.3265, 0.6814],\n",
       "         [0.3966, 0.6059],\n",
       "         [0.2485, 0.7465],\n",
       "         [0.3067, 0.7151],\n",
       "         [0.4353, 0.5719],\n",
       "         [0.4250, 0.5635],\n",
       "         [0.0289, 0.9727],\n",
       "         [0.2129, 0.7841],\n",
       "         [0.4540, 0.5144],\n",
       "         [0.5617, 0.4153],\n",
       "         [0.0441, 0.9540],\n",
       "         [0.1243, 0.8894],\n",
       "         [0.6564, 0.3481],\n",
       "         [0.3542, 0.6431],\n",
       "         [0.1810, 0.8166]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1477, 0.8508],\n",
       "         [0.4423, 0.5765],\n",
       "         [0.3971, 0.6091],\n",
       "         [0.3855, 0.6107],\n",
       "         [0.3944, 0.5884],\n",
       "         [0.3022, 0.6906],\n",
       "         [0.1953, 0.7920],\n",
       "         [0.4694, 0.5250],\n",
       "         [0.3568, 0.6274],\n",
       "         [0.4314, 0.5675],\n",
       "         [0.3180, 0.6865],\n",
       "         [0.3055, 0.6814],\n",
       "         [0.4843, 0.5136],\n",
       "         [0.3595, 0.6446],\n",
       "         [0.3700, 0.6394],\n",
       "         [0.3798, 0.6246],\n",
       "         [0.3413, 0.6505],\n",
       "         [0.2924, 0.7293],\n",
       "         [0.3771, 0.6060],\n",
       "         [0.3568, 0.6429],\n",
       "         [0.2041, 0.7899],\n",
       "         [0.3029, 0.6864],\n",
       "         [0.2797, 0.7157],\n",
       "         [0.4632, 0.5271],\n",
       "         [0.3128, 0.6390],\n",
       "         [0.2023, 0.8141],\n",
       "         [0.3966, 0.6156],\n",
       "         [0.5137, 0.4802],\n",
       "         [0.2181, 0.7815]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1638, 0.8343],\n",
       "         [0.4943, 0.5226],\n",
       "         [0.2754, 0.7315],\n",
       "         [0.2805, 0.7191],\n",
       "         [0.2876, 0.7208],\n",
       "         [0.0641, 0.9379],\n",
       "         [0.2612, 0.7221],\n",
       "         [0.4261, 0.5726],\n",
       "         [0.4214, 0.5479],\n",
       "         [0.3293, 0.6777],\n",
       "         [0.3854, 0.6061],\n",
       "         [0.2727, 0.7174],\n",
       "         [0.2885, 0.7164],\n",
       "         [0.2729, 0.7361],\n",
       "         [0.3465, 0.6608],\n",
       "         [0.5574, 0.4414],\n",
       "         [0.2432, 0.7523],\n",
       "         [0.2059, 0.8126],\n",
       "         [0.3812, 0.6031],\n",
       "         [0.2844, 0.7197],\n",
       "         [0.1505, 0.8458],\n",
       "         [0.2251, 0.7700],\n",
       "         [0.3909, 0.5867],\n",
       "         [0.3099, 0.6990],\n",
       "         [0.2105, 0.7643],\n",
       "         [0.1822, 0.8338],\n",
       "         [0.5162, 0.4933],\n",
       "         [0.6555, 0.3319],\n",
       "         [0.1790, 0.8192]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1291, 0.8704],\n",
       "         [0.3914, 0.6262],\n",
       "         [0.1877, 0.8199],\n",
       "         [0.2216, 0.7799],\n",
       "         [0.3020, 0.7030],\n",
       "         [0.2723, 0.7207],\n",
       "         [0.2110, 0.7759],\n",
       "         [0.4141, 0.5852],\n",
       "         [0.3583, 0.6154],\n",
       "         [0.3186, 0.6861],\n",
       "         [0.2385, 0.7665],\n",
       "         [0.2019, 0.8022],\n",
       "         [0.2657, 0.7388],\n",
       "         [0.2199, 0.7877],\n",
       "         [0.3055, 0.7042],\n",
       "         [0.5029, 0.4947],\n",
       "         [0.2445, 0.7512],\n",
       "         [0.2161, 0.8021],\n",
       "         [0.4453, 0.5313],\n",
       "         [0.3397, 0.6586],\n",
       "         [0.1493, 0.8474],\n",
       "         [0.2119, 0.7901],\n",
       "         [0.3419, 0.6380],\n",
       "         [0.2716, 0.7384],\n",
       "         [0.1631, 0.8170],\n",
       "         [0.1387, 0.8749],\n",
       "         [0.4116, 0.5977],\n",
       "         [0.4103, 0.5866],\n",
       "         [0.2078, 0.7905]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2188, 0.7792],\n",
       "         [0.4436, 0.5754],\n",
       "         [0.2515, 0.7562],\n",
       "         [0.1356, 0.8696],\n",
       "         [0.3547, 0.6502],\n",
       "         [0.6714, 0.3086],\n",
       "         [0.3192, 0.6600],\n",
       "         [0.3866, 0.6139],\n",
       "         [0.4188, 0.5510],\n",
       "         [0.4062, 0.5969],\n",
       "         [0.6033, 0.3949],\n",
       "         [0.2254, 0.7703],\n",
       "         [0.4181, 0.5836],\n",
       "         [0.2517, 0.7575],\n",
       "         [0.3934, 0.6137],\n",
       "         [0.4094, 0.5927],\n",
       "         [0.2956, 0.6979],\n",
       "         [0.2884, 0.7323],\n",
       "         [0.4572, 0.5224],\n",
       "         [0.5187, 0.4633],\n",
       "         [0.3339, 0.6492],\n",
       "         [0.2209, 0.7790],\n",
       "         [0.3942, 0.5874],\n",
       "         [0.3734, 0.6248],\n",
       "         [0.1720, 0.8067],\n",
       "         [0.2172, 0.7998],\n",
       "         [0.4643, 0.5474],\n",
       "         [0.3187, 0.6806],\n",
       "         [0.4592, 0.5267]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2865, 0.7133],\n",
       "         [0.3766, 0.6458],\n",
       "         [0.3544, 0.6508],\n",
       "         [0.5127, 0.4765],\n",
       "         [0.4348, 0.5686],\n",
       "         [0.2594, 0.7354],\n",
       "         [0.3253, 0.6530],\n",
       "         [0.4373, 0.5604],\n",
       "         [0.2282, 0.7589],\n",
       "         [0.4213, 0.5827],\n",
       "         [0.3520, 0.6525],\n",
       "         [0.3669, 0.6103],\n",
       "         [0.4579, 0.5425],\n",
       "         [0.4441, 0.5613],\n",
       "         [0.3708, 0.6350],\n",
       "         [0.5884, 0.4083],\n",
       "         [0.3133, 0.6790],\n",
       "         [0.1951, 0.8232],\n",
       "         [0.4467, 0.5492],\n",
       "         [0.4651, 0.5217],\n",
       "         [0.2437, 0.7459],\n",
       "         [0.3012, 0.6884],\n",
       "         [0.3577, 0.6199],\n",
       "         [0.2364, 0.7742],\n",
       "         [0.4477, 0.4947],\n",
       "         [0.2479, 0.7694],\n",
       "         [0.4993, 0.5146],\n",
       "         [0.5452, 0.4448],\n",
       "         [0.3481, 0.6413]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1522, 0.8467],\n",
       "         [0.4244, 0.5956],\n",
       "         [0.2849, 0.7226],\n",
       "         [0.5390, 0.4500],\n",
       "         [0.5798, 0.3855],\n",
       "         [0.1097, 0.8914],\n",
       "         [0.2443, 0.7391],\n",
       "         [0.5246, 0.4667],\n",
       "         [0.3389, 0.6421],\n",
       "         [0.4098, 0.5929],\n",
       "         [0.2856, 0.7197],\n",
       "         [0.3312, 0.6515],\n",
       "         [0.3859, 0.6157],\n",
       "         [0.5144, 0.4876],\n",
       "         [0.2952, 0.7151],\n",
       "         [0.4596, 0.5424],\n",
       "         [0.2955, 0.6976],\n",
       "         [0.2522, 0.7685],\n",
       "         [0.4227, 0.5732],\n",
       "         [0.3822, 0.6147],\n",
       "         [0.1183, 0.8804],\n",
       "         [0.2879, 0.7029],\n",
       "         [0.2834, 0.7102],\n",
       "         [0.3285, 0.6744],\n",
       "         [0.3543, 0.5954],\n",
       "         [0.1555, 0.8598],\n",
       "         [0.4436, 0.5690],\n",
       "         [0.5588, 0.4314],\n",
       "         [0.2724, 0.7222]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1183, 0.8822],\n",
       "         [0.3928, 0.6272],\n",
       "         [0.1373, 0.8695],\n",
       "         [0.2067, 0.7950],\n",
       "         [0.3333, 0.6448],\n",
       "         [0.0693, 0.9327],\n",
       "         [0.1627, 0.8278],\n",
       "         [0.4026, 0.5974],\n",
       "         [0.2780, 0.7116],\n",
       "         [0.3276, 0.6805],\n",
       "         [0.2035, 0.8025],\n",
       "         [0.2190, 0.7760],\n",
       "         [0.2329, 0.7727],\n",
       "         [0.2415, 0.7703],\n",
       "         [0.2587, 0.7503],\n",
       "         [0.2222, 0.7830],\n",
       "         [0.1654, 0.8332],\n",
       "         [0.2525, 0.7690],\n",
       "         [0.3635, 0.6414],\n",
       "         [0.2856, 0.7160],\n",
       "         [0.0629, 0.9378],\n",
       "         [0.2084, 0.7877],\n",
       "         [0.3225, 0.6574],\n",
       "         [0.2728, 0.7343],\n",
       "         [0.0668, 0.9285],\n",
       "         [0.1312, 0.8828],\n",
       "         [0.5426, 0.4667],\n",
       "         [0.3728, 0.6228],\n",
       "         [0.1712, 0.8265]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2168, 0.7790],\n",
       "         [0.3977, 0.6198],\n",
       "         [0.2647, 0.7425],\n",
       "         [0.3851, 0.6099],\n",
       "         [0.3558, 0.6472],\n",
       "         [0.2336, 0.7617],\n",
       "         [0.3366, 0.6407],\n",
       "         [0.4688, 0.5268],\n",
       "         [0.2990, 0.6896],\n",
       "         [0.4031, 0.5983],\n",
       "         [0.3757, 0.6270],\n",
       "         [0.2477, 0.7459],\n",
       "         [0.4030, 0.5973],\n",
       "         [0.2952, 0.7132],\n",
       "         [0.3609, 0.6488],\n",
       "         [0.4537, 0.5490],\n",
       "         [0.3027, 0.6900],\n",
       "         [0.2554, 0.7656],\n",
       "         [0.4447, 0.5346],\n",
       "         [0.4662, 0.5211],\n",
       "         [0.2418, 0.7491],\n",
       "         [0.2007, 0.8043],\n",
       "         [0.2936, 0.6986],\n",
       "         [0.2805, 0.7275],\n",
       "         [0.1763, 0.8018],\n",
       "         [0.1626, 0.8531],\n",
       "         [0.4290, 0.5794],\n",
       "         [0.5200, 0.4717],\n",
       "         [0.3058, 0.6883]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1195, 0.8822],\n",
       "         [0.4096, 0.6088],\n",
       "         [0.1612, 0.8464],\n",
       "         [0.1776, 0.8251],\n",
       "         [0.1720, 0.8259],\n",
       "         [0.1222, 0.8781],\n",
       "         [0.1598, 0.8303],\n",
       "         [0.3450, 0.6565],\n",
       "         [0.3353, 0.6450],\n",
       "         [0.2199, 0.7868],\n",
       "         [0.2104, 0.7952],\n",
       "         [0.2397, 0.7584],\n",
       "         [0.2297, 0.7752],\n",
       "         [0.1745, 0.8340],\n",
       "         [0.2520, 0.7584],\n",
       "         [0.2989, 0.7102],\n",
       "         [0.3176, 0.6746],\n",
       "         [0.2340, 0.7857],\n",
       "         [0.4661, 0.5207],\n",
       "         [0.2440, 0.7636],\n",
       "         [0.0982, 0.9013],\n",
       "         [0.1760, 0.8228],\n",
       "         [0.3546, 0.6321],\n",
       "         [0.3188, 0.6870],\n",
       "         [0.0704, 0.9232],\n",
       "         [0.1452, 0.8691],\n",
       "         [0.4817, 0.5266],\n",
       "         [0.2858, 0.7146],\n",
       "         [0.1991, 0.7985]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1797, 0.8190],\n",
       "         [0.5272, 0.4865],\n",
       "         [0.1732, 0.8341],\n",
       "         [0.2464, 0.7566],\n",
       "         [0.2820, 0.7284],\n",
       "         [0.1093, 0.8926],\n",
       "         [0.1876, 0.8036],\n",
       "         [0.4668, 0.5249],\n",
       "         [0.3519, 0.6220],\n",
       "         [0.2915, 0.7192],\n",
       "         [0.2652, 0.7416],\n",
       "         [0.2653, 0.7222],\n",
       "         [0.2575, 0.7497],\n",
       "         [0.0911, 0.9177],\n",
       "         [0.2743, 0.7357],\n",
       "         [0.2571, 0.7489],\n",
       "         [0.2913, 0.7023],\n",
       "         [0.2308, 0.7893],\n",
       "         [0.3512, 0.6583],\n",
       "         [0.4026, 0.5929],\n",
       "         [0.1176, 0.8816],\n",
       "         [0.4446, 0.5616],\n",
       "         [0.5065, 0.4644],\n",
       "         [0.3593, 0.6482],\n",
       "         [0.1266, 0.8624],\n",
       "         [0.2271, 0.7911],\n",
       "         [0.6609, 0.3411],\n",
       "         [0.3122, 0.6862],\n",
       "         [0.2111, 0.7865]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1703, 0.8291],\n",
       "         [0.2598, 0.7634],\n",
       "         [0.3600, 0.6458],\n",
       "         [0.2713, 0.7270],\n",
       "         [0.6639, 0.3049],\n",
       "         [0.2107, 0.7865],\n",
       "         [0.4381, 0.5335],\n",
       "         [0.5564, 0.4346],\n",
       "         [0.2654, 0.7300],\n",
       "         [0.5159, 0.4811],\n",
       "         [0.3828, 0.6210],\n",
       "         [0.4291, 0.5471],\n",
       "         [0.6914, 0.3011],\n",
       "         [0.4570, 0.5498],\n",
       "         [0.4674, 0.5385],\n",
       "         [0.5310, 0.4697],\n",
       "         [0.4270, 0.5603],\n",
       "         [0.2024, 0.8162],\n",
       "         [0.3895, 0.5929],\n",
       "         [0.7987, 0.1818],\n",
       "         [0.2158, 0.7781],\n",
       "         [0.4459, 0.5567],\n",
       "         [0.2242, 0.7700],\n",
       "         [0.2353, 0.7722],\n",
       "         [0.2272, 0.7437],\n",
       "         [0.2227, 0.7954],\n",
       "         [0.3334, 0.6842],\n",
       "         [0.6320, 0.3566],\n",
       "         [0.3600, 0.6321]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.4804, 0.5022],\n",
       "         [0.5449, 0.4672],\n",
       "         [0.4503, 0.5520],\n",
       "         [0.6877, 0.3013],\n",
       "         [0.5788, 0.4154],\n",
       "         [0.4485, 0.5372],\n",
       "         [0.5309, 0.4414],\n",
       "         [0.5504, 0.4407],\n",
       "         [0.4331, 0.5362],\n",
       "         [0.5604, 0.4351],\n",
       "         [0.6781, 0.3184],\n",
       "         [0.3345, 0.6455],\n",
       "         [0.5931, 0.4040],\n",
       "         [0.3998, 0.6045],\n",
       "         [0.6064, 0.3934],\n",
       "         [0.2672, 0.7395],\n",
       "         [0.5205, 0.4648],\n",
       "         [0.1777, 0.8387],\n",
       "         [0.4268, 0.5507],\n",
       "         [0.7061, 0.2695],\n",
       "         [0.3868, 0.5974],\n",
       "         [0.2551, 0.7424],\n",
       "         [0.3825, 0.6056],\n",
       "         [0.4080, 0.5960],\n",
       "         [0.4384, 0.5110],\n",
       "         [0.3142, 0.7044],\n",
       "         [0.4921, 0.5142],\n",
       "         [0.5181, 0.4757],\n",
       "         [0.5648, 0.4215]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1430, 0.8580],\n",
       "         [0.5519, 0.4599],\n",
       "         [0.1529, 0.8545],\n",
       "         [0.1595, 0.8448],\n",
       "         [0.2926, 0.7142],\n",
       "         [0.1076, 0.8937],\n",
       "         [0.1527, 0.8392],\n",
       "         [0.4287, 0.5679],\n",
       "         [0.2948, 0.6832],\n",
       "         [0.2865, 0.7205],\n",
       "         [0.2374, 0.7685],\n",
       "         [0.2533, 0.7492],\n",
       "         [0.3242, 0.6800],\n",
       "         [0.2452, 0.7624],\n",
       "         [0.3442, 0.6654],\n",
       "         [0.2694, 0.7369],\n",
       "         [0.2543, 0.7410],\n",
       "         [0.2929, 0.7285],\n",
       "         [0.4072, 0.5887],\n",
       "         [0.4039, 0.5912],\n",
       "         [0.1476, 0.8493],\n",
       "         [0.2102, 0.7871],\n",
       "         [0.4043, 0.5818],\n",
       "         [0.4734, 0.5142],\n",
       "         [0.4498, 0.4940],\n",
       "         [0.1227, 0.8905],\n",
       "         [0.5150, 0.4911],\n",
       "         [0.4300, 0.5662],\n",
       "         [0.1877, 0.8118]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2937, 0.7055],\n",
       "         [0.4735, 0.5445],\n",
       "         [0.4799, 0.5242],\n",
       "         [0.6147, 0.3726],\n",
       "         [0.6771, 0.2885],\n",
       "         [0.4144, 0.5728],\n",
       "         [0.6480, 0.3205],\n",
       "         [0.5196, 0.4727],\n",
       "         [0.5445, 0.4206],\n",
       "         [0.5214, 0.4716],\n",
       "         [0.5731, 0.4243],\n",
       "         [0.4119, 0.5650],\n",
       "         [0.7215, 0.2708],\n",
       "         [0.3326, 0.6725],\n",
       "         [0.5861, 0.4180],\n",
       "         [0.3024, 0.7049],\n",
       "         [0.5013, 0.4842],\n",
       "         [0.2319, 0.7875],\n",
       "         [0.5333, 0.4526],\n",
       "         [0.4057, 0.5895],\n",
       "         [0.4147, 0.5672],\n",
       "         [0.3975, 0.5862],\n",
       "         [0.3870, 0.5970],\n",
       "         [0.5058, 0.4832],\n",
       "         [0.4650, 0.4778],\n",
       "         [0.1166, 0.8928],\n",
       "         [0.4098, 0.6022],\n",
       "         [0.7227, 0.2667],\n",
       "         [0.5468, 0.4413]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2082, 0.7888],\n",
       "         [0.4569, 0.5603],\n",
       "         [0.2269, 0.7802],\n",
       "         [0.2972, 0.7029],\n",
       "         [0.2924, 0.7163],\n",
       "         [0.1671, 0.8321],\n",
       "         [0.2857, 0.6976],\n",
       "         [0.3984, 0.6018],\n",
       "         [0.4048, 0.5660],\n",
       "         [0.3597, 0.6477],\n",
       "         [0.3082, 0.6972],\n",
       "         [0.1750, 0.8249],\n",
       "         [0.3492, 0.6553],\n",
       "         [0.3055, 0.7040],\n",
       "         [0.3837, 0.6226],\n",
       "         [0.4305, 0.5721],\n",
       "         [0.2778, 0.7165],\n",
       "         [0.1679, 0.8488],\n",
       "         [0.4359, 0.5543],\n",
       "         [0.6917, 0.2818],\n",
       "         [0.1833, 0.8112],\n",
       "         [0.2088, 0.7899],\n",
       "         [0.3167, 0.6793],\n",
       "         [0.2871, 0.7241],\n",
       "         [0.1545, 0.8294],\n",
       "         [0.1904, 0.8262],\n",
       "         [0.5280, 0.4807],\n",
       "         [0.4318, 0.5628],\n",
       "         [0.3327, 0.6604]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2718, 0.7248],\n",
       "         [0.4257, 0.5943],\n",
       "         [0.2751, 0.7323],\n",
       "         [0.3565, 0.6418],\n",
       "         [0.3970, 0.6064],\n",
       "         [0.4568, 0.5288],\n",
       "         [0.2950, 0.6850],\n",
       "         [0.4566, 0.5385],\n",
       "         [0.4825, 0.4840],\n",
       "         [0.3726, 0.6318],\n",
       "         [0.5259, 0.4741],\n",
       "         [0.3742, 0.6049],\n",
       "         [0.4051, 0.5965],\n",
       "         [0.3511, 0.6563],\n",
       "         [0.3968, 0.6090],\n",
       "         [0.5184, 0.4823],\n",
       "         [0.3617, 0.6286],\n",
       "         [0.2584, 0.7624],\n",
       "         [0.3832, 0.6060],\n",
       "         [0.2930, 0.7120],\n",
       "         [0.1987, 0.7946],\n",
       "         [0.3684, 0.6200],\n",
       "         [0.3208, 0.6697],\n",
       "         [0.3413, 0.6610],\n",
       "         [0.2675, 0.6895],\n",
       "         [0.1643, 0.8512],\n",
       "         [0.4619, 0.5486],\n",
       "         [0.5184, 0.4733],\n",
       "         [0.4123, 0.5765]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1702, 0.8272],\n",
       "         [0.3798, 0.6413],\n",
       "         [0.0848, 0.9212],\n",
       "         [0.1780, 0.8240],\n",
       "         [0.2743, 0.7151],\n",
       "         [0.1218, 0.8787],\n",
       "         [0.1433, 0.8482],\n",
       "         [0.3451, 0.6565],\n",
       "         [0.4238, 0.5446],\n",
       "         [0.3230, 0.6832],\n",
       "         [0.2641, 0.7415],\n",
       "         [0.2247, 0.7727],\n",
       "         [0.1999, 0.8058],\n",
       "         [0.2856, 0.7226],\n",
       "         [0.2941, 0.7139],\n",
       "         [0.1407, 0.8644],\n",
       "         [0.1423, 0.8574],\n",
       "         [0.2190, 0.8002],\n",
       "         [0.3329, 0.6582],\n",
       "         [0.2957, 0.7075],\n",
       "         [0.1084, 0.8903],\n",
       "         [0.3247, 0.6645],\n",
       "         [0.2355, 0.7675],\n",
       "         [0.3296, 0.6724],\n",
       "         [0.1575, 0.8180],\n",
       "         [0.1037, 0.9057],\n",
       "         [0.4395, 0.5743],\n",
       "         [0.3992, 0.5975],\n",
       "         [0.1851, 0.8121]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1219, 0.8783],\n",
       "         [0.4123, 0.6063],\n",
       "         [0.1810, 0.8263],\n",
       "         [0.0364, 0.9669],\n",
       "         [0.2262, 0.7840],\n",
       "         [0.0798, 0.9220],\n",
       "         [0.1437, 0.8489],\n",
       "         [0.3781, 0.6233],\n",
       "         [0.3872, 0.5849],\n",
       "         [0.2617, 0.7475],\n",
       "         [0.1638, 0.8424],\n",
       "         [0.1876, 0.8134],\n",
       "         [0.2168, 0.7897],\n",
       "         [0.2013, 0.8090],\n",
       "         [0.2658, 0.7427],\n",
       "         [0.2977, 0.7055],\n",
       "         [0.2539, 0.7412],\n",
       "         [0.1852, 0.8325],\n",
       "         [0.3806, 0.6104],\n",
       "         [0.3546, 0.6422],\n",
       "         [0.0697, 0.9309],\n",
       "         [0.1716, 0.8285],\n",
       "         [0.2972, 0.6965],\n",
       "         [0.2999, 0.7059],\n",
       "         [0.0984, 0.8928],\n",
       "         [0.1315, 0.8822],\n",
       "         [0.5640, 0.4438],\n",
       "         [0.2537, 0.7469],\n",
       "         [0.1196, 0.8808]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2437, 0.7508],\n",
       "         [0.3688, 0.6522],\n",
       "         [0.2762, 0.7303],\n",
       "         [0.3616, 0.6358],\n",
       "         [0.3373, 0.6526],\n",
       "         [0.1322, 0.8685],\n",
       "         [0.2494, 0.7357],\n",
       "         [0.4977, 0.4970],\n",
       "         [0.4336, 0.5355],\n",
       "         [0.3502, 0.6577],\n",
       "         [0.2139, 0.7927],\n",
       "         [0.2302, 0.7658],\n",
       "         [0.2854, 0.7207],\n",
       "         [0.3288, 0.6811],\n",
       "         [0.3404, 0.6671],\n",
       "         [0.5099, 0.4920],\n",
       "         [0.2576, 0.7377],\n",
       "         [0.2071, 0.8122],\n",
       "         [0.3954, 0.6032],\n",
       "         [0.7217, 0.2548],\n",
       "         [0.1824, 0.8122],\n",
       "         [0.4055, 0.5833],\n",
       "         [0.3112, 0.6795],\n",
       "         [0.3084, 0.7010],\n",
       "         [0.2234, 0.7432],\n",
       "         [0.2282, 0.7870],\n",
       "         [0.5252, 0.4856],\n",
       "         [0.5109, 0.4805],\n",
       "         [0.2541, 0.7406]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2299, 0.7648],\n",
       "         [0.4447, 0.5744],\n",
       "         [0.1757, 0.8315],\n",
       "         [0.3470, 0.6510],\n",
       "         [0.3140, 0.6952],\n",
       "         [0.2626, 0.7324],\n",
       "         [0.1793, 0.8115],\n",
       "         [0.3637, 0.6385],\n",
       "         [0.4790, 0.4886],\n",
       "         [0.4491, 0.5553],\n",
       "         [0.2277, 0.7789],\n",
       "         [0.1708, 0.8294],\n",
       "         [0.2479, 0.7591],\n",
       "         [0.3971, 0.6119],\n",
       "         [0.3249, 0.6828],\n",
       "         [0.5609, 0.4378],\n",
       "         [0.2196, 0.7773],\n",
       "         [0.2750, 0.7462],\n",
       "         [0.3978, 0.6009],\n",
       "         [0.6865, 0.2866],\n",
       "         [0.1902, 0.8032],\n",
       "         [0.3207, 0.6719],\n",
       "         [0.3319, 0.6546],\n",
       "         [0.3797, 0.6138],\n",
       "         [0.1192, 0.8698],\n",
       "         [0.1718, 0.8443],\n",
       "         [0.5706, 0.4387],\n",
       "         [0.4600, 0.5329],\n",
       "         [0.2188, 0.7769]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1246, 0.8759],\n",
       "         [0.4896, 0.5264],\n",
       "         [0.1555, 0.8520],\n",
       "         [0.1799, 0.8238],\n",
       "         [0.2690, 0.7250],\n",
       "         [0.0553, 0.9470],\n",
       "         [0.1653, 0.8266],\n",
       "         [0.3735, 0.6284],\n",
       "         [0.2702, 0.7095],\n",
       "         [0.2994, 0.7099],\n",
       "         [0.2027, 0.8039],\n",
       "         [0.2085, 0.7932],\n",
       "         [0.2607, 0.7461],\n",
       "         [0.2065, 0.8030],\n",
       "         [0.2966, 0.7128],\n",
       "         [0.3858, 0.6174],\n",
       "         [0.2201, 0.7772],\n",
       "         [0.2397, 0.7805],\n",
       "         [0.3915, 0.6008],\n",
       "         [0.4235, 0.5674],\n",
       "         [0.0553, 0.9463],\n",
       "         [0.3354, 0.6709],\n",
       "         [0.3584, 0.6346],\n",
       "         [0.4689, 0.5213],\n",
       "         [0.1100, 0.8802],\n",
       "         [0.1324, 0.8815],\n",
       "         [0.6001, 0.4054],\n",
       "         [0.3127, 0.6869],\n",
       "         [0.1663, 0.8324]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1217, 0.8796],\n",
       "         [0.4322, 0.5886],\n",
       "         [0.2662, 0.7416],\n",
       "         [0.4580, 0.5330],\n",
       "         [0.4119, 0.5770],\n",
       "         [0.1046, 0.8966],\n",
       "         [0.3296, 0.6475],\n",
       "         [0.5014, 0.4926],\n",
       "         [0.4417, 0.5305],\n",
       "         [0.4082, 0.5949],\n",
       "         [0.3232, 0.6816],\n",
       "         [0.2095, 0.7895],\n",
       "         [0.4266, 0.5737],\n",
       "         [0.2981, 0.7108],\n",
       "         [0.4514, 0.5523],\n",
       "         [0.5069, 0.4949],\n",
       "         [0.3050, 0.6877],\n",
       "         [0.2211, 0.7984],\n",
       "         [0.4651, 0.5306],\n",
       "         [0.6458, 0.3302],\n",
       "         [0.1224, 0.8757],\n",
       "         [0.2306, 0.7675],\n",
       "         [0.3098, 0.6831],\n",
       "         [0.3788, 0.6201],\n",
       "         [0.3310, 0.6198],\n",
       "         [0.1413, 0.8699],\n",
       "         [0.4945, 0.5182],\n",
       "         [0.6231, 0.3657],\n",
       "         [0.3048, 0.6866]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1855, 0.8158],\n",
       "         [0.4040, 0.6152],\n",
       "         [0.1845, 0.8231],\n",
       "         [0.3206, 0.6768],\n",
       "         [0.3133, 0.6947],\n",
       "         [0.1208, 0.8798],\n",
       "         [0.2001, 0.7878],\n",
       "         [0.4749, 0.5187],\n",
       "         [0.4328, 0.5367],\n",
       "         [0.3109, 0.6969],\n",
       "         [0.2357, 0.7703],\n",
       "         [0.2872, 0.6990],\n",
       "         [0.3316, 0.6723],\n",
       "         [0.3426, 0.6661],\n",
       "         [0.3156, 0.6927],\n",
       "         [0.6024, 0.3945],\n",
       "         [0.2337, 0.7623],\n",
       "         [0.2459, 0.7746],\n",
       "         [0.3916, 0.6033],\n",
       "         [0.3548, 0.6425],\n",
       "         [0.1138, 0.8846],\n",
       "         [0.2426, 0.7528],\n",
       "         [0.2890, 0.7089],\n",
       "         [0.3073, 0.6962],\n",
       "         [0.0851, 0.9073],\n",
       "         [0.1374, 0.8768],\n",
       "         [0.4969, 0.5130],\n",
       "         [0.4960, 0.4955],\n",
       "         [0.2102, 0.7863]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2043, 0.8014],\n",
       "         [0.4138, 0.6046],\n",
       "         [0.1657, 0.8422],\n",
       "         [0.1782, 0.8237],\n",
       "         [0.2329, 0.7739],\n",
       "         [0.0691, 0.9326],\n",
       "         [0.1712, 0.8179],\n",
       "         [0.4102, 0.5877],\n",
       "         [0.3465, 0.6260],\n",
       "         [0.2949, 0.7100],\n",
       "         [0.1814, 0.8243],\n",
       "         [0.2413, 0.7628],\n",
       "         [0.1296, 0.8758],\n",
       "         [0.2880, 0.7167],\n",
       "         [0.2022, 0.8071],\n",
       "         [0.4988, 0.5008],\n",
       "         [0.1444, 0.8550],\n",
       "         [0.1540, 0.8614],\n",
       "         [0.3061, 0.6816],\n",
       "         [0.2372, 0.7721],\n",
       "         [0.0764, 0.9237],\n",
       "         [0.2428, 0.7500],\n",
       "         [0.3212, 0.6695],\n",
       "         [0.2834, 0.7249],\n",
       "         [0.2036, 0.7636],\n",
       "         [0.0929, 0.9175],\n",
       "         [0.4371, 0.5736],\n",
       "         [0.4202, 0.5766],\n",
       "         [0.1606, 0.8380]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2092, 0.7879],\n",
       "         [0.6282, 0.3840],\n",
       "         [0.1593, 0.8476],\n",
       "         [0.3427, 0.6578],\n",
       "         [0.3227, 0.6861],\n",
       "         [0.1049, 0.8974],\n",
       "         [0.1957, 0.7959],\n",
       "         [0.3551, 0.6470],\n",
       "         [0.2566, 0.7240],\n",
       "         [0.2959, 0.7148],\n",
       "         [0.2477, 0.7592],\n",
       "         [0.2120, 0.7810],\n",
       "         [0.2514, 0.7558],\n",
       "         [0.2801, 0.7304],\n",
       "         [0.3235, 0.6842],\n",
       "         [0.2960, 0.7072],\n",
       "         [0.2147, 0.7825],\n",
       "         [0.1425, 0.8726],\n",
       "         [0.3476, 0.6630],\n",
       "         [0.5273, 0.4562],\n",
       "         [0.1611, 0.8362],\n",
       "         [0.3759, 0.6316],\n",
       "         [0.4014, 0.5947],\n",
       "         [0.6060, 0.3694],\n",
       "         [0.0790, 0.9163],\n",
       "         [0.1497, 0.8656],\n",
       "         [0.6927, 0.3122],\n",
       "         [0.3265, 0.6713],\n",
       "         [0.1521, 0.8475]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2188, 0.7945],\n",
       "         [0.2412, 0.7782],\n",
       "         [0.2219, 0.7833],\n",
       "         [0.4447, 0.5491],\n",
       "         [0.3220, 0.6837],\n",
       "         [0.2535, 0.7417],\n",
       "         [0.1995, 0.7895],\n",
       "         [0.4069, 0.5924],\n",
       "         [0.4499, 0.5203],\n",
       "         [0.2886, 0.7195],\n",
       "         [0.3540, 0.6493],\n",
       "         [0.2993, 0.6989],\n",
       "         [0.3856, 0.6160],\n",
       "         [0.4019, 0.6073],\n",
       "         [0.3871, 0.6180],\n",
       "         [0.0936, 0.9095],\n",
       "         [0.3425, 0.6490],\n",
       "         [0.0891, 0.9214],\n",
       "         [0.4592, 0.5472],\n",
       "         [0.3114, 0.6873],\n",
       "         [0.1958, 0.7989],\n",
       "         [0.4217, 0.5818],\n",
       "         [0.1563, 0.8463],\n",
       "         [0.1330, 0.8783],\n",
       "         [0.5287, 0.4349],\n",
       "         [0.2208, 0.7965],\n",
       "         [0.4141, 0.5990],\n",
       "         [0.4622, 0.5294],\n",
       "         [0.3426, 0.6483]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.0969, 0.9036],\n",
       "         [0.5104, 0.5002],\n",
       "         [0.1832, 0.8247],\n",
       "         [0.2049, 0.7974],\n",
       "         [0.2210, 0.7855],\n",
       "         [0.1185, 0.8818],\n",
       "         [0.1843, 0.8044],\n",
       "         [0.3668, 0.6111],\n",
       "         [0.3914, 0.5797],\n",
       "         [0.2856, 0.7191],\n",
       "         [0.2848, 0.7194],\n",
       "         [0.2598, 0.7422],\n",
       "         [0.2457, 0.7587],\n",
       "         [0.1969, 0.8098],\n",
       "         [0.2299, 0.7811],\n",
       "         [0.3824, 0.6226],\n",
       "         [0.1828, 0.8148],\n",
       "         [0.3471, 0.6740],\n",
       "         [0.4033, 0.5770],\n",
       "         [0.1916, 0.8220],\n",
       "         [0.1118, 0.8872],\n",
       "         [0.2345, 0.7666],\n",
       "         [0.5239, 0.4301],\n",
       "         [0.4206, 0.5783],\n",
       "         [0.1584, 0.8215],\n",
       "         [0.1104, 0.9016],\n",
       "         [0.4676, 0.5362],\n",
       "         [0.4099, 0.5875],\n",
       "         [0.1711, 0.8295]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2864, 0.7094],\n",
       "         [0.3864, 0.6292],\n",
       "         [0.1703, 0.8329],\n",
       "         [0.3036, 0.6975],\n",
       "         [0.4598, 0.5399],\n",
       "         [0.0741, 0.9284],\n",
       "         [0.2618, 0.7278],\n",
       "         [0.4509, 0.5470],\n",
       "         [0.3011, 0.6828],\n",
       "         [0.3019, 0.7075],\n",
       "         [0.4586, 0.5406],\n",
       "         [0.3500, 0.6457],\n",
       "         [0.2659, 0.7375],\n",
       "         [0.3731, 0.6427],\n",
       "         [0.3700, 0.6316],\n",
       "         [0.1384, 0.8614],\n",
       "         [0.3333, 0.6603],\n",
       "         [0.0407, 0.9659],\n",
       "         [0.4203, 0.5907],\n",
       "         [0.8512, 0.1358],\n",
       "         [0.1614, 0.8377],\n",
       "         [0.6371, 0.3683],\n",
       "         [0.2666, 0.7286],\n",
       "         [0.3370, 0.6734],\n",
       "         [0.1713, 0.8193],\n",
       "         [0.2296, 0.7881],\n",
       "         [0.5769, 0.4292],\n",
       "         [0.3107, 0.6848],\n",
       "         [0.1913, 0.8062]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3082, 0.6820],\n",
       "         [0.6647, 0.3449],\n",
       "         [0.3686, 0.6362],\n",
       "         [0.3327, 0.6691],\n",
       "         [0.4540, 0.5482],\n",
       "         [0.1577, 0.8432],\n",
       "         [0.4477, 0.5273],\n",
       "         [0.4971, 0.4970],\n",
       "         [0.2880, 0.6995],\n",
       "         [0.4493, 0.5547],\n",
       "         [0.4701, 0.5323],\n",
       "         [0.2568, 0.7365],\n",
       "         [0.5352, 0.4641],\n",
       "         [0.4179, 0.5877],\n",
       "         [0.5501, 0.4506],\n",
       "         [0.2213, 0.7890],\n",
       "         [0.3962, 0.5926],\n",
       "         [0.3514, 0.6714],\n",
       "         [0.4938, 0.5113],\n",
       "         [0.7202, 0.2553],\n",
       "         [0.2731, 0.7170],\n",
       "         [0.3592, 0.6256],\n",
       "         [0.5857, 0.3855],\n",
       "         [0.8538, 0.1281],\n",
       "         [0.1786, 0.8016],\n",
       "         [0.2584, 0.7595],\n",
       "         [0.6933, 0.3096],\n",
       "         [0.6262, 0.3629],\n",
       "         [0.3806, 0.6101]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1683, 0.8301],\n",
       "         [0.3060, 0.7158],\n",
       "         [0.2111, 0.7965],\n",
       "         [0.3648, 0.6311],\n",
       "         [0.3568, 0.6494],\n",
       "         [0.1187, 0.8822],\n",
       "         [0.2119, 0.7749],\n",
       "         [0.3758, 0.6091],\n",
       "         [0.2810, 0.7076],\n",
       "         [0.3676, 0.6385],\n",
       "         [0.3292, 0.6757],\n",
       "         [0.3599, 0.6189],\n",
       "         [0.3253, 0.6786],\n",
       "         [0.3291, 0.6809],\n",
       "         [0.3456, 0.6618],\n",
       "         [0.2621, 0.7417],\n",
       "         [0.2371, 0.7591],\n",
       "         [0.2317, 0.7895],\n",
       "         [0.4354, 0.5693],\n",
       "         [0.6896, 0.2846],\n",
       "         [0.1167, 0.8821],\n",
       "         [0.2527, 0.7398],\n",
       "         [0.1814, 0.8230],\n",
       "         [0.2753, 0.7289],\n",
       "         [0.3387, 0.6123],\n",
       "         [0.1607, 0.8545],\n",
       "         [0.4241, 0.5907],\n",
       "         [0.4099, 0.5854],\n",
       "         [0.2352, 0.7600]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1266, 0.8751],\n",
       "         [0.3948, 0.6237],\n",
       "         [0.1854, 0.8216],\n",
       "         [0.2540, 0.7471],\n",
       "         [0.4919, 0.5071],\n",
       "         [0.0542, 0.9480],\n",
       "         [0.1221, 0.8723],\n",
       "         [0.3775, 0.6239],\n",
       "         [0.3518, 0.6236],\n",
       "         [0.3660, 0.6418],\n",
       "         [0.2190, 0.7871],\n",
       "         [0.3073, 0.6769],\n",
       "         [0.1936, 0.8133],\n",
       "         [0.2298, 0.7822],\n",
       "         [0.2438, 0.7659],\n",
       "         [0.4408, 0.5573],\n",
       "         [0.1782, 0.8201],\n",
       "         [0.2109, 0.8091],\n",
       "         [0.3506, 0.6563],\n",
       "         [0.2261, 0.7785],\n",
       "         [0.0491, 0.9522],\n",
       "         [0.3177, 0.6746],\n",
       "         [0.3083, 0.6855],\n",
       "         [0.3316, 0.6729],\n",
       "         [0.2694, 0.6909],\n",
       "         [0.0943, 0.9153],\n",
       "         [0.5778, 0.4282],\n",
       "         [0.2973, 0.7014],\n",
       "         [0.1530, 0.8458]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1402, 0.8592],\n",
       "         [0.4797, 0.5362],\n",
       "         [0.2982, 0.7086],\n",
       "         [0.3663, 0.6299],\n",
       "         [0.4592, 0.5421],\n",
       "         [0.1306, 0.8697],\n",
       "         [0.3010, 0.6798],\n",
       "         [0.5166, 0.4755],\n",
       "         [0.4190, 0.5510],\n",
       "         [0.4760, 0.5251],\n",
       "         [0.3089, 0.6960],\n",
       "         [0.3641, 0.6181],\n",
       "         [0.4351, 0.5662],\n",
       "         [0.3752, 0.6315],\n",
       "         [0.3438, 0.6640],\n",
       "         [0.4586, 0.5421],\n",
       "         [0.2936, 0.6997],\n",
       "         [0.2303, 0.7893],\n",
       "         [0.4603, 0.5231],\n",
       "         [0.3470, 0.6513],\n",
       "         [0.1110, 0.8878],\n",
       "         [0.2835, 0.7098],\n",
       "         [0.3580, 0.6341],\n",
       "         [0.3867, 0.6200],\n",
       "         [0.1522, 0.8304],\n",
       "         [0.1262, 0.8854],\n",
       "         [0.5260, 0.4812],\n",
       "         [0.5489, 0.4416],\n",
       "         [0.2802, 0.7142]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1995, 0.7989],\n",
       "         [0.4000, 0.6220],\n",
       "         [0.2379, 0.7703],\n",
       "         [0.3080, 0.6897],\n",
       "         [0.3619, 0.6245],\n",
       "         [0.2346, 0.7610],\n",
       "         [0.3521, 0.6231],\n",
       "         [0.4660, 0.5299],\n",
       "         [0.3454, 0.6344],\n",
       "         [0.3752, 0.6274],\n",
       "         [0.3266, 0.6779],\n",
       "         [0.2540, 0.7441],\n",
       "         [0.4504, 0.5486],\n",
       "         [0.3525, 0.6527],\n",
       "         [0.4094, 0.5966],\n",
       "         [0.4545, 0.5473],\n",
       "         [0.3369, 0.6543],\n",
       "         [0.1815, 0.8356],\n",
       "         [0.4387, 0.5494],\n",
       "         [0.4835, 0.5036],\n",
       "         [0.1976, 0.7953],\n",
       "         [0.1073, 0.8979],\n",
       "         [0.2779, 0.7187],\n",
       "         [0.3107, 0.6930],\n",
       "         [0.3815, 0.5646],\n",
       "         [0.1722, 0.8428],\n",
       "         [0.4205, 0.5923],\n",
       "         [0.5444, 0.4474],\n",
       "         [0.2854, 0.7086]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1371, 0.8623],\n",
       "         [0.3273, 0.6961],\n",
       "         [0.1869, 0.8208],\n",
       "         [0.3274, 0.6687],\n",
       "         [0.3504, 0.6559],\n",
       "         [0.1421, 0.8576],\n",
       "         [0.2346, 0.7499],\n",
       "         [0.4448, 0.5514],\n",
       "         [0.3843, 0.5932],\n",
       "         [0.3832, 0.6218],\n",
       "         [0.2467, 0.7592],\n",
       "         [0.1908, 0.8071],\n",
       "         [0.4505, 0.5496],\n",
       "         [0.3669, 0.6405],\n",
       "         [0.2816, 0.7271],\n",
       "         [0.3187, 0.6834],\n",
       "         [0.2591, 0.7359],\n",
       "         [0.1686, 0.8480],\n",
       "         [0.3533, 0.6342],\n",
       "         [0.6763, 0.2967],\n",
       "         [0.1184, 0.8798],\n",
       "         [0.1514, 0.8521],\n",
       "         [0.2531, 0.7439],\n",
       "         [0.2323, 0.7787],\n",
       "         [0.1200, 0.8670],\n",
       "         [0.1568, 0.8581],\n",
       "         [0.4336, 0.5808],\n",
       "         [0.6639, 0.3232],\n",
       "         [0.2161, 0.7800]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1785, 0.8188],\n",
       "         [0.4609, 0.5552],\n",
       "         [0.2907, 0.7162],\n",
       "         [0.3024, 0.6968],\n",
       "         [0.3986, 0.5858],\n",
       "         [0.2934, 0.6986],\n",
       "         [0.3111, 0.6686],\n",
       "         [0.4494, 0.5475],\n",
       "         [0.4115, 0.5598],\n",
       "         [0.4141, 0.5875],\n",
       "         [0.3520, 0.6514],\n",
       "         [0.2310, 0.7710],\n",
       "         [0.3796, 0.6223],\n",
       "         [0.3617, 0.6440],\n",
       "         [0.3931, 0.6146],\n",
       "         [0.5244, 0.4751],\n",
       "         [0.3125, 0.6797],\n",
       "         [0.3125, 0.7086],\n",
       "         [0.4117, 0.5697],\n",
       "         [0.4375, 0.5520],\n",
       "         [0.2541, 0.7351],\n",
       "         [0.2536, 0.7521],\n",
       "         [0.3585, 0.6248],\n",
       "         [0.4799, 0.5070],\n",
       "         [0.2046, 0.7694],\n",
       "         [0.1883, 0.8279],\n",
       "         [0.4639, 0.5445],\n",
       "         [0.5254, 0.4664],\n",
       "         [0.2869, 0.7076]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2274, 0.7778],\n",
       "         [0.3465, 0.6750],\n",
       "         [0.2634, 0.7448],\n",
       "         [0.5623, 0.4260],\n",
       "         [0.4796, 0.5023],\n",
       "         [0.2162, 0.7801],\n",
       "         [0.2254, 0.7586],\n",
       "         [0.5187, 0.4733],\n",
       "         [0.4821, 0.4842],\n",
       "         [0.4510, 0.5472],\n",
       "         [0.2488, 0.7565],\n",
       "         [0.2734, 0.7144],\n",
       "         [0.3836, 0.6169],\n",
       "         [0.3682, 0.6366],\n",
       "         [0.3522, 0.6563],\n",
       "         [0.5068, 0.4903],\n",
       "         [0.1859, 0.8121],\n",
       "         [0.2557, 0.7649],\n",
       "         [0.4052, 0.5825],\n",
       "         [0.3618, 0.6362],\n",
       "         [0.1324, 0.8653],\n",
       "         [0.3241, 0.6689],\n",
       "         [0.2224, 0.7779],\n",
       "         [0.2452, 0.7647],\n",
       "         [0.3237, 0.6288],\n",
       "         [0.0881, 0.9194],\n",
       "         [0.3640, 0.6489],\n",
       "         [0.5809, 0.4099],\n",
       "         [0.2221, 0.7750]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1815, 0.8176],\n",
       "         [0.3877, 0.6331],\n",
       "         [0.2598, 0.7477],\n",
       "         [0.2924, 0.7065],\n",
       "         [0.3564, 0.6323],\n",
       "         [0.1582, 0.8408],\n",
       "         [0.2912, 0.6879],\n",
       "         [0.4678, 0.5278],\n",
       "         [0.4341, 0.5405],\n",
       "         [0.4056, 0.5953],\n",
       "         [0.3374, 0.6663],\n",
       "         [0.3254, 0.6570],\n",
       "         [0.4620, 0.5361],\n",
       "         [0.5299, 0.4703],\n",
       "         [0.4046, 0.6025],\n",
       "         [0.4831, 0.5149],\n",
       "         [0.3366, 0.6543],\n",
       "         [0.2132, 0.8062],\n",
       "         [0.4948, 0.4940],\n",
       "         [0.2447, 0.7623],\n",
       "         [0.1574, 0.8392],\n",
       "         [0.1837, 0.8208],\n",
       "         [0.3507, 0.6269],\n",
       "         [0.4338, 0.5585],\n",
       "         [0.3823, 0.5634],\n",
       "         [0.1748, 0.8410],\n",
       "         [0.4007, 0.6131],\n",
       "         [0.5666, 0.4237],\n",
       "         [0.3655, 0.6245]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2223, 0.7767],\n",
       "         [0.4294, 0.5894],\n",
       "         [0.3247, 0.6816],\n",
       "         [0.3287, 0.6703],\n",
       "         [0.4436, 0.5572],\n",
       "         [0.2397, 0.7565],\n",
       "         [0.3179, 0.6614],\n",
       "         [0.4308, 0.5673],\n",
       "         [0.4590, 0.5084],\n",
       "         [0.4179, 0.5846],\n",
       "         [0.3958, 0.6075],\n",
       "         [0.3369, 0.6426],\n",
       "         [0.4633, 0.5370],\n",
       "         [0.3558, 0.6509],\n",
       "         [0.4174, 0.5886],\n",
       "         [0.4585, 0.5431],\n",
       "         [0.3373, 0.6545],\n",
       "         [0.2734, 0.7481],\n",
       "         [0.4584, 0.5285],\n",
       "         [0.3196, 0.6819],\n",
       "         [0.2325, 0.7596],\n",
       "         [0.3250, 0.6660],\n",
       "         [0.2996, 0.6907],\n",
       "         [0.3427, 0.6578],\n",
       "         [0.4249, 0.5186],\n",
       "         [0.2164, 0.8008],\n",
       "         [0.4832, 0.5275],\n",
       "         [0.5097, 0.4827],\n",
       "         [0.3047, 0.6893]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.2124, 0.7847],\n",
       "         [0.4991, 0.5196],\n",
       "         [0.1887, 0.8174],\n",
       "         [0.2495, 0.7542],\n",
       "         [0.3217, 0.6873],\n",
       "         [0.0515, 0.9511],\n",
       "         [0.2239, 0.7664],\n",
       "         [0.3928, 0.6079],\n",
       "         [0.2983, 0.6808],\n",
       "         [0.2762, 0.7348],\n",
       "         [0.3286, 0.6778],\n",
       "         [0.3391, 0.6400],\n",
       "         [0.2680, 0.7389],\n",
       "         [0.3373, 0.6755],\n",
       "         [0.3539, 0.6514],\n",
       "         [0.3627, 0.6418],\n",
       "         [0.2768, 0.7178],\n",
       "         [0.1505, 0.8658],\n",
       "         [0.3865, 0.6242],\n",
       "         [0.6931, 0.2806],\n",
       "         [0.0891, 0.9115],\n",
       "         [0.4115, 0.5970],\n",
       "         [0.3859, 0.5945],\n",
       "         [0.3301, 0.6716],\n",
       "         [0.1344, 0.8556],\n",
       "         [0.1844, 0.8331],\n",
       "         [0.6883, 0.3174],\n",
       "         [0.3400, 0.6568],\n",
       "         [0.1687, 0.8300]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3082, 0.6841],\n",
       "         [0.4979, 0.5178],\n",
       "         [0.4776, 0.5212],\n",
       "         [0.6129, 0.3773],\n",
       "         [0.4948, 0.5041],\n",
       "         [0.3710, 0.6186],\n",
       "         [0.4904, 0.4839],\n",
       "         [0.4967, 0.4971],\n",
       "         [0.4682, 0.5025],\n",
       "         [0.4830, 0.5182],\n",
       "         [0.6229, 0.3753],\n",
       "         [0.4390, 0.5556],\n",
       "         [0.5503, 0.4461],\n",
       "         [0.6368, 0.3642],\n",
       "         [0.5595, 0.4393],\n",
       "         [0.2819, 0.7232],\n",
       "         [0.5177, 0.4669],\n",
       "         [0.1148, 0.8983],\n",
       "         [0.4037, 0.5803],\n",
       "         [0.4778, 0.5108],\n",
       "         [0.4044, 0.5779],\n",
       "         [0.1793, 0.8247],\n",
       "         [0.3813, 0.5995],\n",
       "         [0.2949, 0.7164],\n",
       "         [0.3699, 0.5892],\n",
       "         [0.3484, 0.6695],\n",
       "         [0.5707, 0.4355],\n",
       "         [0.5350, 0.4548],\n",
       "         [0.4175, 0.5718]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[1.5575e-03, 9.9886e-01],\n",
       "         [5.8229e-01, 4.6974e-01],\n",
       "         [5.9510e-01, 4.4410e-01],\n",
       "         [6.1463e-01, 4.2634e-01],\n",
       "         [5.8736e-01, 4.6545e-01],\n",
       "         [4.0834e-01, 5.9043e-01],\n",
       "         [3.2879e-03, 9.9739e-01],\n",
       "         [5.9671e-01, 3.9455e-01],\n",
       "         [9.9527e-01, 3.9483e-03],\n",
       "         [9.9371e-01, 6.2143e-03],\n",
       "         [1.7438e-03, 9.9856e-01],\n",
       "         [1.4061e-03, 9.9889e-01],\n",
       "         [5.1759e-01, 5.1445e-01],\n",
       "         [9.8895e-01, 1.1139e-02],\n",
       "         [2.5811e-04, 9.9982e-01],\n",
       "         [5.5188e-01, 5.1595e-01],\n",
       "         [2.0406e-03, 9.9846e-01],\n",
       "         [1.0444e-03, 9.9912e-01],\n",
       "         [2.8222e-03, 9.9790e-01],\n",
       "         [1.8573e-03, 9.9845e-01],\n",
       "         [4.9089e-01, 5.0655e-01],\n",
       "         [5.4536e-01, 4.9134e-01],\n",
       "         [9.9109e-01, 9.1838e-03],\n",
       "         [4.8503e-01, 5.2773e-01],\n",
       "         [1.2320e-03, 9.9906e-01],\n",
       "         [5.7667e-01, 4.8762e-01],\n",
       "         [9.9904e-01, 9.1403e-04],\n",
       "         [5.0624e-01, 4.9876e-01],\n",
       "         [7.4980e-04, 9.9936e-01]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1371, 0.8642],\n",
       "         [0.3938, 0.6210],\n",
       "         [0.0581, 0.9462],\n",
       "         [0.1414, 0.8636],\n",
       "         [0.1754, 0.8351],\n",
       "         [0.0238, 0.9781],\n",
       "         [0.0870, 0.9123],\n",
       "         [0.3446, 0.6580],\n",
       "         [0.2152, 0.7708],\n",
       "         [0.1619, 0.8456],\n",
       "         [0.1929, 0.8140],\n",
       "         [0.1300, 0.8799],\n",
       "         [0.1438, 0.8630],\n",
       "         [0.1725, 0.8402],\n",
       "         [0.2229, 0.7851],\n",
       "         [0.0436, 0.9596],\n",
       "         [0.1441, 0.8566],\n",
       "         [0.0928, 0.9190],\n",
       "         [0.2086, 0.8023],\n",
       "         [0.8204, 0.1632],\n",
       "         [0.0561, 0.9456],\n",
       "         [0.3851, 0.6231],\n",
       "         [0.2760, 0.7145],\n",
       "         [0.3023, 0.7044],\n",
       "         [0.0807, 0.9166],\n",
       "         [0.1296, 0.8834],\n",
       "         [0.6111, 0.3913],\n",
       "         [0.2168, 0.7841],\n",
       "         [0.1100, 0.8912]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1132, 0.8891],\n",
       "         [0.3793, 0.6358],\n",
       "         [0.1216, 0.8838],\n",
       "         [0.1317, 0.8733],\n",
       "         [0.1821, 0.8285],\n",
       "         [0.0354, 0.9668],\n",
       "         [0.0812, 0.9183],\n",
       "         [0.3474, 0.6552],\n",
       "         [0.2159, 0.7700],\n",
       "         [0.1967, 0.8121],\n",
       "         [0.1479, 0.8588],\n",
       "         [0.1368, 0.8738],\n",
       "         [0.1369, 0.8697],\n",
       "         [0.1856, 0.8275],\n",
       "         [0.1672, 0.8415],\n",
       "         [0.0574, 0.9460],\n",
       "         [0.1120, 0.8894],\n",
       "         [0.0841, 0.9270],\n",
       "         [0.2211, 0.7902],\n",
       "         [0.8058, 0.1765],\n",
       "         [0.0468, 0.9549],\n",
       "         [0.3918, 0.6161],\n",
       "         [0.2604, 0.7314],\n",
       "         [0.3102, 0.6960],\n",
       "         [0.0549, 0.9443],\n",
       "         [0.1451, 0.8688],\n",
       "         [0.5895, 0.4138],\n",
       "         [0.1818, 0.8195],\n",
       "         [0.0764, 0.9255]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1355, 0.8640],\n",
       "         [0.3940, 0.6227],\n",
       "         [0.1977, 0.8109],\n",
       "         [0.2239, 0.7776],\n",
       "         [0.6423, 0.3235],\n",
       "         [0.1536, 0.8458],\n",
       "         [0.2253, 0.7588],\n",
       "         [0.4186, 0.5802],\n",
       "         [0.2431, 0.7560],\n",
       "         [0.3340, 0.6653],\n",
       "         [0.2023, 0.8023],\n",
       "         [0.2411, 0.7621],\n",
       "         [0.3110, 0.6900],\n",
       "         [0.1653, 0.8409],\n",
       "         [0.3162, 0.6977],\n",
       "         [0.3861, 0.6204],\n",
       "         [0.2135, 0.7829],\n",
       "         [0.2545, 0.7655],\n",
       "         [0.5069, 0.4716],\n",
       "         [0.2576, 0.7505],\n",
       "         [0.1219, 0.8779],\n",
       "         [0.2582, 0.7432],\n",
       "         [0.4130, 0.5501],\n",
       "         [0.3296, 0.6745],\n",
       "         [0.1493, 0.8311],\n",
       "         [0.1188, 0.8941],\n",
       "         [0.3890, 0.6191],\n",
       "         [0.5409, 0.4526],\n",
       "         [0.2107, 0.7900]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.1816, 0.8205],\n",
       "         [0.3410, 0.6797],\n",
       "         [0.1387, 0.8688],\n",
       "         [0.1945, 0.8070],\n",
       "         [0.3617, 0.6169],\n",
       "         [0.0576, 0.9444],\n",
       "         [0.2179, 0.7665],\n",
       "         [0.4149, 0.5841],\n",
       "         [0.2632, 0.7302],\n",
       "         [0.3974, 0.6017],\n",
       "         [0.1863, 0.8188],\n",
       "         [0.3237, 0.6614],\n",
       "         [0.3123, 0.6893],\n",
       "         [0.2095, 0.7976],\n",
       "         [0.2567, 0.7544],\n",
       "         [0.4103, 0.5961],\n",
       "         [0.2401, 0.7555],\n",
       "         [0.1853, 0.8325],\n",
       "         [0.3358, 0.6488],\n",
       "         [0.1754, 0.8375],\n",
       "         [0.0679, 0.9333],\n",
       "         [0.3005, 0.7022],\n",
       "         [0.2750, 0.7175],\n",
       "         [0.3773, 0.6200],\n",
       "         [0.1961, 0.7712],\n",
       "         [0.1229, 0.8901],\n",
       "         [0.3776, 0.6350],\n",
       "         [0.4043, 0.5930],\n",
       "         [0.1794, 0.8198]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.0825, 0.9186],\n",
       "         [0.3999, 0.6189],\n",
       "         [0.0681, 0.9370],\n",
       "         [0.0761, 0.9279],\n",
       "         [0.5036, 0.4628],\n",
       "         [0.0167, 0.9846],\n",
       "         [0.1236, 0.8691],\n",
       "         [0.3204, 0.6835],\n",
       "         [0.2433, 0.7481],\n",
       "         [0.2592, 0.7471],\n",
       "         [0.1586, 0.8469],\n",
       "         [0.2117, 0.7943],\n",
       "         [0.1865, 0.8183],\n",
       "         [0.1748, 0.8346],\n",
       "         [0.2348, 0.7756],\n",
       "         [0.1692, 0.8369],\n",
       "         [0.1772, 0.8208],\n",
       "         [0.2916, 0.7302],\n",
       "         [0.3415, 0.6519],\n",
       "         [0.3268, 0.6709],\n",
       "         [0.0547, 0.9463],\n",
       "         [0.2667, 0.7255],\n",
       "         [0.3003, 0.6868],\n",
       "         [0.3078, 0.6960],\n",
       "         [0.1020, 0.8871],\n",
       "         [0.0533, 0.9523],\n",
       "         [0.4258, 0.5851],\n",
       "         [0.4015, 0.5946],\n",
       "         [0.1632, 0.8352]], grad_fn=<SigmoidBackward>)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_l = [list(np.squeeze(i.detach().numpy())) for i in y_hat_l]\n",
    "y_hat_l = [z for y in y_hat_l for z in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_l = [y[1] for y in y_hat_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7510622,\n",
       " 0.36967477,\n",
       " 0.80878544,\n",
       " 0.71799827,\n",
       " 0.6551511,\n",
       " 0.905933,\n",
       " 0.69224125,\n",
       " 0.5805447,\n",
       " 0.6720963,\n",
       " 0.6664721,\n",
       " 0.6772482,\n",
       " 0.6879442,\n",
       " 0.6654967,\n",
       " 0.6595472,\n",
       " 0.6022927,\n",
       " 0.7794788,\n",
       " 0.6725395,\n",
       " 0.81850195,\n",
       " 0.5928065,\n",
       " 0.81586474,\n",
       " 0.8772021,\n",
       " 0.5521879,\n",
       " 0.41796514,\n",
       " 0.1336909,\n",
       " 0.9063242,\n",
       " 0.82005376,\n",
       " 0.21621192,\n",
       " 0.61428064,\n",
       " 0.8344264,\n",
       " 0.8454016,\n",
       " 0.45681918,\n",
       " 0.92868906,\n",
       " 0.83400255,\n",
       " 0.83235365,\n",
       " 0.9825515,\n",
       " 0.9306407,\n",
       " 0.6481995,\n",
       " 0.7520678,\n",
       " 0.83850485,\n",
       " 0.8249852,\n",
       " 0.89410543,\n",
       " 0.8994224,\n",
       " 0.8210432,\n",
       " 0.8170324,\n",
       " 0.95012295,\n",
       " 0.882835,\n",
       " 0.9221026,\n",
       " 0.83387864,\n",
       " 0.15474838,\n",
       " 0.97092664,\n",
       " 0.61608803,\n",
       " 0.48040196,\n",
       " 0.5872975,\n",
       " 0.9507781,\n",
       " 0.877131,\n",
       " 0.32451764,\n",
       " 0.79722583,\n",
       " 0.9030667,\n",
       " 0.6679942,\n",
       " 0.59206545,\n",
       " 0.5807945,\n",
       " 0.52944535,\n",
       " 0.3348449,\n",
       " 0.55904347,\n",
       " 0.5457959,\n",
       " 0.4610916,\n",
       " 0.44583404,\n",
       " 0.41582236,\n",
       " 0.5098483,\n",
       " 0.62035257,\n",
       " 0.42747405,\n",
       " 0.5328888,\n",
       " 0.44028848,\n",
       " 0.39611262,\n",
       " 0.49759465,\n",
       " 0.7437328,\n",
       " 0.50086534,\n",
       " 0.56331646,\n",
       " 0.65032375,\n",
       " 0.71813536,\n",
       " 0.66635454,\n",
       " 0.57453644,\n",
       " 0.484931,\n",
       " 0.74520355,\n",
       " 0.55532867,\n",
       " 0.45250583,\n",
       " 0.58116394,\n",
       " 0.7052424,\n",
       " 0.42891777,\n",
       " 0.5563387,\n",
       " 0.53298575,\n",
       " 0.38460213,\n",
       " 0.5091742,\n",
       " 0.4624195,\n",
       " 0.55149585,\n",
       " 0.5539227,\n",
       " 0.4812391,\n",
       " 0.47498402,\n",
       " 0.61776406,\n",
       " 0.2868543,\n",
       " 0.29016864,\n",
       " 0.5019553,\n",
       " 0.7299238,\n",
       " 0.5822616,\n",
       " 0.7218812,\n",
       " 0.47171432,\n",
       " 0.7634841,\n",
       " 0.65399927,\n",
       " 0.74455607,\n",
       " 0.47398707,\n",
       " 0.47633293,\n",
       " 0.7403722,\n",
       " 0.7904103,\n",
       " 0.51749974,\n",
       " 0.42362317,\n",
       " 0.59068346,\n",
       " 0.746111,\n",
       " 0.47605613,\n",
       " 0.6347805,\n",
       " 0.3465403,\n",
       " 0.49215102,\n",
       " 0.75761974,\n",
       " 0.6323972,\n",
       " 0.5690416,\n",
       " 0.5431643,\n",
       " 0.52151215,\n",
       " 0.5941577,\n",
       " 0.62169486,\n",
       " 0.657346,\n",
       " 0.567743,\n",
       " 0.5115955,\n",
       " 0.60032254,\n",
       " 0.6246958,\n",
       " 0.6780597,\n",
       " 0.50356185,\n",
       " 0.7705749,\n",
       " 0.7856082,\n",
       " 0.5257687,\n",
       " 0.4553179,\n",
       " 0.4198183,\n",
       " 0.5211576,\n",
       " 0.8557558,\n",
       " 0.43451405,\n",
       " 0.34794518,\n",
       " 0.6995036,\n",
       " 0.90602285,\n",
       " 0.5267737,\n",
       " 0.80774367,\n",
       " 0.8237492,\n",
       " 0.5804293,\n",
       " 0.9758883,\n",
       " 0.8177671,\n",
       " 0.54933184,\n",
       " 0.6488154,\n",
       " 0.6993477,\n",
       " 0.77920717,\n",
       " 0.78794384,\n",
       " 0.68431604,\n",
       " 0.6911037,\n",
       " 0.68137264,\n",
       " 0.605942,\n",
       " 0.74652445,\n",
       " 0.7151478,\n",
       " 0.57194525,\n",
       " 0.56354827,\n",
       " 0.9727222,\n",
       " 0.7840642,\n",
       " 0.51438564,\n",
       " 0.4152616,\n",
       " 0.9539657,\n",
       " 0.8894129,\n",
       " 0.34808177,\n",
       " 0.6430607,\n",
       " 0.8166491,\n",
       " 0.850837,\n",
       " 0.5765215,\n",
       " 0.6091394,\n",
       " 0.61065525,\n",
       " 0.58840543,\n",
       " 0.69060653,\n",
       " 0.79200745,\n",
       " 0.52496076,\n",
       " 0.62735707,\n",
       " 0.5675069,\n",
       " 0.6865121,\n",
       " 0.6814137,\n",
       " 0.51363015,\n",
       " 0.64463687,\n",
       " 0.6394217,\n",
       " 0.6246149,\n",
       " 0.6504585,\n",
       " 0.7293371,\n",
       " 0.60600775,\n",
       " 0.64292413,\n",
       " 0.7899141,\n",
       " 0.68641335,\n",
       " 0.71567637,\n",
       " 0.5271394,\n",
       " 0.638953,\n",
       " 0.81410795,\n",
       " 0.6155975,\n",
       " 0.4802287,\n",
       " 0.78148246,\n",
       " 0.8342543,\n",
       " 0.5225585,\n",
       " 0.731514,\n",
       " 0.71909225,\n",
       " 0.7207788,\n",
       " 0.9379171,\n",
       " 0.7221169,\n",
       " 0.57259506,\n",
       " 0.5479143,\n",
       " 0.6777224,\n",
       " 0.6060596,\n",
       " 0.7173531,\n",
       " 0.71637416,\n",
       " 0.73610216,\n",
       " 0.6607585,\n",
       " 0.4414321,\n",
       " 0.7523104,\n",
       " 0.81263644,\n",
       " 0.6031286,\n",
       " 0.7196742,\n",
       " 0.8458053,\n",
       " 0.77002794,\n",
       " 0.58673877,\n",
       " 0.69898474,\n",
       " 0.76426834,\n",
       " 0.83379906,\n",
       " 0.4933066,\n",
       " 0.33188742,\n",
       " 0.81918454,\n",
       " 0.87039196,\n",
       " 0.6262186,\n",
       " 0.8199466,\n",
       " 0.7798969,\n",
       " 0.7029962,\n",
       " 0.7207031,\n",
       " 0.7758513,\n",
       " 0.5852089,\n",
       " 0.61536026,\n",
       " 0.68608415,\n",
       " 0.76648784,\n",
       " 0.8022494,\n",
       " 0.7388163,\n",
       " 0.7876747,\n",
       " 0.70415044,\n",
       " 0.49466825,\n",
       " 0.7511561,\n",
       " 0.8021295,\n",
       " 0.53130805,\n",
       " 0.65863734,\n",
       " 0.84741503,\n",
       " 0.7901355,\n",
       " 0.63800436,\n",
       " 0.7384334,\n",
       " 0.8170429,\n",
       " 0.874926,\n",
       " 0.59770775,\n",
       " 0.58662456,\n",
       " 0.7904839,\n",
       " 0.77923286,\n",
       " 0.5754253,\n",
       " 0.7561533,\n",
       " 0.8695647,\n",
       " 0.65020245,\n",
       " 0.30855823,\n",
       " 0.66000926,\n",
       " 0.61390656,\n",
       " 0.55100834,\n",
       " 0.59688157,\n",
       " 0.39493147,\n",
       " 0.7703458,\n",
       " 0.5835558,\n",
       " 0.75746876,\n",
       " 0.6137336,\n",
       " 0.5927499,\n",
       " 0.6978644,\n",
       " 0.7322742,\n",
       " 0.5223786,\n",
       " 0.46331233,\n",
       " 0.6492085,\n",
       " 0.7789979,\n",
       " 0.58735627,\n",
       " 0.6247794,\n",
       " 0.80674654,\n",
       " 0.799804,\n",
       " 0.54738027,\n",
       " 0.68064433,\n",
       " 0.52670026,\n",
       " 0.7133003,\n",
       " 0.64580125,\n",
       " 0.6507669,\n",
       " 0.47645468,\n",
       " 0.56855416,\n",
       " 0.7353501,\n",
       " 0.65296197,\n",
       " 0.56038254,\n",
       " 0.75890577,\n",
       " 0.58265203,\n",
       " 0.6524682,\n",
       " 0.61027044,\n",
       " 0.5424936,\n",
       " 0.56132144,\n",
       " 0.63495934,\n",
       " 0.4082924,\n",
       " 0.6790167,\n",
       " 0.82323104,\n",
       " 0.54920465,\n",
       " 0.5216997,\n",
       " 0.7459322,\n",
       " 0.6884353,\n",
       " 0.6199342,\n",
       " 0.7741515,\n",
       " 0.49468857,\n",
       " 0.76943624,\n",
       " 0.5145791,\n",
       " 0.4447603,\n",
       " 0.6413253,\n",
       " 0.8467357,\n",
       " 0.59558463,\n",
       " 0.7225876,\n",
       " 0.45002517,\n",
       " 0.38551372,\n",
       " 0.8913536,\n",
       " 0.7390831,\n",
       " 0.4667033,\n",
       " 0.64214534,\n",
       " 0.59293675,\n",
       " 0.7197287,\n",
       " 0.6515164,\n",
       " 0.615657,\n",
       " 0.4875936,\n",
       " 0.7151231,\n",
       " 0.5424266,\n",
       " 0.69761485,\n",
       " 0.76853454,\n",
       " 0.5732265,\n",
       " 0.61471975,\n",
       " 0.8804265,\n",
       " 0.702892,\n",
       " 0.7102092,\n",
       " 0.67440295,\n",
       " 0.595354,\n",
       " 0.8597774,\n",
       " 0.56902725,\n",
       " 0.431412,\n",
       " 0.7221738,\n",
       " 0.8821751,\n",
       " 0.6272421,\n",
       " 0.86953074,\n",
       " 0.7949529,\n",
       " 0.6447934,\n",
       " 0.93267554,\n",
       " 0.827754,\n",
       " 0.59743994,\n",
       " 0.7116215,\n",
       " 0.68046874,\n",
       " 0.80247545,\n",
       " 0.7759585,\n",
       " 0.77265984,\n",
       " 0.7703447,\n",
       " 0.75030863,\n",
       " 0.7829598,\n",
       " 0.8332237,\n",
       " 0.7690249,\n",
       " 0.641421,\n",
       " 0.71603256,\n",
       " 0.9378324,\n",
       " 0.78766537,\n",
       " 0.6573661,\n",
       " 0.7342733,\n",
       " 0.9285063,\n",
       " 0.88283026,\n",
       " 0.46674156,\n",
       " 0.6227692,\n",
       " 0.82651,\n",
       " 0.7790157,\n",
       " 0.6198003,\n",
       " 0.7425005,\n",
       " 0.6098617,\n",
       " 0.6471993,\n",
       " 0.7617462,\n",
       " 0.64070594,\n",
       " 0.52682364,\n",
       " 0.6895605,\n",
       " 0.59825695,\n",
       " 0.62697417,\n",
       " 0.74589103,\n",
       " 0.59732246,\n",
       " 0.713212,\n",
       " 0.6487997,\n",
       " 0.5489533,\n",
       " 0.69002074,\n",
       " 0.76555884,\n",
       " 0.53464484,\n",
       " 0.52109295,\n",
       " 0.7490785,\n",
       " 0.80429673,\n",
       " 0.69858587,\n",
       " 0.7275191,\n",
       " 0.8017981,\n",
       " 0.8531,\n",
       " 0.57937044,\n",
       " 0.47167152,\n",
       " 0.68830824,\n",
       " 0.88219815,\n",
       " 0.60875493,\n",
       " 0.8463939,\n",
       " 0.8251359,\n",
       " 0.82594156,\n",
       " 0.87813556,\n",
       " 0.83031684,\n",
       " 0.656495,\n",
       " 0.64501417,\n",
       " 0.78679836,\n",
       " 0.7952107,\n",
       " 0.7584193,\n",
       " 0.77524555,\n",
       " 0.8340369,\n",
       " 0.7583938,\n",
       " 0.7102025,\n",
       " 0.6746038,\n",
       " 0.78572536,\n",
       " 0.5207234,\n",
       " 0.7636232,\n",
       " 0.9012846,\n",
       " 0.8227657,\n",
       " 0.63207924,\n",
       " 0.68696725,\n",
       " 0.9232257,\n",
       " 0.86911297,\n",
       " 0.52664024,\n",
       " 0.7146199,\n",
       " 0.79852575,\n",
       " 0.81902045,\n",
       " 0.48654443,\n",
       " 0.83413535,\n",
       " 0.75661445,\n",
       " 0.72838783,\n",
       " 0.8925764,\n",
       " 0.8036249,\n",
       " 0.5249366,\n",
       " 0.62202203,\n",
       " 0.71915936,\n",
       " 0.7416373,\n",
       " 0.7221707,\n",
       " 0.7497387,\n",
       " 0.91769975,\n",
       " 0.7356921,\n",
       " 0.7488577,\n",
       " 0.70233446,\n",
       " 0.7893,\n",
       " 0.6582776,\n",
       " 0.59290737,\n",
       " 0.8816072,\n",
       " 0.56157005,\n",
       " 0.46440208,\n",
       " 0.64820313,\n",
       " 0.86241347,\n",
       " 0.79110205,\n",
       " 0.34108797,\n",
       " 0.6861738,\n",
       " 0.7865165,\n",
       " 0.82913864,\n",
       " 0.763418,\n",
       " 0.6457651,\n",
       " 0.72699505,\n",
       " 0.30489755,\n",
       " 0.7864612,\n",
       " 0.5335455,\n",
       " 0.434603,\n",
       " 0.7300274,\n",
       " 0.48113954,\n",
       " 0.6209612,\n",
       " 0.54711276,\n",
       " 0.30110344,\n",
       " 0.54978144,\n",
       " 0.53848076,\n",
       " 0.4696849,\n",
       " 0.5602979,\n",
       " 0.8161763,\n",
       " 0.5929127,\n",
       " 0.18178183,\n",
       " 0.77806956,\n",
       " 0.55665517,\n",
       " 0.7700109,\n",
       " 0.77219576,\n",
       " 0.7436837,\n",
       " 0.79544705,\n",
       " 0.6841511,\n",
       " 0.35658744,\n",
       " 0.6321465,\n",
       " 0.50223196,\n",
       " 0.46723124,\n",
       " 0.55203897,\n",
       " 0.30127075,\n",
       " 0.4153849,\n",
       " 0.5371569,\n",
       " 0.44141927,\n",
       " 0.44072694,\n",
       " 0.5361641,\n",
       " 0.4350983,\n",
       " 0.3183902,\n",
       " 0.64549965,\n",
       " 0.40399587,\n",
       " 0.6044803,\n",
       " 0.39343277,\n",
       " 0.73951197,\n",
       " 0.46479887,\n",
       " 0.8387391,\n",
       " 0.5507204,\n",
       " 0.2695274,\n",
       " 0.59742135,\n",
       " 0.7423517,\n",
       " 0.605598,\n",
       " 0.5960307,\n",
       " 0.51102394,\n",
       " 0.7044495,\n",
       " 0.51419795,\n",
       " 0.4757053,\n",
       " 0.42150962,\n",
       " 0.8579698,\n",
       " 0.45991346,\n",
       " 0.8545307,\n",
       " 0.8448491,\n",
       " 0.71418446,\n",
       " 0.8937207,\n",
       " 0.8391503,\n",
       " 0.56785184,\n",
       " 0.68316525,\n",
       " 0.7205487,\n",
       " 0.76845014,\n",
       " 0.7492454,\n",
       " 0.68003815,\n",
       " 0.76242757,\n",
       " 0.66539794,\n",
       " 0.7369292,\n",
       " 0.740969,\n",
       " 0.72845066,\n",
       " 0.588653,\n",
       " 0.5911752,\n",
       " 0.8493341,\n",
       " 0.7870785,\n",
       " 0.58183074,\n",
       " 0.5142264,\n",
       " 0.49401966,\n",
       " 0.8904837,\n",
       " 0.49114388,\n",
       " 0.5661814,\n",
       " 0.81182534,\n",
       " 0.70552987,\n",
       " 0.5445212,\n",
       " 0.5241826,\n",
       " 0.37257934,\n",
       " 0.2884849,\n",
       " 0.57277936,\n",
       " 0.32046607,\n",
       " 0.47272056,\n",
       " 0.42059806,\n",
       " 0.47156015,\n",
       " 0.42426485,\n",
       " 0.56501466,\n",
       " 0.27080905,\n",
       " 0.6724694,\n",
       " 0.41804242,\n",
       " 0.7049048,\n",
       " 0.48416835,\n",
       " 0.78752756,\n",
       " 0.4525548,\n",
       " 0.5894712,\n",
       " 0.56716245,\n",
       " 0.5861737,\n",
       " 0.5970445,\n",
       " 0.48316494,\n",
       " 0.47777623,\n",
       " 0.8928136,\n",
       " 0.6022392,\n",
       " 0.26673612,\n",
       " 0.44133008,\n",
       " 0.7888299,\n",
       " 0.56030345,\n",
       " 0.78024447,\n",
       " 0.70293915,\n",
       " 0.71628785,\n",
       " 0.8320759,\n",
       " 0.6976385,\n",
       " 0.6018342,\n",
       " 0.56600654,\n",
       " 0.64770854,\n",
       " 0.6971986,\n",
       " 0.82491773,\n",
       " 0.6553425,\n",
       " 0.7040417,\n",
       " 0.62261,\n",
       " 0.572092,\n",
       " 0.7164959,\n",
       " 0.8487919,\n",
       " 0.55430806,\n",
       " 0.28184435,\n",
       " 0.81119305,\n",
       " 0.7899363,\n",
       " 0.6793388,\n",
       " 0.7240587,\n",
       " 0.82938343,\n",
       " 0.8262235,\n",
       " 0.48065123,\n",
       " 0.56283784,\n",
       " 0.6603899,\n",
       " 0.7247885,\n",
       " 0.59434456,\n",
       " 0.7323429,\n",
       " 0.64175737,\n",
       " 0.60640824,\n",
       " 0.5287678,\n",
       " 0.685,\n",
       " 0.53846693,\n",
       " 0.48404405,\n",
       " 0.63178843,\n",
       " 0.47406015,\n",
       " 0.6048886,\n",
       " 0.5964783,\n",
       " 0.6562964,\n",
       " 0.6089884,\n",
       " 0.48233244,\n",
       " 0.62862307,\n",
       " 0.76241773,\n",
       " 0.606032,\n",
       " 0.7119871,\n",
       " 0.794589,\n",
       " 0.62004924,\n",
       " 0.6697045,\n",
       " 0.66100144,\n",
       " 0.6894888,\n",
       " 0.8512431,\n",
       " 0.54855776,\n",
       " 0.4733399,\n",
       " 0.57648444,\n",
       " 0.8271805,\n",
       " 0.641317,\n",
       " 0.9212267,\n",
       " 0.8239515,\n",
       " 0.7151437,\n",
       " 0.8787411,\n",
       " 0.84821326,\n",
       " 0.65648925,\n",
       " 0.5446292,\n",
       " 0.6831903,\n",
       " 0.7414896,\n",
       " 0.7727218,\n",
       " 0.80580884,\n",
       " 0.7226402,\n",
       " 0.713902,\n",
       " 0.8643571,\n",
       " 0.85743034,\n",
       " 0.8001656,\n",
       " 0.65816563,\n",
       " 0.7075318,\n",
       " 0.89029473,\n",
       " 0.66445017,\n",
       " 0.76752335,\n",
       " 0.6724348,\n",
       " 0.8180406,\n",
       " 0.90571487,\n",
       " 0.5743149,\n",
       " 0.5975152,\n",
       " 0.81213886,\n",
       " 0.8782895,\n",
       " 0.60634035,\n",
       " 0.826348,\n",
       " 0.96690124,\n",
       " 0.7839701,\n",
       " 0.92199355,\n",
       " 0.8489132,\n",
       " 0.62330323,\n",
       " 0.58485603,\n",
       " 0.747544,\n",
       " 0.8423539,\n",
       " 0.81336224,\n",
       " 0.7897337,\n",
       " 0.80900353,\n",
       " 0.7426761,\n",
       " 0.70548207,\n",
       " 0.74122536,\n",
       " 0.8324802,\n",
       " 0.61038643,\n",
       " 0.6422325,\n",
       " 0.9308864,\n",
       " 0.8284819,\n",
       " 0.69654244,\n",
       " 0.70587236,\n",
       " 0.89278936,\n",
       " 0.882173,\n",
       " 0.4437998,\n",
       " 0.7468925,\n",
       " 0.88075,\n",
       " 0.75084573,\n",
       " 0.6522362,\n",
       " 0.7303452,\n",
       " 0.6357833,\n",
       " 0.6525528,\n",
       " 0.86846036,\n",
       " 0.73571354,\n",
       " 0.49700245,\n",
       " 0.53548235,\n",
       " 0.657725,\n",
       " 0.79267114,\n",
       " 0.7657752,\n",
       " 0.72070336,\n",
       " 0.6810904,\n",
       " 0.6671239,\n",
       " 0.49197543,\n",
       " 0.7377318,\n",
       " 0.81216484,\n",
       " 0.6032024,\n",
       " 0.25484335,\n",
       " 0.81224144,\n",
       " 0.5833228,\n",
       " 0.6794824,\n",
       " 0.70095617,\n",
       " 0.74319905,\n",
       " 0.78704387,\n",
       " 0.4856136,\n",
       " 0.4804846,\n",
       " 0.7405568,\n",
       " 0.7648325,\n",
       " 0.57442874,\n",
       " 0.8315481,\n",
       " 0.65100485,\n",
       " 0.6952162,\n",
       " 0.7323725,\n",
       " 0.81148195,\n",
       " 0.6385335,\n",
       " 0.48862213,\n",
       " 0.5553443,\n",
       " 0.7788664,\n",
       " 0.82938015,\n",
       " 0.7591115,\n",
       " 0.61194533,\n",
       " 0.6827736,\n",
       " 0.43775645,\n",
       " 0.7772932,\n",
       " 0.7461618,\n",
       " 0.6009274,\n",
       " 0.28661978,\n",
       " 0.8031968,\n",
       " 0.67192256,\n",
       " 0.6545642,\n",
       " 0.6138438,\n",
       " 0.8697589,\n",
       " 0.84425944,\n",
       " 0.43870422,\n",
       " 0.5328819,\n",
       " 0.77693963,\n",
       " 0.8759234,\n",
       " 0.5263518,\n",
       " 0.8519736,\n",
       " 0.8237915,\n",
       " 0.7249813,\n",
       " 0.9469668,\n",
       " 0.8266384,\n",
       " 0.628449,\n",
       " 0.7094631,\n",
       " 0.7098544,\n",
       " 0.80388063,\n",
       " 0.7932247,\n",
       " 0.74612737,\n",
       " 0.80303115,\n",
       " 0.7127733,\n",
       " 0.6173836,\n",
       " 0.7771906,\n",
       " 0.78048843,\n",
       " 0.60075027,\n",
       " 0.5673596,\n",
       " 0.9463063,\n",
       " 0.6709492,\n",
       " 0.634615,\n",
       " 0.5212717,\n",
       " 0.88022405,\n",
       " 0.88149565,\n",
       " 0.405379,\n",
       " 0.6869106,\n",
       " 0.83242357,\n",
       " 0.87961733,\n",
       " 0.5886081,\n",
       " 0.7416274,\n",
       " 0.53301406,\n",
       " 0.57701856,\n",
       " 0.8966075,\n",
       " 0.64751303,\n",
       " 0.4925755,\n",
       " 0.5305122,\n",
       " 0.5949166,\n",
       " 0.68163025,\n",
       " 0.78949356,\n",
       " 0.5737179,\n",
       " 0.7107673,\n",
       " 0.5522973,\n",
       " 0.4948661,\n",
       " 0.6876504,\n",
       " 0.7984199,\n",
       " 0.5305851,\n",
       " 0.3302218,\n",
       " 0.8757329,\n",
       " 0.76750433,\n",
       " 0.68307644,\n",
       " 0.6200647,\n",
       " 0.61980206,\n",
       " 0.8698529,\n",
       " 0.51823026,\n",
       " 0.3656984,\n",
       " 0.68663406,\n",
       " 0.8157751,\n",
       " 0.6152079,\n",
       " 0.8230503,\n",
       " 0.6768131,\n",
       " 0.69467187,\n",
       " 0.8797917,\n",
       " 0.78777605,\n",
       " 0.51869273,\n",
       " 0.5366716,\n",
       " 0.6968681,\n",
       " 0.77033234,\n",
       " 0.6990325,\n",
       " 0.67233396,\n",
       " 0.666051,\n",
       " 0.69265825,\n",
       " 0.39447454,\n",
       " 0.7622755,\n",
       " 0.77462035,\n",
       " 0.6033165,\n",
       " 0.6424982,\n",
       " 0.8846275,\n",
       " 0.75283647,\n",
       " 0.7089253,\n",
       " 0.69618654,\n",
       " 0.9073464,\n",
       " 0.87684953,\n",
       " 0.51304483,\n",
       " 0.4955329,\n",
       " 0.7862964,\n",
       " 0.8014251,\n",
       " 0.6045827,\n",
       " 0.84220487,\n",
       " 0.8237475,\n",
       " 0.7739496,\n",
       " 0.9326009,\n",
       " 0.8179281,\n",
       " 0.58769053,\n",
       " 0.62595564,\n",
       " 0.71003205,\n",
       " 0.8242611,\n",
       " 0.76282376,\n",
       " 0.8757781,\n",
       " 0.71673733,\n",
       " 0.80705196,\n",
       " 0.50077987,\n",
       " 0.8549626,\n",
       " 0.8613781,\n",
       " 0.68162656,\n",
       " 0.77206343,\n",
       " 0.9236526,\n",
       " 0.7499786,\n",
       " 0.66953194,\n",
       " 0.72490066,\n",
       " 0.76362026,\n",
       " 0.9175197,\n",
       " 0.5735915,\n",
       " 0.57657874,\n",
       " 0.8380205,\n",
       " 0.78785855,\n",
       " 0.38402906,\n",
       " 0.8475959,\n",
       " 0.6577635,\n",
       " 0.68612504,\n",
       " 0.89735305,\n",
       " 0.7959253,\n",
       " 0.64700234,\n",
       " 0.72403777,\n",
       " 0.7148365,\n",
       " 0.7591759,\n",
       " 0.78101873,\n",
       " 0.75581545,\n",
       " 0.7304058,\n",
       " 0.68417734,\n",
       " 0.7072117,\n",
       " 0.78246504,\n",
       " 0.8725763,\n",
       " 0.66304535,\n",
       " 0.45621184,\n",
       " 0.83618486,\n",
       " 0.63159734,\n",
       " 0.59467995,\n",
       " 0.36937532,\n",
       " 0.91630095,\n",
       " 0.8655934,\n",
       " 0.31216615,\n",
       " 0.6713149,\n",
       " 0.84749156,\n",
       " 0.7945445,\n",
       " 0.77815855,\n",
       " 0.7832715,\n",
       " 0.54905313,\n",
       " 0.68372005,\n",
       " 0.74165505,\n",
       " 0.7895017,\n",
       " 0.59236336,\n",
       " 0.5203411,\n",
       " 0.71948767,\n",
       " 0.64934486,\n",
       " 0.69892204,\n",
       " 0.61602354,\n",
       " 0.6073396,\n",
       " 0.61802495,\n",
       " 0.90952057,\n",
       " 0.6490208,\n",
       " 0.9213742,\n",
       " 0.5471527,\n",
       " 0.6873047,\n",
       " 0.79888827,\n",
       " 0.581807,\n",
       " 0.846308,\n",
       " 0.8783353,\n",
       " 0.43490872,\n",
       " 0.796471,\n",
       " 0.5989992,\n",
       " 0.5294366,\n",
       " 0.6483007,\n",
       " 0.9036268,\n",
       " 0.50019914,\n",
       " 0.8246589,\n",
       " 0.797412,\n",
       " 0.7855363,\n",
       " 0.8818287,\n",
       " 0.80438465,\n",
       " 0.61106014,\n",
       " 0.5797239,\n",
       " 0.7190897,\n",
       " 0.7193542,\n",
       " 0.74217224,\n",
       " 0.75874114,\n",
       " 0.8097781,\n",
       " 0.78108025,\n",
       " 0.6226496,\n",
       " 0.8148324,\n",
       " 0.6740089,\n",
       " 0.57696027,\n",
       " 0.82196295,\n",
       " 0.8871515,\n",
       " 0.7665859,\n",
       " 0.43008208,\n",
       " 0.57829845,\n",
       " 0.8215163,\n",
       " 0.9015537,\n",
       " 0.53618544,\n",
       " 0.5875002,\n",
       " 0.8294898,\n",
       " 0.70940274,\n",
       " 0.62924856,\n",
       " 0.83288383,\n",
       " 0.69745004,\n",
       " 0.53991586,\n",
       " 0.92840827,\n",
       " 0.7278059,\n",
       " 0.5470173,\n",
       " 0.68275946,\n",
       " 0.7074868,\n",
       " 0.5406076,\n",
       " 0.64574546,\n",
       " 0.73751724,\n",
       " 0.6427321,\n",
       " 0.6315598,\n",
       " 0.86142725,\n",
       " 0.6602565,\n",
       " 0.9659165,\n",
       " 0.5907036,\n",
       " 0.1358435,\n",
       " 0.83769137,\n",
       " 0.36828712,\n",
       " 0.7285577,\n",
       " 0.6733902,\n",
       " 0.8193158,\n",
       " 0.7880975,\n",
       " 0.4292321,\n",
       " 0.6848405,\n",
       " 0.8061783,\n",
       " 0.68199366,\n",
       " 0.34490874,\n",
       " 0.636195,\n",
       " 0.66909647,\n",
       " 0.548222,\n",
       " 0.84322196,\n",
       " 0.5273479,\n",
       " 0.4969537,\n",
       " 0.6994997,\n",
       " 0.5547199,\n",
       " 0.53230935,\n",
       " 0.7364922,\n",
       " 0.4641112,\n",
       " 0.5877272,\n",
       " ...]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6017447000805548"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "y = true_label\n",
    "pred = np.array(y_hat_l)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_list = [1 if x > 0.69 else 0 for x in y_hat_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = []\n",
    "for time, snapshot in enumerate(test_dataset):\n",
    "    true_label.append(list(snapshot.y.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = [int(z) for y in true_label for z in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.50      0.60      0.55       661\n",
      "     class 1       0.63      0.54      0.58       847\n",
      "\n",
      "    accuracy                           0.56      1508\n",
      "   macro avg       0.57      0.57      0.56      1508\n",
      "weighted avg       0.58      0.56      0.57      1508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = true_label\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_true, y_hat_list, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
